{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-ultralytics-yolov8","title":"Welcome to Ultralytics YOLOv8","text":"<p>Welcome to the Ultralytics YOLOv8 documentation landing page! Ultralytics YOLOv8 is the latest version of the YOLO (You Only Look Once) object detection and image segmentation model developed by Ultralytics. This page serves as the starting point for exploring the various resources available to help you get started with YOLOv8 and understand its features and capabilities.</p> <p>The YOLOv8 model is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and image segmentation tasks. It can be trained on large datasets and is capable of running on a variety of hardware platforms, from CPUs to GPUs.</p> <p>Whether you are a seasoned machine learning practitioner or new to the field, we hope that the resources on this page will help you get the most out of YOLOv8. For any bugs and feature requests please visit GitHub Issues. For professional support please Contact Us.</p>"},{"location":"#a-brief-history-of-yolo","title":"A Brief History of YOLO","text":"<p>YOLO (You Only Look Once) is a popular object detection and image segmentation model developed by Joseph Redmon and Ali Farhadi at the University of Washington. The first version of YOLO was released in 2015 and quickly gained popularity due to its high speed and accuracy.</p> <p>YOLOv2 was released in 2016 and improved upon the original model by incorporating batch normalization, anchor boxes, and dimension clusters. YOLOv3 was released in 2018 and further improved the model's performance by using a more efficient backbone network, adding a feature pyramid, and making use of focal loss.</p> <p>In 2020, YOLOv4 was released which introduced a number of innovations such as the use of Mosaic data augmentation, a new anchor-free detection head, and a new loss function.</p> <p>In 2021, Ultralytics released YOLOv5, which further improved the model's performance and added new features such as support for panoptic segmentation and object tracking.</p> <p>YOLO has been widely used in a variety of applications, including autonomous vehicles, security and surveillance, and medical imaging. It has also been used to win several competitions, such as the COCO Object Detection Challenge and the DOTA Object Detection Challenge.</p> <p>For more information about the history and development of YOLO, you can refer to the following references:</p> <ul> <li>Redmon, J., &amp; Farhadi, A. (2015). You only look once: Unified, real-time object detection. In Proceedings of the IEEE   conference on computer vision and pattern recognition (pp. 779-788).</li> <li>Redmon, J., &amp; Farhadi, A. (2016). YOLO9000: Better, faster, stronger. In Proceedings</li> </ul>"},{"location":"#ultralytics-yolov8","title":"Ultralytics YOLOv8","text":"<p>Ultralytics YOLOv8 is the latest version of the YOLO object detection and image segmentation model developed by Ultralytics. YOLOv8 is a cutting-edge, state-of-the-art (SOTA) model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility.</p> <p>One key feature of YOLOv8 is its extensibility. It is designed as a framework that supports all previous versions of YOLO, making it easy to switch between different versions and compare their performance. This makes YOLOv8 an ideal choice for users who want to take advantage of the latest YOLO technology while still being able to use their existing YOLO models.</p> <p>In addition to its extensibility, YOLOv8 includes a number of other innovations that make it an appealing choice for a wide range of object detection and image segmentation tasks. These include a new backbone network, a new anchor-free detection head, and a new loss function. YOLOv8 is also highly efficient and can be run on a variety of hardware platforms, from CPUs to GPUs.</p> <p>Overall, YOLOv8 is a powerful and flexible tool for object detection and image segmentation that offers the best of both worlds: the latest SOTA technology and the ability to use and compare all previous YOLO versions.</p>"},{"location":"app/","title":"Ultralytics HUB App for YOLOv8","text":"<p>Welcome to the Ultralytics HUB app for demonstrating YOLOv5 and YOLOv8 models! In this app, available on the Apple App Store and the Google Play Store, you will be able to see the power and capabilities of YOLOv5, a state-of-the-art object detection model developed by Ultralytics. </p> <p>To install simply scan the QR code above. The App currently features YOLOv5 models, with YOLOv8 models coming soon.</p> <p>With YOLOv5, you can detect and classify objects in images and videos with high accuracy and speed. The model has been trained on a large dataset and is able to detect a wide range of objects, including cars, pedestrians, and traffic signs.</p> <p>In this app, you will be able to try out YOLOv5 on your own images and videos, and see the model in action. You can also learn more about how YOLOv5 works and how it can be used in real-world applications.</p> <p>We hope you enjoy using YOLOv5 and seeing its capabilities firsthand. Thank you for choosing Ultralytics for your object detection needs!</p>"},{"location":"cli/","title":"CLI","text":""},{"location":"cli/#cli-basics","title":"CLI Basics","text":"<p>If you want to train, validate or run inference on models and don't need to make any modifications to the code, using YOLO command line interface is the easiest way to get started.</p>  <p>Syntax</p> <pre><code>yolo task=detect    mode=train    model=yolov8n.yaml    epochs=1 ...\n            ...           ...           ...\n          segment        predict        yolov8n-seg.pt\n          classify        val           yolov8n-cls.pt\n</code></pre>  <p>The experiment arguments can be overridden directly by pass <code>arg=val</code> covered in the next section. You can run any supported task by setting <code>task</code> and <code>mode</code> in cli.</p> Training         <code>task</code> snippet     Detection <code>detect</code> <pre><code>yolo task=detect mode=train       </code></pre>   Instance Segment <code>segment</code> <pre><code>yolo task=segment mode=train      </code></pre>   Classification <code>classify</code> <pre><code>yolo task=classify mode=train    </code></pre>    Prediction         <code>task</code> snippet     Detection <code>detect</code> <pre><code>yolo task=detect mode=predict       </code></pre>   Instance Segment <code>segment</code> <pre><code>yolo task=segment mode=predict     </code></pre>   Classification <code>classify</code> <pre><code>yolo task=classify mode=predict    </code></pre>    Validation         <code>task</code> snippet     Detection <code>detect</code> <pre><code>yolo task=detect mode=val        </code></pre>   Instance Segment <code>segment</code> <pre><code>yolo task=segment mode=val       </code></pre>   Classification <code>classify</code> <pre><code>yolo task=classify mode=val      </code></pre>     <p>Note: The arguments don't require <code>'--'</code> prefix. These are reserved for special commands covered later</p>"},{"location":"cli/#overriding-default-config-arguments","title":"Overriding default config arguments","text":"<p>All global default arguments can be overriden by simply passing them as arguments in the cli.</p>  SyntaxExample   <p><code>yolo task= ... mode= ...  arg=val </code></p>   <p>Perform detection training for <code>10 epochs</code> with <code>learning_rate</code> of <code>0.01</code> <pre><code>yolo task=detect mode=train  epochs=10 lr0=0.01 \n</code></pre></p>"},{"location":"cli/#overriding-default-config-file","title":"Overriding default config file","text":"<p>You can override config file entirely by passing a new file. You can create a copy of default config file in your current working dir as follows: <pre><code>yolo task=init\n</code></pre> You can then use <code>cfg=name.yaml</code> command to pass the new config file <pre><code>yolo cfg=default.yaml\n</code></pre></p>  Example CommandResult   <pre><code>yolo task=init\nyolo cfg=default.yaml\n</code></pre>   <p>TODO: add terminal output</p>"},{"location":"config/","title":"Configuration","text":"<p>YOLO settings and hyperparameters play a critical role in the model's performance, speed, and accuracy. These settings and hyperparameters can affect the model's behavior at various stages of the model development process, including training, validation, and prediction.</p> <p>Properly setting and tuning these parameters can have a significant impact on the model's ability to learn effectively from the training data and generalize to new data. For example, choosing an appropriate learning rate, batch size, and optimization algorithm can greatly affect the model's convergence speed and accuracy. Similarly, setting the correct confidence threshold and non-maximum suppression (NMS) threshold can affect the model's performance on detection tasks.</p> <p>It is important to carefully consider and experiment with these settings and hyperparameters to achieve the best possible performance for a given task. This can involve trial and error, as well as using techniques such as hyperparameter optimization to search for the optimal set of parameters.</p> <p>In summary, YOLO settings and hyperparameters are a key factor in the success of a YOLO model, and it is important to pay careful attention to them to achieve the desired results.</p>"},{"location":"config/#setting-the-operation-type","title":"Setting the operation type","text":"<p>YOLO models can be used for a variety of tasks, including detection, segmentation, and classification. These tasks differ in the type of output they produce and the specific problem they are designed to solve.</p> <ul> <li>Detection: Detection tasks involve identifying and localizing objects or regions of interest in an image or video.   YOLO models can be used for object detection tasks by predicting the bounding boxes and class labels of objects in an   image.</li> <li>Segmentation: Segmentation tasks involve dividing an image or video into regions or pixels that correspond to   different objects or classes. YOLO models can be used for image segmentation tasks by predicting a mask or label for   each pixel in an image.</li> <li>Classification: Classification tasks involve assigning a class label to an input, such as an image or text. YOLO   models can be used for image classification tasks by predicting the class label of an input image.</li> </ul> <p>YOLO models can be used in different modes depending on the specific problem you are trying to solve. These modes include train, val, and predict.</p> <ul> <li>Train: The train mode is used to train the model on a dataset. This mode is typically used during the development and   testing phase of a model.</li> <li>Val: The val mode is used to evaluate the model's performance on a validation dataset. This mode is typically used to   tune the model's hyperparameters and detect overfitting.</li> <li>Predict: The predict mode is used to make predictions with the model on new data. This mode is typically used in   production or when deploying the model to users.</li> </ul>    Key Value Description     task <code>detect</code> Set the task via CLI. See Tasks for all supported tasks like - <code>detect</code>, <code>segment</code>, <code>classify</code>. - <code>init</code> is a special case that creates a copy of default.yaml configs to the current working dir   mode <code>train</code> Set the mode via CLI. It can be <code>train</code>, <code>val</code>, <code>predict</code>   resume <code>False</code> Resume last given task when set to <code>True</code>.  Resume from a given checkpoint is <code>model.pt</code> is passed   model null Set the model. Format can differ for task type. Supports <code>model_name</code>, <code>model.yaml</code> &amp; <code>model.pt</code>   data null Set the data. Format can differ for task type. Supports <code>data.yaml</code>, <code>data_folder</code>, <code>dataset_name</code>"},{"location":"config/#training","title":"Training","text":"<p>Training settings for YOLO models refer to the various hyperparameters and configurations used to train the model on a dataset. These settings can affect the model's performance, speed, and accuracy. Some common YOLO training settings include the batch size, learning rate, momentum, and weight decay. Other factors that may affect the training process include the choice of optimizer, the choice of loss function, and the size and composition of the training dataset. It is important to carefully tune and experiment with these settings to achieve the best possible performance for a given task.</p>    Key Value Description     device '' cuda device, i.e. 0 or 0,1,2,3 or cpu. <code>''</code> selects available cuda 0 device   epochs 100 Number of epochs to train   workers 8 Number of cpu workers used per process. Scales automatically with DDP   batch 16 Batch size of the dataloader   imgsz 640 Image size of data in dataloader   optimizer SGD Optimizer used. Supported optimizer are: <code>Adam</code>, <code>SGD</code>, <code>RMSProp</code>   single_cls False Train on multi-class data as single-class   image_weights False Use weighted image selection for training   rect False Enable rectangular training   cos_lr False Use cosine LR scheduler   lr0 0.01 Initial learning rate   lrf 0.01 Final OneCycleLR learning rate   momentum 0.937 Use as <code>momentum</code> for SGD and <code>beta1</code> for Adam   weight_decay 0.0005 Optimizer weight decay   warmup_epochs 3.0 Warmup epochs. Fractions are ok.   warmup_momentum 0.8 Warmup initial momentum   warmup_bias_lr 0.1 Warmup initial bias lr   box 0.05 Box loss gain   cls 0.5 cls loss gain   cls_pw 1.0 cls BCELoss positive_weight   obj 1.0 bj loss gain (scale with pixels)   obj_pw 1.0 obj BCELoss positive_weight   iou_t 0.20 IOU training threshold   anchor_t 4.0 anchor-multiple threshold   fl_gamma 0.0 focal loss gamma   label_smoothing 0.0    nbs 64 nominal batch size   overlap_mask <code>True</code> Segmentation: Use mask overlapping during training   mask_ratio 4 Segmentation: Set mask downsampling   dropout <code>False</code> Classification: Use dropout while training"},{"location":"config/#prediction","title":"Prediction","text":"<p>Prediction settings for YOLO models refer to the various hyperparameters and configurations used to make predictions with the model on new data. These settings can affect the model's performance, speed, and accuracy. Some common YOLO prediction settings include the confidence threshold, non-maximum suppression (NMS) threshold, and the number of classes to consider. Other factors that may affect the prediction process include the size and format of the input data, the presence of additional features such as masks or multiple labels per box, and the specific task the model is being used for. It is important to carefully tune and experiment with these settings to achieve the best possible performance for a given task.</p>    Key Value Description     source <code>ultralytics/assets</code> Input source. Accepts image, folder, video, url   show <code>False</code> View the prediction images   save_txt <code>False</code> Save the results in a txt file   save_conf <code>False</code> Save the condidence scores   save_crop <code>Fasle</code>    hide_labels <code>False</code> Hide the labels   hide_conf <code>False</code> Hide the confidence scores   vid_stride <code>False</code> Input video frame-rate stride   line_thickness <code>3</code> Bounding-box thickness (pixels)   visualize <code>False</code> Visualize model features   augment <code>False</code> Augmented inference   agnostic_nms <code>False</code> Class-agnostic NMS   retina_masks <code>False</code> Segmentation: High resolution masks"},{"location":"config/#validation","title":"Validation","text":"<p>Validation settings for YOLO models refer to the various hyperparameters and configurations used to evaluate the model's performance on a validation dataset. These settings can affect the model's performance, speed, and accuracy. Some common YOLO validation settings include the batch size, the frequency with which validation is performed during training, and the metrics used to evaluate the model's performance. Other factors that may affect the validation process include the size and composition of the validation dataset and the specific task the model is being used for. It is important to carefully tune and experiment with these settings to ensure that the model is performing well on the validation dataset and to detect and prevent overfitting.</p>    Key Value Description     noval <code>False</code> ???   save_json <code>False</code>    save_hybrid <code>False</code>    conf <code>0.001</code> Confidence threshold   iou <code>0.6</code> IoU threshold   max_det <code>300</code> Maximum number of detections   half <code>True</code> Use .half() mode.   dnn <code>False</code> Use OpenCV DNN for ONNX inference   plots <code>False</code>"},{"location":"config/#export","title":"Export","text":"<p>Export settings for YOLO models refer to the various configurations and options used to save or export the model for use in other environments or platforms. These settings can affect the model's performance, size, and compatibility with different systems. Some common YOLO export settings include the format of the exported model file (e.g. ONNX, TensorFlow SavedModel), the device on which the model will be run (e.g. CPU, GPU), and the presence of additional features such as masks or multiple labels per box. Other factors that may affect the export process include the specific task the model is being used for and the requirements or constraints of the target environment or platform. It is important to carefully consider and configure these settings to ensure that the exported model is optimized for the intended use case and can be used effectively in the target environment.</p>"},{"location":"config/#augmentation","title":"Augmentation","text":"<p>Augmentation settings for YOLO models refer to the various transformations and modifications applied to the training data to increase the diversity and size of the dataset. These settings can affect the model's performance, speed, and accuracy. Some common YOLO augmentation settings include the type and intensity of the transformations applied (e.g. random flips, rotations, cropping, color changes), the probability with which each transformation is applied, and the presence of additional features such as masks or multiple labels per box. Other factors that may affect the augmentation process include the size and composition of the original dataset and the specific task the model is being used for. It is important to carefully tune and experiment with these settings to ensure that the augmented dataset is diverse and representative enough to train a high-performing model.</p>    hsv_h 0.015 Image HSV-Hue augmentation (fraction)     hsv_s 0.7 Image HSV-Saturation augmentation (fraction)   hsv_v 0.4 Image HSV-Value augmentation (fraction)   degrees 0.0 Image rotation (+/- deg)   translate 0.1 Image translation (+/- fraction)   scale 0.5 Image scale (+/- gain)   shear 0.0 Image shear (+/- deg)   perspective 0.0 Image perspective (+/- fraction), range 0-0.001   flipud 0.0 Image flip up-down (probability)   fliplr 0.5 Image flip left-right (probability)   mosaic 1.0 Image mosaic (probability)   mixup 0.0 Image mixup (probability)   copy_paste 0.0 Segment copy-paste (probability)"},{"location":"config/#logging-checkpoints-plotting-and-file-management","title":"Logging, checkpoints, plotting and file management","text":"<p>Logging, checkpoints, plotting, and file management are important considerations when training a YOLO model.</p> <ul> <li>Logging: It is often helpful to log various metrics and statistics during training to track the model's progress and   diagnose any issues that may arise. This can be done using a logging library such as TensorBoard or by writing log   messages to a file.</li> <li>Checkpoints: It is a good practice to save checkpoints of the model at regular intervals during training. This allows   you to resume training from a previous point if the training process is interrupted or if you want to experiment with   different training configurations.</li> <li>Plotting: Visualizing the model's performance and training progress can be helpful for understanding how the model is   behaving and identifying potential issues. This can be done using a plotting library such as matplotlib or by   generating plots using a logging library such as TensorBoard.</li> <li>File management: Managing the various files generated during the training process, such as model checkpoints, log   files, and plots, can be challenging. It is important to have a clear and organized file structure to keep track of   these files and make it easy to access and analyze them as needed.</li> </ul> <p>Effective logging, checkpointing, plotting, and file management can help you keep track of the model's progress and make it easier to debug and optimize the training process.</p>    Key Value Description     project: 'runs' The project name   name: 'exp' The run name. <code>exp</code> gets automatically incremented if not specified, i.e, <code>exp</code>, <code>exp2</code> ...   exist_ok: <code>False</code> ???   plots <code>False</code> Validation: Save plots while validation   nosave <code>False</code> Don't save any plots, models or files"},{"location":"engine/","title":"Customization Guide","text":"<p>Both the Ultralytics YOLO command-line and python interfaces are simply a high-level abstraction on the base engine executors. Let's take a look at the Trainer engine.</p>"},{"location":"engine/#basetrainer","title":"BaseTrainer","text":"<p>BaseTrainer contains the generic boilerplate training routine. It can be customized for any task based over overidding the required functions or operations as long the as correct formats are followed. For example you can support your own custom model and dataloder by just overriding these functions:</p> <ul> <li><code>get_model(cfg, weights)</code> - The function that builds a the model to be trained</li> <li><code>get_dataloder()</code> - The function that builds the dataloder More details and source code can be found in <code>BaseTrainer</code> Reference</li> </ul>"},{"location":"engine/#detectiontrainer","title":"DetectionTrainer","text":"<p>Here's how you can use the YOLOv8 <code>DetectionTrainer</code> and customize it. <pre><code>from ultralytics.yolo.v8 import DetectionTrainer\n\ntrainer = DetectionTrainer(overrides={...})\ntrainer.train()\ntrained_model = trainer.best # get best model\n</code></pre></p>"},{"location":"engine/#customizing-the-detectiontrainer","title":"Customizing the DetectionTrainer","text":"<p>Let's customize the trainer to train a custom detection model that is not supported directly. You can do this by simply overloading the existing the <code>get_model</code> functionality: <pre><code>from ultralytics.yolo.v8 import DetectionTrainer\n\nclass CustomTrainer(DetectionTrainer):\n    def get_model(self, cfg, weights):\n        ...\n\ntrainer = CustomTrainer(overrides={...})\ntrainer.train()\n</code></pre> You now realize that you need to customize the trainer further to:</p> <ul> <li>Customize the <code>loss function</code>. </li> <li>Add <code>callback</code> that uploads model to your google drive after every 10 <code>epochs</code> Here's how you can do it:</li> </ul> <pre><code>from ultralytics.yolo.v8 import DetectionTrainer\n\nclass CustomTrainer(DetectionTrainer):\n    def get_model(self, cfg, weights):\n        ...\n\n    def criterion(self, preds, batch):\n        # get ground truth\n        imgs = batch[\"imgs\"]\n        bboxes = batch[\"bboxes\"]\n        ...\n        return loss, loss_items # see Reference-&gt; Trainer for details on the expected format\n\n# callback to upload model weights\ndef log_model(trainer):\n    last_weight_path = trainer.last\n    ...\n\ntrainer = CustomTrainer(overrides={...})\ntrainer.add_callback(\"on_train_epoch_end\", log_model) # Adds to existing callback\ntrainer.train()\n</code></pre> <p>To know more about Callback triggering events and entry point,  checkout our Callbacks guide # TODO</p>"},{"location":"engine/#other-engine-components","title":"Other engine components","text":"<p>There are other componenets that can be customized similarly like <code>Validators</code> and <code>Predictiors</code> To know more about their implementation details, go to Reference</p>"},{"location":"hub/","title":"Ultralytics HUB","text":"<p>Ultralytics HUB is a new no-code online tool developed by Ultralytics, the creators of the popular YOLOv5 object detection and image segmentation models. With Ultralytics HUB, users can easily train and deploy YOLOv5 models without any coding or technical expertise.</p> <p>Ultralytics HUB is designed to be user-friendly and intuitive, with a drag-and-drop interface that allows users to easily upload their data and select their model configurations. It also offers a range of pre-trained models and templates to choose from, making it easy for users to get started with training their own models. Once a model is trained, it can be easily deployed and used for real-time object detection and image segmentation tasks. Overall, Ultralytics HUB is an essential tool for anyone looking to use YOLOv5 for their object detection and image segmentation projects.</p> <p>Get started now and experience the power and simplicity of Ultralytics HUB for yourself. Sign up for a free account and start building, training, and deploying YOLOv5 and YOLOv8 models today.</p>"},{"location":"hub/#1-upload-a-dataset","title":"1. Upload a Dataset","text":"<p>Ultralytics HUB datasets are just like YOLOv5 \ud83d\ude80 datasets, they use the same structure and the same label formats to keep everything simple. </p> <p>When you upload a dataset to Ultralytics HUB, make sure to place your dataset YAML inside the dataset root directory as in the example shown below, and then zip for upload to https://hub.ultralytics.com/. Your dataset YAML, directory and zip should all share the same name. For example, if your dataset is called 'coco6' as in our example ultralytics/hub/coco6.zip, then you should have a coco6.yaml inside your coco6/ directory, which should zip to create coco6.zip for upload:</p> <pre><code>zip -r coco6.zip coco6\n</code></pre> <p>The example coco6.zip dataset in this repository can be downloaded and unzipped to see exactly how to structure your custom dataset.</p>  <p>The dataset YAML is the same standard YOLOv5 YAML format. See the YOLOv5 Train Custom Data tutorial for full details. <pre><code># Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath:  # dataset root dir (leave empty for HUB)\ntrain: images/train  # train images (relative to 'path') 8 images\nval: images/val  # val images (relative to 'path') 8 images\ntest:  # test images (optional)\n\n# Classes\nnames:\n  0: person\n  1: bicycle\n  2: car\n  3: motorcycle\n  ...\n</code></pre></p> <p>After zipping your dataset, sign in to Ultralytics HUB and click the Datasets tab. Click 'Upload Dataset' to upload, scan and visualize your new dataset before training new YOLOv5 models on it!</p>"},{"location":"hub/#2-train-a-model","title":"2. Train a Model","text":"<p>Connect to the Ultralytics HUB notebook and use your model API key to begin training! </p>"},{"location":"hub/#3-deploy-to-real-world","title":"3. Deploy to Real World","text":"<p>Export your model to 13 different formats, including TensorFlow, ONNX, OpenVINO, CoreML, Paddle and many others. Run models directly on your mobile device by downloading the Ultralytics App!</p>"},{"location":"hub/#issues","title":"\u2753 Issues","text":"<p>If you are a new Ultralytics HUB user and have questions or comments, you are in the right place! Please raise a New Issue and let us know what we can do to make your life better \ud83d\ude03!</p>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#installation","title":"Installation","text":"<p>Install YOLOv8 via the <code>ultralytics</code> pip package for the latest stable release or by cloning the https://github.com/ultralytics/ultralytics repository for the most up-to-date version.</p>  <p>pip install (recommended)</p> <pre><code>pip install ultralytics\n</code></pre>   <p>git clone</p> <p><pre><code>git clone https://github.com/ultralytics/ultralytics\ncd ultralytics\npip install -e '.[dev]'\n</code></pre> See contributing section to know more about contributing to the project</p>"},{"location":"quickstart/#cli","title":"CLI","text":"<p>The command line YOLO interface lets you simply train, validate or infer models on various tasks and versions. CLI requires no customization or code. You can simply run all tasks from the terminal with the <code>yolo</code> command.</p>  <p>Note</p> SyntaxExample trainingExample Multi-GPU training   <pre><code>yolo task=detect    mode=train    model=yolov8n.yaml      args...\n          classify       predict        yolov8n-cls.yaml  args...\n          segment        val            yolov8n-seg.yaml  args...\n                         export         yolov8n.pt        format=onnx  args...\n</code></pre>   <pre><code>yolo task=detect mode=train model=yolov8n.pt data=coco128.yaml device=0\n</code></pre>   <pre><code>yolo task=detect mode=train model=yolov8n.pt data=coco128.yaml device=\\'0,1,2,3\\'\n</code></pre>     <p>CLI Guide</p>"},{"location":"quickstart/#python-api","title":"Python API","text":"<p>The Python API allows users to easily use YOLOv8 in their Python projects. It provides functions for loading and running the model, as well as for processing the model's output. The interface is designed to be easy to use, so that users can quickly implement object detection in their projects.</p> <p>Overall, the Python interface is a useful tool for anyone looking to incorporate object detection, segmentation or classification into their Python projects using YOLOv8.</p>  <p>Note</p> <pre><code>from ultralytics import YOLO\n\nmodel = YOLO('yolov8n.yaml')                # build a new model from scratch\nmodel = YOLO('yolov8n.pt')                  # load a pretrained model (recommended for best training results)\nresults = model.train(data='coco128.yaml')  # train the model\nresults = model.val()                       # evaluate model performance on the validation set\nresults = model.predict(source='bus.jpg')   # predict on an image\nsuccess = model.export(format='onnx')       # export the model to ONNX format\n</code></pre>  <p>API Guide</p>"},{"location":"sdk/","title":"Python Interface","text":""},{"location":"sdk/#using-yolo-models","title":"Using YOLO models","text":"<p>This is the simplest way of simply using yolo models in a python environment. It can be imported from the <code>ultralytics</code> module.</p>  <p>Usage</p> TrainingTraining pretrainedResume TrainingVisualize/save Predictions   <pre><code>from ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.yaml\")\nmodel(img_tensor) # Or model.forward(). inference.\nmodel.train(data=\"coco128.yaml\", epochs=5)\n</code></pre>   <pre><code>from ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\") # pass any model type\nmodel(...) # inference\nmodel.train(epochs=5)\n</code></pre>   <pre><code>from ultralytics import YOLO\n\nmodel = YOLO()\nmodel.resume(task=\"detect\") # resume last detection training\nmodel.resume(model=\"last.pt\") # resume from a given model/run\n</code></pre>     <pre><code>from ultralytics import YOLO\n\nmodel = YOLO(\"model.pt\")\nmodel.predict(source=\"0\") # accepts all formats - img/folder/vid.*(mp4/format). 0 for webcam\nmodel.predict(source=\"folder\", show=True) # Display preds. Accepts all yolo predict arguments\n</code></pre>   <p>Export and Deployment</p> Export, Fuse &amp; infoDeployment   <pre><code>from ultralytics import YOLO\n\nmodel = YOLO(\"model.pt\")\nmodel.fuse()  \nmodel.info(verbose=True)  # Print model information\nmodel.export(format=)  # TODO: \n</code></pre>     <p>More functionality coming soon</p>  <p>To know more about using <code>YOLO</code> models, refer Model class Reference</p> <p>Model reference</p>"},{"location":"sdk/#using-trainers","title":"Using Trainers","text":"<p><code>YOLO</code> model class is a high-level wrapper on the Trainer classes. Each YOLO task has its own trainer that inherits from <code>BaseTrainer</code>. </p>  <p>Detection Trainer Example<pre><code>from ultralytics.yolo import v8 import DetectionTrainer, DetectionValidator, DetectionPredictor\n\n# trainer\ntrainer = DetectionTrainer(overrides={})\ntrainer.train()\ntrained_model = trainer.best\n\n# Validator\nval = DetectionValidator(args=...)\nval(model=trained_model)\n\n# predictor\npred = DetectionPredictor(overrides={})\npred(source=SOURCE, model=trained_model)\n\n# resume from last weight\noverrides[\"resume\"] = trainer.last\ntrainer = detect.DetectionTrainer(overrides=overrides)\n</code></pre> </p>  <p>You can easily customize Trainers to support custom tasks or explore R&amp;D ideas. Learn more about Customizing <code>Trainers</code>, <code>Validators</code> and <code>Predictors</code> to suit your project needs in the Customization Section.</p> <p>Customization tutorials</p>"},{"location":"reference/base_pred/","title":"Predictor","text":"<p>All task Predictors are inherited from <code>BasePredictors</code> class that contains the model validation routine boilerplate. You can override any function of these Trainers to suit your needs.</p>"},{"location":"reference/base_pred/#basepredictor-api-reference","title":"BasePredictor API Reference","text":"<p>BasePredictor</p> <p>A base class for creating predictors.</p> <p>Attributes:</p>    Name Type Description     <code>args</code>  <code>OmegaConf</code>  <p>Configuration for the predictor.</p>   <code>save_dir</code>  <code>Path</code>  <p>Directory to save results.</p>   <code>done_setup</code>  <code>bool</code>  <p>Whether the predictor has finished setup.</p>   <code>model</code>  <code>nn.Module</code>  <p>Model used for prediction.</p>   <code>data</code>  <code>dict</code>  <p>Data configuration.</p>   <code>device</code>  <code>torch.device</code>  <p>Device used for prediction.</p>   <code>dataset</code>  <code>Dataset</code>  <p>Dataset used for prediction.</p>   <code>vid_path</code>  <code>str</code>  <p>Path to video file.</p>   <code>vid_writer</code>  <code>cv2.VideoWriter</code>  <p>Video writer for saving video output.</p>   <code>annotator</code>  <code>Annotator</code>  <p>Annotator used for prediction.</p>   <code>data_path</code>  <code>str</code>  <p>Path to data.</p>     Source code in <code>ultralytics/yolo/engine/predictor.py</code> <pre><code>class BasePredictor:\n    \"\"\"\n    BasePredictor\n\n    A base class for creating predictors.\n\n    Attributes:\n        args (OmegaConf): Configuration for the predictor.\n        save_dir (Path): Directory to save results.\n        done_setup (bool): Whether the predictor has finished setup.\n        model (nn.Module): Model used for prediction.\n        data (dict): Data configuration.\n        device (torch.device): Device used for prediction.\n        dataset (Dataset): Dataset used for prediction.\n        vid_path (str): Path to video file.\n        vid_writer (cv2.VideoWriter): Video writer for saving video output.\n        annotator (Annotator): Annotator used for prediction.\n        data_path (str): Path to data.\n    \"\"\"\n\n    def __init__(self, config=DEFAULT_CONFIG, overrides=None):\n        \"\"\"\n        Initializes the BasePredictor class.\n\n        Args:\n            config (str, optional): Path to a configuration file. Defaults to DEFAULT_CONFIG.\n            overrides (dict, optional): Configuration overrides. Defaults to None.\n        \"\"\"\n        if overrides is None:\n            overrides = {}\n        self.args = get_config(config, overrides)\n        project = self.args.project or Path(SETTINGS['runs_dir']) / self.args.task\n        name = self.args.name or f\"{self.args.mode}\"\n        self.save_dir = increment_path(Path(project) / name, exist_ok=self.args.exist_ok)\n        if self.args.save:\n            (self.save_dir / 'labels' if self.args.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)\n        if self.args.conf is None:\n            self.args.conf = 0.25  # default conf=0.25\n        self.done_setup = False\n\n        # Usable if setup is done\n        self.model = None\n        self.data = self.args.data  # data_dict\n        self.device = None\n        self.dataset = None\n        self.vid_path, self.vid_writer = None, None\n        self.annotator = None\n        self.data_path = None\n        self.output = dict()\n        self.callbacks = defaultdict(list, {k: [v] for k, v in callbacks.default_callbacks.items()})  # add callbacks\n        callbacks.add_integration_callbacks(self)\n\n    def preprocess(self, img):\n        pass\n\n    def get_annotator(self, img):\n        raise NotImplementedError(\"get_annotator function needs to be implemented\")\n\n    def write_results(self, pred, batch, print_string):\n        raise NotImplementedError(\"print_results function needs to be implemented\")\n\n    def postprocess(self, preds, img, orig_img):\n        return preds\n\n    def setup(self, source=None, model=None, return_outputs=True):\n        # source\n        source = str(source if source is not None else self.args.source)\n        is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n        is_url = source.lower().startswith(('rtsp://', 'rtmp://', 'http://', 'https://'))\n        webcam = source.isnumeric() or source.endswith('.streams') or (is_url and not is_file)\n        screenshot = source.lower().startswith('screen')\n        if is_url and is_file:\n            source = check_file(source)  # download\n\n        # model\n        device = select_device(self.args.device)\n        model = model or self.args.model\n        self.args.half &amp;= device.type != 'cpu'  # half precision only supported on CUDA\n        model = AutoBackend(model, device=device, dnn=self.args.dnn, fp16=self.args.half)\n        stride, pt = model.stride, model.pt\n        imgsz = check_imgsz(self.args.imgsz, stride=stride)  # check image size\n\n        # Dataloader\n        bs = 1  # batch_size\n        if webcam:\n            self.args.show = check_imshow(warn=True)\n            self.dataset = LoadStreams(source,\n                                       imgsz=imgsz,\n                                       stride=stride,\n                                       auto=pt,\n                                       transforms=getattr(model.model, 'transforms', None),\n                                       vid_stride=self.args.vid_stride)\n            bs = len(self.dataset)\n        elif screenshot:\n            self.dataset = LoadScreenshots(source,\n                                           imgsz=imgsz,\n                                           stride=stride,\n                                           auto=pt,\n                                           transforms=getattr(model.model, 'transforms', None))\n        else:\n            self.dataset = LoadImages(source,\n                                      imgsz=imgsz,\n                                      stride=stride,\n                                      auto=pt,\n                                      transforms=getattr(model.model, 'transforms', None),\n                                      vid_stride=self.args.vid_stride)\n        self.vid_path, self.vid_writer = [None] * bs, [None] * bs\n        model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n\n        self.model = model\n        self.webcam = webcam\n        self.screenshot = screenshot\n        self.imgsz = imgsz\n        self.done_setup = True\n        self.device = device\n        self.return_outputs = return_outputs\n\n        return model\n\n    @smart_inference_mode()\n    def __call__(self, source=None, model=None, return_outputs=True):\n        self.run_callbacks(\"on_predict_start\")\n        model = self.model if self.done_setup else self.setup(source, model, return_outputs)\n        model.eval()\n        self.seen, self.windows, self.dt = 0, [], (ops.Profile(), ops.Profile(), ops.Profile())\n        for batch in self.dataset:\n            self.run_callbacks(\"on_predict_batch_start\")\n            path, im, im0s, vid_cap, s = batch\n            visualize = increment_path(self.save_dir / Path(path).stem, mkdir=True) if self.args.visualize else False\n            with self.dt[0]:\n                im = self.preprocess(im)\n                if len(im.shape) == 3:\n                    im = im[None]  # expand for batch dim\n\n            # Inference\n            with self.dt[1]:\n                preds = model(im, augment=self.args.augment, visualize=visualize)\n\n            # postprocess\n            with self.dt[2]:\n                preds = self.postprocess(preds, im, im0s)\n\n            for i in range(len(im)):\n                if self.webcam:\n                    path, im0s = path[i], im0s[i]\n                p = Path(path)\n                s += self.write_results(i, preds, (p, im, im0s))\n\n                if self.args.show:\n                    self.show(p)\n\n                if self.args.save:\n                    self.save_preds(vid_cap, i, str(self.save_dir / p.name))\n\n            if self.return_outputs:\n                yield self.output\n                self.output.clear()\n\n            # Print time (inference-only)\n            LOGGER.info(f\"{s}{'' if len(preds) else '(no detections), '}{self.dt[1].dt * 1E3:.1f}ms\")\n\n            self.run_callbacks(\"on_predict_batch_end\")\n\n        # Print results\n        t = tuple(x.t / self.seen * 1E3 for x in self.dt)  # speeds per image\n        LOGGER.info(\n            f'Speed: %.1fms pre-process, %.1fms inference, %.1fms postprocess per image at shape {(1, 3, *self.imgsz)}'\n            % t)\n        if self.args.save_txt or self.args.save:\n            s = f\"\\n{len(list(self.save_dir.glob('labels/*.txt')))} labels saved to {self.save_dir / 'labels'}\" if self.args.save_txt else ''\n            LOGGER.info(f\"Results saved to {colorstr('bold', self.save_dir)}{s}\")\n\n        self.run_callbacks(\"on_predict_end\")\n\n    def predict_cli(self, source=None, model=None, return_outputs=False):\n        # as __call__ is a genertor now so have to treat it like a genertor\n        for _ in (self.__call__(source, model, return_outputs)):\n            pass\n\n    def show(self, p):\n        im0 = self.annotator.result()\n        if platform.system() == 'Linux' and p not in self.windows:\n            self.windows.append(p)\n            cv2.namedWindow(str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)  # allow window resize (Linux)\n            cv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])\n        cv2.imshow(str(p), im0)\n        cv2.waitKey(1)  # 1 millisecond\n\n    def save_preds(self, vid_cap, idx, save_path):\n        im0 = self.annotator.result()\n        # save imgs\n        if self.dataset.mode == 'image':\n            cv2.imwrite(save_path, im0)\n        else:  # 'video' or 'stream'\n            if self.vid_path[idx] != save_path:  # new video\n                self.vid_path[idx] = save_path\n                if isinstance(self.vid_writer[idx], cv2.VideoWriter):\n                    self.vid_writer[idx].release()  # release previous video writer\n                if vid_cap:  # video\n                    fps = vid_cap.get(cv2.CAP_PROP_FPS)\n                    w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n                    h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n                else:  # stream\n                    fps, w, h = 30, im0.shape[1], im0.shape[0]\n                save_path = str(Path(save_path).with_suffix('.mp4'))  # force *.mp4 suffix on results videos\n                self.vid_writer[idx] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n            self.vid_writer[idx].write(im0)\n\n    def run_callbacks(self, event: str):\n        for callback in self.callbacks.get(event, []):\n            callback(self)\n</code></pre>"},{"location":"reference/base_pred/#ultralytics.yolo.engine.predictor.BasePredictor.__init__","title":"<code>__init__(config=DEFAULT_CONFIG, overrides=None)</code>","text":"<p>Initializes the BasePredictor class.</p> <p>Parameters:</p>    Name Type Description Default     <code>config</code>  <code>str</code>  <p>Path to a configuration file. Defaults to DEFAULT_CONFIG.</p>  <code>DEFAULT_CONFIG</code>    <code>overrides</code>  <code>dict</code>  <p>Configuration overrides. Defaults to None.</p>  <code>None</code>      Source code in <code>ultralytics/yolo/engine/predictor.py</code> <pre><code>def __init__(self, config=DEFAULT_CONFIG, overrides=None):\n    \"\"\"\n    Initializes the BasePredictor class.\n\n    Args:\n        config (str, optional): Path to a configuration file. Defaults to DEFAULT_CONFIG.\n        overrides (dict, optional): Configuration overrides. Defaults to None.\n    \"\"\"\n    if overrides is None:\n        overrides = {}\n    self.args = get_config(config, overrides)\n    project = self.args.project or Path(SETTINGS['runs_dir']) / self.args.task\n    name = self.args.name or f\"{self.args.mode}\"\n    self.save_dir = increment_path(Path(project) / name, exist_ok=self.args.exist_ok)\n    if self.args.save:\n        (self.save_dir / 'labels' if self.args.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)\n    if self.args.conf is None:\n        self.args.conf = 0.25  # default conf=0.25\n    self.done_setup = False\n\n    # Usable if setup is done\n    self.model = None\n    self.data = self.args.data  # data_dict\n    self.device = None\n    self.dataset = None\n    self.vid_path, self.vid_writer = None, None\n    self.annotator = None\n    self.data_path = None\n    self.output = dict()\n    self.callbacks = defaultdict(list, {k: [v] for k, v in callbacks.default_callbacks.items()})  # add callbacks\n    callbacks.add_integration_callbacks(self)\n</code></pre>"},{"location":"reference/base_trainer/","title":"Trainer","text":"<p>All task Trainers are inherited from <code>BaseTrainer</code> class that contains the model training and optimzation routine boilerplate. You can override any function of these Trainers to suit your needs.</p>"},{"location":"reference/base_trainer/#basetrainer-api-reference","title":"BaseTrainer API Reference","text":"<p>BaseTrainer</p>  <p>A base class for creating trainers.</p>  <p>Attributes:</p>    Name Type Description     <code>args</code>  <code>OmegaConf</code>  <p>Configuration for the trainer.</p>   <code>check_resume</code>  <code>method</code>  <p>Method to check if training should be resumed from a saved checkpoint.</p>   <code>console</code>  <code>logging.Logger</code>  <p>Logger instance.</p>   <code>validator</code>  <code>BaseValidator</code>  <p>Validator instance.</p>   <code>model</code>  <code>nn.Module</code>  <p>Model instance.</p>   <code>callbacks</code>  <code>defaultdict</code>  <p>Dictionary of callbacks.</p>   <code>save_dir</code>  <code>Path</code>  <p>Directory to save results.</p>   <code>wdir</code>  <code>Path</code>  <p>Directory to save weights.</p>   <code>last</code>  <code>Path</code>  <p>Path to last checkpoint.</p>   <code>best</code>  <code>Path</code>  <p>Path to best checkpoint.</p>   <code>batch_size</code>  <code>int</code>  <p>Batch size for training.</p>   <code>epochs</code>  <code>int</code>  <p>Number of epochs to train for.</p>   <code>start_epoch</code>  <code>int</code>  <p>Starting epoch for training.</p>   <code>device</code>  <code>torch.device</code>  <p>Device to use for training.</p>   <code>amp</code>  <code>bool</code>  <p>Flag to enable AMP (Automatic Mixed Precision).</p>   <code>scaler</code>  <code>amp.GradScaler</code>  <p>Gradient scaler for AMP.</p>   <code>data</code>  <code>str</code>  <p>Path to data.</p>   <code>trainset</code>  <code>torch.utils.data.Dataset</code>  <p>Training dataset.</p>   <code>testset</code>  <code>torch.utils.data.Dataset</code>  <p>Testing dataset.</p>   <code>ema</code>  <code>nn.Module</code>  <p>EMA (Exponential Moving Average) of the model.</p>   <code>lf</code>  <code>nn.Module</code>  <p>Loss function.</p>   <code>scheduler</code>  <code>torch.optim.lr_scheduler._LRScheduler</code>  <p>Learning rate scheduler.</p>   <code>best_fitness</code>  <code>float</code>  <p>The best fitness value achieved.</p>   <code>fitness</code>  <code>float</code>  <p>Current fitness value.</p>   <code>loss</code>  <code>float</code>  <p>Current loss value.</p>   <code>tloss</code>  <code>float</code>  <p>Total loss value.</p>   <code>loss_names</code>  <code>list</code>  <p>List of loss names.</p>   <code>csv</code>  <code>Path</code>  <p>Path to results CSV file.</p>     Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>class BaseTrainer:\n    \"\"\"\n    BaseTrainer\n\n    &gt; A base class for creating trainers.\n\n    Attributes:\n        args (OmegaConf): Configuration for the trainer.\n        check_resume (method): Method to check if training should be resumed from a saved checkpoint.\n        console (logging.Logger): Logger instance.\n        validator (BaseValidator): Validator instance.\n        model (nn.Module): Model instance.\n        callbacks (defaultdict): Dictionary of callbacks.\n        save_dir (Path): Directory to save results.\n        wdir (Path): Directory to save weights.\n        last (Path): Path to last checkpoint.\n        best (Path): Path to best checkpoint.\n        batch_size (int): Batch size for training.\n        epochs (int): Number of epochs to train for.\n        start_epoch (int): Starting epoch for training.\n        device (torch.device): Device to use for training.\n        amp (bool): Flag to enable AMP (Automatic Mixed Precision).\n        scaler (amp.GradScaler): Gradient scaler for AMP.\n        data (str): Path to data.\n        trainset (torch.utils.data.Dataset): Training dataset.\n        testset (torch.utils.data.Dataset): Testing dataset.\n        ema (nn.Module): EMA (Exponential Moving Average) of the model.\n        lf (nn.Module): Loss function.\n        scheduler (torch.optim.lr_scheduler._LRScheduler): Learning rate scheduler.\n        best_fitness (float): The best fitness value achieved.\n        fitness (float): Current fitness value.\n        loss (float): Current loss value.\n        tloss (float): Total loss value.\n        loss_names (list): List of loss names.\n        csv (Path): Path to results CSV file.\n    \"\"\"\n\n    def __init__(self, config=DEFAULT_CONFIG, overrides=None):\n        \"\"\"\n        &gt; Initializes the BaseTrainer class.\n\n        Args:\n            config (str, optional): Path to a configuration file. Defaults to DEFAULT_CONFIG.\n            overrides (dict, optional): Configuration overrides. Defaults to None.\n        \"\"\"\n        if overrides is None:\n            overrides = {}\n        self.args = get_config(config, overrides)\n        self.check_resume()\n        self.console = LOGGER\n        self.validator = None\n        self.model = None\n        self.callbacks = defaultdict(list)\n        init_seeds(self.args.seed + 1 + RANK, deterministic=self.args.deterministic)\n\n        # Dirs\n        project = self.args.project or Path(SETTINGS['runs_dir']) / self.args.task\n        name = self.args.name or f\"{self.args.mode}\"\n        self.save_dir = Path(\n            self.args.get(\n                \"save_dir\",\n                increment_path(Path(project) / name, exist_ok=self.args.exist_ok if RANK in {-1, 0} else True)))\n        self.wdir = self.save_dir / 'weights'  # weights dir\n        if RANK in {-1, 0}:\n            self.wdir.mkdir(parents=True, exist_ok=True)  # make dir\n            with open_dict(self.args):\n                self.args.save_dir = str(self.save_dir)\n            yaml_save(self.save_dir / 'args.yaml', OmegaConf.to_container(self.args, resolve=True))  # save run args\n        self.last, self.best = self.wdir / 'last.pt', self.wdir / 'best.pt'  # checkpoint paths\n\n        self.batch_size = self.args.batch\n        self.epochs = self.args.epochs\n        self.start_epoch = 0\n        if RANK == -1:\n            print_args(dict(self.args))\n\n        # Device\n        self.device = utils.torch_utils.select_device(self.args.device, self.batch_size)\n        self.amp = self.device.type != 'cpu'\n        self.scaler = amp.GradScaler(enabled=self.amp)\n        if self.device.type == 'cpu':\n            self.args.workers = 0  # faster CPU training as time dominated by inference, not dataloading\n\n        # Model and Dataloaders.\n        self.model = self.args.model\n        self.data = self.args.data\n        if self.data.endswith(\".yaml\"):\n            self.data = check_dataset_yaml(self.data)\n        else:\n            self.data = check_dataset(self.data)\n        self.trainset, self.testset = self.get_dataset(self.data)\n        self.ema = None\n\n        # Optimization utils init\n        self.lf = None\n        self.scheduler = None\n\n        # Epoch level metrics\n        self.best_fitness = None\n        self.fitness = None\n        self.loss = None\n        self.tloss = None\n        self.loss_names = ['Loss']\n        self.csv = self.save_dir / 'results.csv'\n        self.plot_idx = [0, 1, 2]\n\n        # Callbacks\n        self.callbacks = defaultdict(list, {k: [v] for k, v in callbacks.default_callbacks.items()})  # add callbacks\n        if RANK in {0, -1}:\n            callbacks.add_integration_callbacks(self)\n\n    def add_callback(self, event: str, callback):\n        \"\"\"\n        &gt; Appends the given callback.\n        \"\"\"\n        self.callbacks[event].append(callback)\n\n    def set_callback(self, event: str, callback):\n        \"\"\"\n        &gt; Overrides the existing callbacks with the given callback.\n        \"\"\"\n        self.callbacks[event] = [callback]\n\n    def run_callbacks(self, event: str):\n        for callback in self.callbacks.get(event, []):\n            callback(self)\n\n    def train(self):\n        world_size = torch.cuda.device_count()\n        if world_size &gt; 1 and \"LOCAL_RANK\" not in os.environ:\n            command = generate_ddp_command(world_size, self)\n            try:\n                subprocess.run(command)\n            except Exception as e:\n                self.console(e)\n            finally:\n                ddp_cleanup(command, self)\n        else:\n            self._do_train(int(os.getenv(\"RANK\", -1)), world_size)\n\n    def _setup_ddp(self, rank, world_size):\n        # os.environ['MASTER_ADDR'] = 'localhost'\n        # os.environ['MASTER_PORT'] = '9020'\n        torch.cuda.set_device(rank)\n        self.device = torch.device('cuda', rank)\n        self.console.info(f\"DDP settings: RANK {rank}, WORLD_SIZE {world_size}, DEVICE {self.device}\")\n        dist.init_process_group(\"nccl\" if dist.is_nccl_available() else \"gloo\", rank=rank, world_size=world_size)\n\n    def _setup_train(self, rank, world_size):\n        \"\"\"\n        &gt; Builds dataloaders and optimizer on correct rank process.\n        \"\"\"\n        # model\n        self.run_callbacks(\"on_pretrain_routine_start\")\n        ckpt = self.setup_model()\n        self.model = self.model.to(self.device)\n        self.set_model_attributes()\n        if world_size &gt; 1:\n            self.model = DDP(self.model, device_ids=[rank])\n\n        # Batch size\n        if self.batch_size == -1:\n            if RANK == -1:  # single-GPU only, estimate best batch size\n                self.batch_size = check_train_batch_size(self.model, self.args.imgsz, self.amp)\n            else:\n                SyntaxError('batch=-1 to use AutoBatch is only available in Single-GPU training. '\n                            'Please pass a valid batch size value for Multi-GPU DDP training, i.e. batch=16')\n\n        # Optimizer\n        self.accumulate = max(round(self.args.nbs / self.batch_size), 1)  # accumulate loss before optimizing\n        self.args.weight_decay *= self.batch_size * self.accumulate / self.args.nbs  # scale weight_decay\n        self.optimizer = self.build_optimizer(model=self.model,\n                                              name=self.args.optimizer,\n                                              lr=self.args.lr0,\n                                              momentum=self.args.momentum,\n                                              decay=self.args.weight_decay)\n        # Scheduler\n        if self.args.cos_lr:\n            self.lf = one_cycle(1, self.args.lrf, self.epochs)  # cosine 1-&gt;hyp['lrf']\n        else:\n            self.lf = lambda x: (1 - x / self.epochs) * (1.0 - self.args.lrf) + self.args.lrf  # linear\n        self.scheduler = lr_scheduler.LambdaLR(self.optimizer, lr_lambda=self.lf)\n        self.scheduler.last_epoch = self.start_epoch - 1  # do not move\n\n        # dataloaders\n        batch_size = self.batch_size // world_size if world_size &gt; 1 else self.batch_size\n        self.train_loader = self.get_dataloader(self.trainset, batch_size=batch_size, rank=rank, mode=\"train\")\n        if rank in {0, -1}:\n            self.test_loader = self.get_dataloader(self.testset, batch_size=batch_size * 2, rank=-1, mode=\"val\")\n            self.validator = self.get_validator()\n            metric_keys = self.validator.metrics.keys + self.label_loss_items(prefix=\"val\")\n            self.metrics = dict(zip(metric_keys, [0] * len(metric_keys)))  # TODO: init metrics for plot_results()?\n            self.ema = ModelEMA(self.model)\n        self.resume_training(ckpt)\n        self.run_callbacks(\"on_pretrain_routine_end\")\n\n    def _do_train(self, rank=-1, world_size=1):\n        if world_size &gt; 1:\n            self._setup_ddp(rank, world_size)\n\n        self._setup_train(rank, world_size)\n\n        self.epoch_time = None\n        self.epoch_time_start = time.time()\n        self.train_time_start = time.time()\n        nb = len(self.train_loader)  # number of batches\n        nw = max(round(self.args.warmup_epochs * nb), 100)  # number of warmup iterations\n        last_opt_step = -1\n        self.run_callbacks(\"on_train_start\")\n        self.log(f\"Image sizes {self.args.imgsz} train, {self.args.imgsz} val\\n\"\n                 f'Using {self.train_loader.num_workers * (world_size or 1)} dataloader workers\\n'\n                 f\"Logging results to {colorstr('bold', self.save_dir)}\\n\"\n                 f\"Starting training for {self.epochs} epochs...\")\n        if self.args.close_mosaic:\n            base_idx = (self.epochs - self.args.close_mosaic) * nb\n            self.plot_idx.extend([base_idx, base_idx + 1, base_idx + 2])\n        for epoch in range(self.start_epoch, self.epochs):\n            self.epoch = epoch\n            self.run_callbacks(\"on_train_epoch_start\")\n            self.model.train()\n            if rank != -1:\n                self.train_loader.sampler.set_epoch(epoch)\n            pbar = enumerate(self.train_loader)\n            # Update dataloader attributes (optional)\n            if epoch == (self.epochs - self.args.close_mosaic):\n                self.console.info(\"Closing dataloader mosaic\")\n                if hasattr(self.train_loader.dataset, 'mosaic'):\n                    self.train_loader.dataset.mosaic = False\n                if hasattr(self.train_loader.dataset, 'close_mosaic'):\n                    self.train_loader.dataset.close_mosaic(hyp=self.args)\n\n            if rank in {-1, 0}:\n                self.console.info(self.progress_string())\n                pbar = tqdm(enumerate(self.train_loader), total=nb, bar_format=TQDM_BAR_FORMAT)\n            self.tloss = None\n            self.optimizer.zero_grad()\n            for i, batch in pbar:\n                self.run_callbacks(\"on_train_batch_start\")\n                # Warmup\n                ni = i + nb * epoch\n                if ni &lt;= nw:\n                    xi = [0, nw]  # x interp\n                    self.accumulate = max(1, np.interp(ni, xi, [1, self.args.nbs / self.batch_size]).round())\n                    for j, x in enumerate(self.optimizer.param_groups):\n                        # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n                        x['lr'] = np.interp(\n                            ni, xi, [self.args.warmup_bias_lr if j == 0 else 0.0, x['initial_lr'] * self.lf(epoch)])\n                        if 'momentum' in x:\n                            x['momentum'] = np.interp(ni, xi, [self.args.warmup_momentum, self.args.momentum])\n\n                # Forward\n                with torch.cuda.amp.autocast(self.amp):\n                    batch = self.preprocess_batch(batch)\n                    preds = self.model(batch[\"img\"])\n                    self.loss, self.loss_items = self.criterion(preds, batch)\n                    if rank != -1:\n                        self.loss *= world_size\n                    self.tloss = (self.tloss * i + self.loss_items) / (i + 1) if self.tloss is not None \\\n                        else self.loss_items\n\n                # Backward\n                self.scaler.scale(self.loss).backward()\n\n                # Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\n                if ni - last_opt_step &gt;= self.accumulate:\n                    self.optimizer_step()\n                    last_opt_step = ni\n\n                # Log\n                mem = f'{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G'  # (GB)\n                loss_len = self.tloss.shape[0] if len(self.tloss.size()) else 1\n                losses = self.tloss if loss_len &gt; 1 else torch.unsqueeze(self.tloss, 0)\n                if rank in {-1, 0}:\n                    pbar.set_description(\n                        ('%11s' * 2 + '%11.4g' * (2 + loss_len)) %\n                        (f'{epoch + 1}/{self.epochs}', mem, *losses, batch[\"cls\"].shape[0], batch[\"img\"].shape[-1]))\n                    self.run_callbacks('on_batch_end')\n                    if self.args.plots and ni in self.plot_idx:\n                        self.plot_training_samples(batch, ni)\n\n                self.run_callbacks(\"on_train_batch_end\")\n\n            self.lr = {f\"lr/pg{ir}\": x['lr'] for ir, x in enumerate(self.optimizer.param_groups)}  # for loggers\n\n            self.scheduler.step()\n            self.run_callbacks(\"on_train_epoch_end\")\n\n            if rank in {-1, 0}:\n\n                # Validation\n                self.ema.update_attr(self.model, include=['yaml', 'nc', 'args', 'names', 'stride', 'class_weights'])\n                final_epoch = (epoch + 1 == self.epochs)\n                if self.args.val or final_epoch:\n                    self.metrics, self.fitness = self.validate()\n                self.save_metrics(metrics={**self.label_loss_items(self.tloss), **self.metrics, **self.lr})\n\n                # Save model\n                if self.args.save or (epoch + 1 == self.epochs):\n                    self.save_model()\n                    self.run_callbacks('on_model_save')\n\n            tnow = time.time()\n            self.epoch_time = tnow - self.epoch_time_start\n            self.epoch_time_start = tnow\n            self.run_callbacks(\"on_fit_epoch_end\")\n            # TODO: termination condition\n\n        if rank in {-1, 0}:\n            # Do final val with best.pt\n            self.log(f'\\n{epoch - self.start_epoch + 1} epochs completed in '\n                     f'{(time.time() - self.train_time_start) / 3600:.3f} hours.')\n            self.final_eval()\n            if self.args.plots:\n                self.plot_metrics()\n            self.log(f\"Results saved to {colorstr('bold', self.save_dir)}\")\n            self.run_callbacks('on_train_end')\n        torch.cuda.empty_cache()\n        self.run_callbacks('teardown')\n\n    def save_model(self):\n        ckpt = {\n            'epoch': self.epoch,\n            'best_fitness': self.best_fitness,\n            'model': deepcopy(de_parallel(self.model)).half(),\n            'ema': deepcopy(self.ema.ema).half(),\n            'updates': self.ema.updates,\n            'optimizer': self.optimizer.state_dict(),\n            'train_args': self.args,\n            'date': datetime.now().isoformat(),\n            'version': __version__}\n\n        # Save last, best and delete\n        torch.save(ckpt, self.last)\n        if self.best_fitness == self.fitness:\n            torch.save(ckpt, self.best)\n        del ckpt\n\n    def get_dataset(self, data):\n        \"\"\"\n        &gt; Get train, val path from data dict if it exists. Returns None if data format is not recognized.\n        \"\"\"\n        return data[\"train\"], data.get(\"val\") or data.get(\"test\")\n\n    def setup_model(self):\n        \"\"\"\n        &gt; load/create/download model for any task.\n        \"\"\"\n        if isinstance(self.model, torch.nn.Module):  # if model is loaded beforehand. No setup needed\n            return\n\n        model, weights = self.model, None\n        ckpt = None\n        if str(model).endswith(\".pt\"):\n            weights, ckpt = attempt_load_one_weight(model)\n            cfg = ckpt[\"model\"].yaml\n        else:\n            cfg = model\n        self.model = self.get_model(cfg=cfg, weights=weights)  # calls Model(cfg, weights)\n        return ckpt\n\n    def optimizer_step(self):\n        self.scaler.unscale_(self.optimizer)  # unscale gradients\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10.0)  # clip gradients\n        self.scaler.step(self.optimizer)\n        self.scaler.update()\n        self.optimizer.zero_grad()\n        if self.ema:\n            self.ema.update(self.model)\n\n    def preprocess_batch(self, batch):\n        \"\"\"\n        &gt; Allows custom preprocessing model inputs and ground truths depending on task type.\n        \"\"\"\n        return batch\n\n    def validate(self):\n        \"\"\"\n        &gt; Runs validation on test set using self.validator. The returned dict is expected to contain \"fitness\" key.\n        \"\"\"\n        metrics = self.validator(self)\n        fitness = metrics.pop(\"fitness\", -self.loss.detach().cpu().numpy())  # use loss as fitness measure if not found\n        if not self.best_fitness or self.best_fitness &lt; fitness:\n            self.best_fitness = fitness\n        return metrics, fitness\n\n    def log(self, text, rank=-1):\n        \"\"\"\n        &gt; Logs the given text to given ranks process if provided, otherwise logs to all ranks.\n\n        Args\"\n            text (str): text to log\n            rank (List[Int]): process rank\n\n        \"\"\"\n        if rank in {-1, 0}:\n            self.console.info(text)\n\n    def get_model(self, cfg=None, weights=None, verbose=True):\n        raise NotImplementedError(\"This task trainer doesn't support loading cfg files\")\n\n    def get_validator(self):\n        raise NotImplementedError(\"get_validator function not implemented in trainer\")\n\n    def get_dataloader(self, dataset_path, batch_size=16, rank=0):\n        \"\"\"\n        &gt; Returns dataloader derived from torch.data.Dataloader.\n        \"\"\"\n        raise NotImplementedError(\"get_dataloader function not implemented in trainer\")\n\n    def criterion(self, preds, batch):\n        \"\"\"\n        &gt; Returns loss and individual loss items as Tensor.\n        \"\"\"\n        raise NotImplementedError(\"criterion function not implemented in trainer\")\n\n    def label_loss_items(self, loss_items=None, prefix=\"train\"):\n        \"\"\"\n        Returns a loss dict with labelled training loss items tensor\n        \"\"\"\n        # Not needed for classification but necessary for segmentation &amp; detection\n        return {\"loss\": loss_items} if loss_items is not None else [\"loss\"]\n\n    def set_model_attributes(self):\n        \"\"\"\n        To set or update model parameters before training.\n        \"\"\"\n        self.model.names = self.data[\"names\"]\n\n    def build_targets(self, preds, targets):\n        pass\n\n    def progress_string(self):\n        return \"\"\n\n    # TODO: may need to put these following functions into callback\n    def plot_training_samples(self, batch, ni):\n        pass\n\n    def save_metrics(self, metrics):\n        keys, vals = list(metrics.keys()), list(metrics.values())\n        n = len(metrics) + 1  # number of cols\n        s = '' if self.csv.exists() else (('%23s,' * n % tuple(['epoch'] + keys)).rstrip(',') + '\\n')  # header\n        with open(self.csv, 'a') as f:\n            f.write(s + ('%23.5g,' * n % tuple([self.epoch] + vals)).rstrip(',') + '\\n')\n\n    def plot_metrics(self):\n        pass\n\n    def final_eval(self):\n        for f in self.last, self.best:\n            if f.exists():\n                strip_optimizer(f)  # strip optimizers\n                if f is self.best:\n                    self.console.info(f'\\nValidating {f}...')\n                    self.validator.args.save_json = True\n                    self.metrics = self.validator(model=f)\n                    self.metrics.pop('fitness', None)\n                    self.run_callbacks('on_fit_epoch_end')\n\n    def check_resume(self):\n        resume = self.args.resume\n        if resume:\n            last = Path(check_file(resume) if isinstance(resume, str) else get_latest_run())\n            args_yaml = last.parent.parent / 'args.yaml'  # train options yaml\n            if args_yaml.is_file():\n                args = get_config(args_yaml)  # replace\n            args.model, resume = str(last), True  # reinstate\n            self.args = args\n        self.resume = resume\n\n    def resume_training(self, ckpt):\n        if ckpt is None:\n            return\n        best_fitness = 0.0\n        start_epoch = ckpt['epoch'] + 1\n        if ckpt['optimizer'] is not None:\n            self.optimizer.load_state_dict(ckpt['optimizer'])  # optimizer\n            best_fitness = ckpt['best_fitness']\n        if self.ema and ckpt.get('ema'):\n            self.ema.ema.load_state_dict(ckpt['ema'].float().state_dict())  # EMA\n            self.ema.updates = ckpt['updates']\n        if self.resume:\n            assert start_epoch &gt; 0, \\\n                f'{self.args.model} training to {self.epochs} epochs is finished, nothing to resume.\\n' \\\n                f\"Start a new training without --resume, i.e. 'yolo task=... mode=train model={self.args.model}'\"\n            LOGGER.info(\n                f'Resuming training from {self.args.model} from epoch {start_epoch} to {self.epochs} total epochs')\n        if self.epochs &lt; start_epoch:\n            LOGGER.info(\n                f\"{self.model} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {self.epochs} more epochs.\")\n            self.epochs += ckpt['epoch']  # finetune additional epochs\n        self.best_fitness = best_fitness\n        self.start_epoch = start_epoch\n\n    @staticmethod\n    def build_optimizer(model, name='Adam', lr=0.001, momentum=0.9, decay=1e-5):\n        \"\"\"\n        &gt; Builds an optimizer with the specified parameters and parameter groups.\n\n        Args:\n            model (nn.Module): model to optimize\n            name (str): name of the optimizer to use\n            lr (float): learning rate\n            momentum (float): momentum\n            decay (float): weight decay\n\n        Returns:\n            optimizer (torch.optim.Optimizer): the built optimizer\n        \"\"\"\n        g = [], [], []  # optimizer parameter groups\n        bn = tuple(v for k, v in nn.__dict__.items() if 'Norm' in k)  # normalization layers, i.e. BatchNorm2d()\n        for v in model.modules():\n            if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias (no decay)\n                g[2].append(v.bias)\n            if isinstance(v, bn):  # weight (no decay)\n                g[1].append(v.weight)\n            elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)\n                g[0].append(v.weight)\n\n        if name == 'Adam':\n            optimizer = torch.optim.Adam(g[2], lr=lr, betas=(momentum, 0.999))  # adjust beta1 to momentum\n        elif name == 'AdamW':\n            optimizer = torch.optim.AdamW(g[2], lr=lr, betas=(momentum, 0.999), weight_decay=0.0)\n        elif name == 'RMSProp':\n            optimizer = torch.optim.RMSprop(g[2], lr=lr, momentum=momentum)\n        elif name == 'SGD':\n            optimizer = torch.optim.SGD(g[2], lr=lr, momentum=momentum, nesterov=True)\n        else:\n            raise NotImplementedError(f'Optimizer {name} not implemented.')\n\n        optimizer.add_param_group({'params': g[0], 'weight_decay': decay})  # add g0 with weight_decay\n        optimizer.add_param_group({'params': g[1], 'weight_decay': 0.0})  # add g1 (BatchNorm2d weights)\n        LOGGER.info(f\"{colorstr('optimizer:')} {type(optimizer).__name__}(lr={lr}) with parameter groups \"\n                    f\"{len(g[1])} weight(decay=0.0), {len(g[0])} weight(decay={decay}), {len(g[2])} bias\")\n        return optimizer\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.__init__","title":"<code>__init__(config=DEFAULT_CONFIG, overrides=None)</code>","text":"<p>Initializes the BaseTrainer class.</p>  <p>Parameters:</p>    Name Type Description Default     <code>config</code>  <code>str</code>  <p>Path to a configuration file. Defaults to DEFAULT_CONFIG.</p>  <code>DEFAULT_CONFIG</code>    <code>overrides</code>  <code>dict</code>  <p>Configuration overrides. Defaults to None.</p>  <code>None</code>      Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def __init__(self, config=DEFAULT_CONFIG, overrides=None):\n    \"\"\"\n    &gt; Initializes the BaseTrainer class.\n\n    Args:\n        config (str, optional): Path to a configuration file. Defaults to DEFAULT_CONFIG.\n        overrides (dict, optional): Configuration overrides. Defaults to None.\n    \"\"\"\n    if overrides is None:\n        overrides = {}\n    self.args = get_config(config, overrides)\n    self.check_resume()\n    self.console = LOGGER\n    self.validator = None\n    self.model = None\n    self.callbacks = defaultdict(list)\n    init_seeds(self.args.seed + 1 + RANK, deterministic=self.args.deterministic)\n\n    # Dirs\n    project = self.args.project or Path(SETTINGS['runs_dir']) / self.args.task\n    name = self.args.name or f\"{self.args.mode}\"\n    self.save_dir = Path(\n        self.args.get(\n            \"save_dir\",\n            increment_path(Path(project) / name, exist_ok=self.args.exist_ok if RANK in {-1, 0} else True)))\n    self.wdir = self.save_dir / 'weights'  # weights dir\n    if RANK in {-1, 0}:\n        self.wdir.mkdir(parents=True, exist_ok=True)  # make dir\n        with open_dict(self.args):\n            self.args.save_dir = str(self.save_dir)\n        yaml_save(self.save_dir / 'args.yaml', OmegaConf.to_container(self.args, resolve=True))  # save run args\n    self.last, self.best = self.wdir / 'last.pt', self.wdir / 'best.pt'  # checkpoint paths\n\n    self.batch_size = self.args.batch\n    self.epochs = self.args.epochs\n    self.start_epoch = 0\n    if RANK == -1:\n        print_args(dict(self.args))\n\n    # Device\n    self.device = utils.torch_utils.select_device(self.args.device, self.batch_size)\n    self.amp = self.device.type != 'cpu'\n    self.scaler = amp.GradScaler(enabled=self.amp)\n    if self.device.type == 'cpu':\n        self.args.workers = 0  # faster CPU training as time dominated by inference, not dataloading\n\n    # Model and Dataloaders.\n    self.model = self.args.model\n    self.data = self.args.data\n    if self.data.endswith(\".yaml\"):\n        self.data = check_dataset_yaml(self.data)\n    else:\n        self.data = check_dataset(self.data)\n    self.trainset, self.testset = self.get_dataset(self.data)\n    self.ema = None\n\n    # Optimization utils init\n    self.lf = None\n    self.scheduler = None\n\n    # Epoch level metrics\n    self.best_fitness = None\n    self.fitness = None\n    self.loss = None\n    self.tloss = None\n    self.loss_names = ['Loss']\n    self.csv = self.save_dir / 'results.csv'\n    self.plot_idx = [0, 1, 2]\n\n    # Callbacks\n    self.callbacks = defaultdict(list, {k: [v] for k, v in callbacks.default_callbacks.items()})  # add callbacks\n    if RANK in {0, -1}:\n        callbacks.add_integration_callbacks(self)\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.add_callback","title":"<code>add_callback(event, callback)</code>","text":"<p>Appends the given callback.</p>   Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def add_callback(self, event: str, callback):\n    \"\"\"\n    &gt; Appends the given callback.\n    \"\"\"\n    self.callbacks[event].append(callback)\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.build_optimizer","title":"<code>build_optimizer(model, name='Adam', lr=0.001, momentum=0.9, decay=1e-05)</code>  <code>staticmethod</code>","text":"<p>Builds an optimizer with the specified parameters and parameter groups.</p>  <p>Parameters:</p>    Name Type Description Default     <code>model</code>  <code>nn.Module</code>  <p>model to optimize</p>  required    <code>name</code>  <code>str</code>  <p>name of the optimizer to use</p>  <code>'Adam'</code>    <code>lr</code>  <code>float</code>  <p>learning rate</p>  <code>0.001</code>    <code>momentum</code>  <code>float</code>  <p>momentum</p>  <code>0.9</code>    <code>decay</code>  <code>float</code>  <p>weight decay</p>  <code>1e-05</code>     <p>Returns:</p>    Name Type Description     <code>optimizer</code>  <code>torch.optim.Optimizer</code>  <p>the built optimizer</p>     Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>@staticmethod\ndef build_optimizer(model, name='Adam', lr=0.001, momentum=0.9, decay=1e-5):\n    \"\"\"\n    &gt; Builds an optimizer with the specified parameters and parameter groups.\n\n    Args:\n        model (nn.Module): model to optimize\n        name (str): name of the optimizer to use\n        lr (float): learning rate\n        momentum (float): momentum\n        decay (float): weight decay\n\n    Returns:\n        optimizer (torch.optim.Optimizer): the built optimizer\n    \"\"\"\n    g = [], [], []  # optimizer parameter groups\n    bn = tuple(v for k, v in nn.__dict__.items() if 'Norm' in k)  # normalization layers, i.e. BatchNorm2d()\n    for v in model.modules():\n        if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias (no decay)\n            g[2].append(v.bias)\n        if isinstance(v, bn):  # weight (no decay)\n            g[1].append(v.weight)\n        elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)\n            g[0].append(v.weight)\n\n    if name == 'Adam':\n        optimizer = torch.optim.Adam(g[2], lr=lr, betas=(momentum, 0.999))  # adjust beta1 to momentum\n    elif name == 'AdamW':\n        optimizer = torch.optim.AdamW(g[2], lr=lr, betas=(momentum, 0.999), weight_decay=0.0)\n    elif name == 'RMSProp':\n        optimizer = torch.optim.RMSprop(g[2], lr=lr, momentum=momentum)\n    elif name == 'SGD':\n        optimizer = torch.optim.SGD(g[2], lr=lr, momentum=momentum, nesterov=True)\n    else:\n        raise NotImplementedError(f'Optimizer {name} not implemented.')\n\n    optimizer.add_param_group({'params': g[0], 'weight_decay': decay})  # add g0 with weight_decay\n    optimizer.add_param_group({'params': g[1], 'weight_decay': 0.0})  # add g1 (BatchNorm2d weights)\n    LOGGER.info(f\"{colorstr('optimizer:')} {type(optimizer).__name__}(lr={lr}) with parameter groups \"\n                f\"{len(g[1])} weight(decay=0.0), {len(g[0])} weight(decay={decay}), {len(g[2])} bias\")\n    return optimizer\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.criterion","title":"<code>criterion(preds, batch)</code>","text":"<p>Returns loss and individual loss items as Tensor.</p>   Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def criterion(self, preds, batch):\n    \"\"\"\n    &gt; Returns loss and individual loss items as Tensor.\n    \"\"\"\n    raise NotImplementedError(\"criterion function not implemented in trainer\")\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.get_dataloader","title":"<code>get_dataloader(dataset_path, batch_size=16, rank=0)</code>","text":"<p>Returns dataloader derived from torch.data.Dataloader.</p>   Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def get_dataloader(self, dataset_path, batch_size=16, rank=0):\n    \"\"\"\n    &gt; Returns dataloader derived from torch.data.Dataloader.\n    \"\"\"\n    raise NotImplementedError(\"get_dataloader function not implemented in trainer\")\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.get_dataset","title":"<code>get_dataset(data)</code>","text":"<p>Get train, val path from data dict if it exists. Returns None if data format is not recognized.</p>   Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def get_dataset(self, data):\n    \"\"\"\n    &gt; Get train, val path from data dict if it exists. Returns None if data format is not recognized.\n    \"\"\"\n    return data[\"train\"], data.get(\"val\") or data.get(\"test\")\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.label_loss_items","title":"<code>label_loss_items(loss_items=None, prefix='train')</code>","text":"<p>Returns a loss dict with labelled training loss items tensor</p>  Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def label_loss_items(self, loss_items=None, prefix=\"train\"):\n    \"\"\"\n    Returns a loss dict with labelled training loss items tensor\n    \"\"\"\n    # Not needed for classification but necessary for segmentation &amp; detection\n    return {\"loss\": loss_items} if loss_items is not None else [\"loss\"]\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.log","title":"<code>log(text, rank=-1)</code>","text":"<p>Logs the given text to given ranks process if provided, otherwise logs to all ranks.</p>  <p>Args\"     text (str): text to log     rank (List[Int]): process rank</p>  Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def log(self, text, rank=-1):\n    \"\"\"\n    &gt; Logs the given text to given ranks process if provided, otherwise logs to all ranks.\n\n    Args\"\n        text (str): text to log\n        rank (List[Int]): process rank\n\n    \"\"\"\n    if rank in {-1, 0}:\n        self.console.info(text)\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.preprocess_batch","title":"<code>preprocess_batch(batch)</code>","text":"<p>Allows custom preprocessing model inputs and ground truths depending on task type.</p>   Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def preprocess_batch(self, batch):\n    \"\"\"\n    &gt; Allows custom preprocessing model inputs and ground truths depending on task type.\n    \"\"\"\n    return batch\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.set_callback","title":"<code>set_callback(event, callback)</code>","text":"<p>Overrides the existing callbacks with the given callback.</p>   Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def set_callback(self, event: str, callback):\n    \"\"\"\n    &gt; Overrides the existing callbacks with the given callback.\n    \"\"\"\n    self.callbacks[event] = [callback]\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.set_model_attributes","title":"<code>set_model_attributes()</code>","text":"<p>To set or update model parameters before training.</p>  Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def set_model_attributes(self):\n    \"\"\"\n    To set or update model parameters before training.\n    \"\"\"\n    self.model.names = self.data[\"names\"]\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.setup_model","title":"<code>setup_model()</code>","text":"<p>load/create/download model for any task.</p>   Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def setup_model(self):\n    \"\"\"\n    &gt; load/create/download model for any task.\n    \"\"\"\n    if isinstance(self.model, torch.nn.Module):  # if model is loaded beforehand. No setup needed\n        return\n\n    model, weights = self.model, None\n    ckpt = None\n    if str(model).endswith(\".pt\"):\n        weights, ckpt = attempt_load_one_weight(model)\n        cfg = ckpt[\"model\"].yaml\n    else:\n        cfg = model\n    self.model = self.get_model(cfg=cfg, weights=weights)  # calls Model(cfg, weights)\n    return ckpt\n</code></pre>"},{"location":"reference/base_trainer/#ultralytics.yolo.engine.trainer.BaseTrainer.validate","title":"<code>validate()</code>","text":"<p>Runs validation on test set using self.validator. The returned dict is expected to contain \"fitness\" key.</p>   Source code in <code>ultralytics/yolo/engine/trainer.py</code> <pre><code>def validate(self):\n    \"\"\"\n    &gt; Runs validation on test set using self.validator. The returned dict is expected to contain \"fitness\" key.\n    \"\"\"\n    metrics = self.validator(self)\n    fitness = metrics.pop(\"fitness\", -self.loss.detach().cpu().numpy())  # use loss as fitness measure if not found\n    if not self.best_fitness or self.best_fitness &lt; fitness:\n        self.best_fitness = fitness\n    return metrics, fitness\n</code></pre>"},{"location":"reference/base_val/","title":"Validator","text":"<p>All task Validators are inherited from <code>BaseValidator</code> class that contains the model validation routine boilerplate. You can override any function of these Trainers to suit your needs.</p>"},{"location":"reference/base_val/#basevalidator-api-reference","title":"BaseValidator API Reference","text":"<p>BaseValidator</p> <p>A base class for creating validators.</p> <p>Attributes:</p>    Name Type Description     <code>dataloader</code>  <code>DataLoader</code>  <p>Dataloader to use for validation.</p>   <code>pbar</code>  <code>tqdm</code>  <p>Progress bar to update during validation.</p>   <code>logger</code>  <code>logging.Logger</code>  <p>Logger to use for validation.</p>   <code>args</code>  <code>OmegaConf</code>  <p>Configuration for the validator.</p>   <code>model</code>  <code>nn.Module</code>  <p>Model to validate.</p>   <code>data</code>  <code>dict</code>  <p>Data dictionary.</p>   <code>device</code>  <code>torch.device</code>  <p>Device to use for validation.</p>   <code>batch_i</code>  <code>int</code>  <p>Current batch index.</p>   <code>training</code>  <code>bool</code>  <p>Whether the model is in training mode.</p>   <code>speed</code>  <code>float</code>  <p>Batch processing speed in seconds.</p>   <code>jdict</code>  <code>dict</code>  <p>Dictionary to store validation results.</p>   <code>save_dir</code>  <code>Path</code>  <p>Directory to save results.</p>     Source code in <code>ultralytics/yolo/engine/validator.py</code> <pre><code>class BaseValidator:\n    \"\"\"\n    BaseValidator\n\n    A base class for creating validators.\n\n    Attributes:\n        dataloader (DataLoader): Dataloader to use for validation.\n        pbar (tqdm): Progress bar to update during validation.\n        logger (logging.Logger): Logger to use for validation.\n        args (OmegaConf): Configuration for the validator.\n        model (nn.Module): Model to validate.\n        data (dict): Data dictionary.\n        device (torch.device): Device to use for validation.\n        batch_i (int): Current batch index.\n        training (bool): Whether the model is in training mode.\n        speed (float): Batch processing speed in seconds.\n        jdict (dict): Dictionary to store validation results.\n        save_dir (Path): Directory to save results.\n    \"\"\"\n\n    def __init__(self, dataloader=None, save_dir=None, pbar=None, logger=None, args=None):\n        \"\"\"\n        Initializes a BaseValidator instance.\n\n        Args:\n            dataloader (torch.utils.data.DataLoader): Dataloader to be used for validation.\n            save_dir (Path): Directory to save results.\n            pbar (tqdm.tqdm): Progress bar for displaying progress.\n            logger (logging.Logger): Logger to log messages.\n            args (OmegaConf): Configuration for the validator.\n        \"\"\"\n        self.dataloader = dataloader\n        self.pbar = pbar\n        self.logger = logger or LOGGER\n        self.args = args or OmegaConf.load(DEFAULT_CONFIG)\n        self.model = None\n        self.data = None\n        self.device = None\n        self.batch_i = None\n        self.training = True\n        self.speed = None\n        self.jdict = None\n\n        project = self.args.project or Path(SETTINGS['runs_dir']) / self.args.task\n        name = self.args.name or f\"{self.args.mode}\"\n        self.save_dir = save_dir or increment_path(Path(project) / name,\n                                                   exist_ok=self.args.exist_ok if RANK in {-1, 0} else True)\n        (self.save_dir / 'labels' if self.args.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)\n\n        if self.args.conf is None:\n            self.args.conf = 0.001  # default conf=0.001\n\n        self.callbacks = defaultdict(list, {k: [v] for k, v in callbacks.default_callbacks.items()})  # add callbacks\n\n    @smart_inference_mode()\n    def __call__(self, trainer=None, model=None):\n        \"\"\"\n        Supports validation of a pre-trained model if passed or a model being trained\n        if trainer is passed (trainer gets priority).\n        \"\"\"\n        self.training = trainer is not None\n        if self.training:\n            self.device = trainer.device\n            self.data = trainer.data\n            model = trainer.ema.ema or trainer.model\n            self.args.half = self.device.type != 'cpu'  # force FP16 val during training\n            model = model.half() if self.args.half else model.float()\n            self.model = model\n            self.loss = torch.zeros_like(trainer.loss_items, device=trainer.device)\n            self.args.plots = trainer.epoch == trainer.epochs - 1  # always plot final epoch\n            model.eval()\n        else:\n            callbacks.add_integration_callbacks(self)\n            self.run_callbacks('on_val_start')\n            assert model is not None, \"Either trainer or model is needed for validation\"\n            self.device = select_device(self.args.device, self.args.batch)\n            self.args.half &amp;= self.device.type != 'cpu'\n            model = AutoBackend(model, device=self.device, dnn=self.args.dnn, fp16=self.args.half)\n            self.model = model\n            stride, pt, jit, engine = model.stride, model.pt, model.jit, model.engine\n            imgsz = check_imgsz(self.args.imgsz, stride=stride)\n            if engine:\n                self.args.batch = model.batch_size\n            else:\n                self.device = model.device\n                if not pt and not jit:\n                    self.args.batch = 1  # export.py models default to batch-size 1\n                    self.logger.info(\n                        f'Forcing --batch-size 1 square inference (1,3,{imgsz},{imgsz}) for non-PyTorch models')\n\n            if isinstance(self.args.data, str) and self.args.data.endswith(\".yaml\"):\n                self.data = check_dataset_yaml(self.args.data)\n            else:\n                self.data = check_dataset(self.args.data)\n\n            if self.device.type == 'cpu':\n                self.args.workers = 0  # faster CPU val as time dominated by inference, not dataloading\n            self.dataloader = self.dataloader or \\\n                              self.get_dataloader(self.data.get(\"val\") or self.data.set(\"test\"), self.args.batch)\n\n            model.eval()\n            model.warmup(imgsz=(1 if pt else self.args.batch, 3, imgsz, imgsz))  # warmup\n\n        dt = Profile(), Profile(), Profile(), Profile()\n        n_batches = len(self.dataloader)\n        desc = self.get_desc()\n        # NOTE: keeping `not self.training` in tqdm will eliminate pbar after segmentation evaluation during training,\n        # which may affect classification task since this arg is in yolov5/classify/val.py.\n        # bar = tqdm(self.dataloader, desc, n_batches, not self.training, bar_format=TQDM_BAR_FORMAT)\n        bar = tqdm(self.dataloader, desc, n_batches, bar_format=TQDM_BAR_FORMAT)\n        self.init_metrics(de_parallel(model))\n        self.jdict = []  # empty before each val\n        for batch_i, batch in enumerate(bar):\n            self.run_callbacks('on_val_batch_start')\n            self.batch_i = batch_i\n            # pre-process\n            with dt[0]:\n                batch = self.preprocess(batch)\n\n            # inference\n            with dt[1]:\n                preds = model(batch[\"img\"])\n\n            # loss\n            with dt[2]:\n                if self.training:\n                    self.loss += trainer.criterion(preds, batch)[1]\n\n            # pre-process predictions\n            with dt[3]:\n                preds = self.postprocess(preds)\n\n            self.update_metrics(preds, batch)\n            if self.args.plots and batch_i &lt; 3:\n                self.plot_val_samples(batch, batch_i)\n                self.plot_predictions(batch, preds, batch_i)\n\n            self.run_callbacks('on_val_batch_end')\n        stats = self.get_stats()\n        self.check_stats(stats)\n        self.print_results()\n        self.speed = tuple(x.t / len(self.dataloader.dataset) * 1E3 for x in dt)  # speeds per image\n        self.run_callbacks('on_val_end')\n        if self.training:\n            model.float()\n            results = {**stats, **trainer.label_loss_items(self.loss.cpu() / len(self.dataloader), prefix=\"val\")}\n            return {k: round(float(v), 5) for k, v in results.items()}  # return results as 5 decimal place floats\n        else:\n            self.logger.info('Speed: %.1fms pre-process, %.1fms inference, %.1fms loss, %.1fms post-process per image' %\n                             self.speed)\n            if self.args.save_json and self.jdict:\n                with open(str(self.save_dir / \"predictions.json\"), 'w') as f:\n                    self.logger.info(f\"Saving {f.name}...\")\n                    json.dump(self.jdict, f)  # flatten and save\n                stats = self.eval_json(stats)  # update stats\n            return stats\n\n    def run_callbacks(self, event: str):\n        for callback in self.callbacks.get(event, []):\n            callback(self)\n\n    def get_dataloader(self, dataset_path, batch_size):\n        raise NotImplementedError(\"get_dataloader function not implemented for this validator\")\n\n    def preprocess(self, batch):\n        return batch\n\n    def postprocess(self, preds):\n        return preds\n\n    def init_metrics(self, model):\n        pass\n\n    def update_metrics(self, preds, batch):\n        pass\n\n    def get_stats(self):\n        return {}\n\n    def check_stats(self, stats):\n        pass\n\n    def print_results(self):\n        pass\n\n    def get_desc(self):\n        pass\n\n    @property\n    def metric_keys(self):\n        return []\n\n    # TODO: may need to put these following functions into callback\n    def plot_val_samples(self, batch, ni):\n        pass\n\n    def plot_predictions(self, batch, preds, ni):\n        pass\n\n    def pred_to_json(self, preds, batch):\n        pass\n\n    def eval_json(self, stats):\n        pass\n</code></pre>"},{"location":"reference/base_val/#ultralytics.yolo.engine.validator.BaseValidator.__call__","title":"<code>__call__(trainer=None, model=None)</code>","text":"<p>Supports validation of a pre-trained model if passed or a model being trained if trainer is passed (trainer gets priority).</p>  Source code in <code>ultralytics/yolo/engine/validator.py</code> <pre><code>@smart_inference_mode()\ndef __call__(self, trainer=None, model=None):\n    \"\"\"\n    Supports validation of a pre-trained model if passed or a model being trained\n    if trainer is passed (trainer gets priority).\n    \"\"\"\n    self.training = trainer is not None\n    if self.training:\n        self.device = trainer.device\n        self.data = trainer.data\n        model = trainer.ema.ema or trainer.model\n        self.args.half = self.device.type != 'cpu'  # force FP16 val during training\n        model = model.half() if self.args.half else model.float()\n        self.model = model\n        self.loss = torch.zeros_like(trainer.loss_items, device=trainer.device)\n        self.args.plots = trainer.epoch == trainer.epochs - 1  # always plot final epoch\n        model.eval()\n    else:\n        callbacks.add_integration_callbacks(self)\n        self.run_callbacks('on_val_start')\n        assert model is not None, \"Either trainer or model is needed for validation\"\n        self.device = select_device(self.args.device, self.args.batch)\n        self.args.half &amp;= self.device.type != 'cpu'\n        model = AutoBackend(model, device=self.device, dnn=self.args.dnn, fp16=self.args.half)\n        self.model = model\n        stride, pt, jit, engine = model.stride, model.pt, model.jit, model.engine\n        imgsz = check_imgsz(self.args.imgsz, stride=stride)\n        if engine:\n            self.args.batch = model.batch_size\n        else:\n            self.device = model.device\n            if not pt and not jit:\n                self.args.batch = 1  # export.py models default to batch-size 1\n                self.logger.info(\n                    f'Forcing --batch-size 1 square inference (1,3,{imgsz},{imgsz}) for non-PyTorch models')\n\n        if isinstance(self.args.data, str) and self.args.data.endswith(\".yaml\"):\n            self.data = check_dataset_yaml(self.args.data)\n        else:\n            self.data = check_dataset(self.args.data)\n\n        if self.device.type == 'cpu':\n            self.args.workers = 0  # faster CPU val as time dominated by inference, not dataloading\n        self.dataloader = self.dataloader or \\\n                          self.get_dataloader(self.data.get(\"val\") or self.data.set(\"test\"), self.args.batch)\n\n        model.eval()\n        model.warmup(imgsz=(1 if pt else self.args.batch, 3, imgsz, imgsz))  # warmup\n\n    dt = Profile(), Profile(), Profile(), Profile()\n    n_batches = len(self.dataloader)\n    desc = self.get_desc()\n    # NOTE: keeping `not self.training` in tqdm will eliminate pbar after segmentation evaluation during training,\n    # which may affect classification task since this arg is in yolov5/classify/val.py.\n    # bar = tqdm(self.dataloader, desc, n_batches, not self.training, bar_format=TQDM_BAR_FORMAT)\n    bar = tqdm(self.dataloader, desc, n_batches, bar_format=TQDM_BAR_FORMAT)\n    self.init_metrics(de_parallel(model))\n    self.jdict = []  # empty before each val\n    for batch_i, batch in enumerate(bar):\n        self.run_callbacks('on_val_batch_start')\n        self.batch_i = batch_i\n        # pre-process\n        with dt[0]:\n            batch = self.preprocess(batch)\n\n        # inference\n        with dt[1]:\n            preds = model(batch[\"img\"])\n\n        # loss\n        with dt[2]:\n            if self.training:\n                self.loss += trainer.criterion(preds, batch)[1]\n\n        # pre-process predictions\n        with dt[3]:\n            preds = self.postprocess(preds)\n\n        self.update_metrics(preds, batch)\n        if self.args.plots and batch_i &lt; 3:\n            self.plot_val_samples(batch, batch_i)\n            self.plot_predictions(batch, preds, batch_i)\n\n        self.run_callbacks('on_val_batch_end')\n    stats = self.get_stats()\n    self.check_stats(stats)\n    self.print_results()\n    self.speed = tuple(x.t / len(self.dataloader.dataset) * 1E3 for x in dt)  # speeds per image\n    self.run_callbacks('on_val_end')\n    if self.training:\n        model.float()\n        results = {**stats, **trainer.label_loss_items(self.loss.cpu() / len(self.dataloader), prefix=\"val\")}\n        return {k: round(float(v), 5) for k, v in results.items()}  # return results as 5 decimal place floats\n    else:\n        self.logger.info('Speed: %.1fms pre-process, %.1fms inference, %.1fms loss, %.1fms post-process per image' %\n                         self.speed)\n        if self.args.save_json and self.jdict:\n            with open(str(self.save_dir / \"predictions.json\"), 'w') as f:\n                self.logger.info(f\"Saving {f.name}...\")\n                json.dump(self.jdict, f)  # flatten and save\n            stats = self.eval_json(stats)  # update stats\n        return stats\n</code></pre>"},{"location":"reference/base_val/#ultralytics.yolo.engine.validator.BaseValidator.__init__","title":"<code>__init__(dataloader=None, save_dir=None, pbar=None, logger=None, args=None)</code>","text":"<p>Initializes a BaseValidator instance.</p> <p>Parameters:</p>    Name Type Description Default     <code>dataloader</code>  <code>torch.utils.data.DataLoader</code>  <p>Dataloader to be used for validation.</p>  <code>None</code>    <code>save_dir</code>  <code>Path</code>  <p>Directory to save results.</p>  <code>None</code>    <code>pbar</code>  <code>tqdm.tqdm</code>  <p>Progress bar for displaying progress.</p>  <code>None</code>    <code>logger</code>  <code>logging.Logger</code>  <p>Logger to log messages.</p>  <code>None</code>    <code>args</code>  <code>OmegaConf</code>  <p>Configuration for the validator.</p>  <code>None</code>      Source code in <code>ultralytics/yolo/engine/validator.py</code> <pre><code>def __init__(self, dataloader=None, save_dir=None, pbar=None, logger=None, args=None):\n    \"\"\"\n    Initializes a BaseValidator instance.\n\n    Args:\n        dataloader (torch.utils.data.DataLoader): Dataloader to be used for validation.\n        save_dir (Path): Directory to save results.\n        pbar (tqdm.tqdm): Progress bar for displaying progress.\n        logger (logging.Logger): Logger to log messages.\n        args (OmegaConf): Configuration for the validator.\n    \"\"\"\n    self.dataloader = dataloader\n    self.pbar = pbar\n    self.logger = logger or LOGGER\n    self.args = args or OmegaConf.load(DEFAULT_CONFIG)\n    self.model = None\n    self.data = None\n    self.device = None\n    self.batch_i = None\n    self.training = True\n    self.speed = None\n    self.jdict = None\n\n    project = self.args.project or Path(SETTINGS['runs_dir']) / self.args.task\n    name = self.args.name or f\"{self.args.mode}\"\n    self.save_dir = save_dir or increment_path(Path(project) / name,\n                                               exist_ok=self.args.exist_ok if RANK in {-1, 0} else True)\n    (self.save_dir / 'labels' if self.args.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)\n\n    if self.args.conf is None:\n        self.args.conf = 0.001  # default conf=0.001\n\n    self.callbacks = defaultdict(list, {k: [v] for k, v in callbacks.default_callbacks.items()})  # add callbacks\n</code></pre>"},{"location":"reference/exporter/","title":"Exporter","text":""},{"location":"reference/exporter/#exporter-api-reference","title":"Exporter API Reference","text":"<p>Exporter</p> <p>A class for exporting a model.</p> <p>Attributes:</p>    Name Type Description     <code>args</code>  <code>OmegaConf</code>  <p>Configuration for the exporter.</p>   <code>save_dir</code>  <code>Path</code>  <p>Directory to save results.</p>     Source code in <code>ultralytics/yolo/engine/exporter.py</code> <pre><code>class Exporter:\n    \"\"\"\n    Exporter\n\n    A class for exporting a model.\n\n    Attributes:\n        args (OmegaConf): Configuration for the exporter.\n        save_dir (Path): Directory to save results.\n    \"\"\"\n\n    def __init__(self, config=DEFAULT_CONFIG, overrides=None):\n        \"\"\"\n        Initializes the Exporter class.\n\n        Args:\n            config (str, optional): Path to a configuration file. Defaults to DEFAULT_CONFIG.\n            overrides (dict, optional): Configuration overrides. Defaults to None.\n        \"\"\"\n        if overrides is None:\n            overrides = {}\n        self.args = get_config(config, overrides)\n        self.callbacks = defaultdict(list, {k: [v] for k, v in callbacks.default_callbacks.items()})  # add callbacks\n        callbacks.add_integration_callbacks(self)\n\n    @smart_inference_mode()\n    def __call__(self, model=None):\n        self.run_callbacks(\"on_export_start\")\n        t = time.time()\n        format = self.args.format.lower()  # to lowercase\n        fmts = tuple(export_formats()['Argument'][1:])  # available export formats\n        flags = [x == format for x in fmts]\n        assert sum(flags), f'ERROR: Invalid format={format}, valid formats are {fmts}'\n        jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle = flags  # export booleans\n\n        # Load PyTorch model\n        self.device = select_device('cpu' if self.args.device is None else self.args.device)\n        if self.args.half:\n            if self.device.type == 'cpu' and not coreml:\n                LOGGER.info('half=True only compatible with GPU or CoreML export, i.e. use device=0 or format=coreml')\n                self.args.half = False\n            assert not self.args.dynamic, '--half not compatible with --dynamic, i.e. use either --half or --dynamic'\n\n        # Checks\n        # if self.args.batch == model.args['batch_size']:  # user has not modified training batch_size\n        self.args.batch = 1\n        self.imgsz = check_imgsz(self.args.imgsz, stride=model.stride, min_dim=2)  # check image size\n        if self.args.optimize:\n            assert self.device.type == 'cpu', '--optimize not compatible with cuda devices, i.e. use --device cpu'\n\n        # Input\n        im = torch.zeros(self.args.batch, 3, *self.imgsz).to(self.device)\n        file = Path(getattr(model, 'pt_path', None) or getattr(model, 'yaml_file', None) or model.yaml['yaml_file'])\n        if file.suffix == '.yaml':\n            file = Path(file.name)\n\n        # Update model\n        model = deepcopy(model).to(self.device)\n        for p in model.parameters():\n            p.requires_grad = False\n        model.eval()\n        model = model.fuse()\n        for k, m in model.named_modules():\n            if isinstance(m, (Detect, Segment)):\n                m.dynamic = self.args.dynamic\n                m.export = True\n\n        y = None\n        for _ in range(2):\n            y = model(im)  # dry runs\n        if self.args.half and not coreml:\n            im, model = im.half(), model.half()  # to FP16\n        shape = tuple((y[0] if isinstance(y, tuple) else y).shape)  # model output shape\n        LOGGER.info(\n            f\"\\n{colorstr('PyTorch:')} starting from {file} with output shape {shape} ({file_size(file):.1f} MB)\")\n\n        # Warnings\n        warnings.filterwarnings('ignore', category=torch.jit.TracerWarning)  # suppress TracerWarning\n        warnings.filterwarnings('ignore', category=UserWarning)  # suppress shape prim::Constant missing ONNX warning\n        warnings.filterwarnings('ignore', category=DeprecationWarning)  # suppress CoreML np.bool deprecation warning\n\n        # Assign\n        self.im = im\n        self.model = model\n        self.file = file\n        self.output_shape = tuple(y.shape) if isinstance(y, torch.Tensor) else (x.shape for x in y)\n        self.metadata = {'stride': int(max(model.stride)), 'names': model.names}  # model metadata\n        self.pretty_name = self.file.stem.replace('yolo', 'YOLO')\n\n        # Exports\n        f = [''] * len(fmts)  # exported filenames\n        if jit:  # TorchScript\n            f[0], _ = self._export_torchscript()\n        if engine:  # TensorRT required before ONNX\n            f[1], _ = self._export_engine()\n        if onnx or xml:  # OpenVINO requires ONNX\n            f[2], _ = self._export_onnx()\n        if xml:  # OpenVINO\n            f[3], _ = self._export_openvino()\n        if coreml:  # CoreML\n            f[4], _ = self._export_coreml()\n        if any((saved_model, pb, tflite, edgetpu, tfjs)):  # TensorFlow formats\n            raise NotImplementedError('YOLOv8 TensorFlow export support is still under development. '\n                                      'Please consider contributing to the effort if you have TF expertise. Thank you!')\n            assert not isinstance(model, ClassificationModel), 'ClassificationModel TF exports not yet supported.'\n            nms = False\n            f[5], s_model = self._export_saved_model(nms=nms or self.args.agnostic_nms or tfjs,\n                                                     agnostic_nms=self.args.agnostic_nms or tfjs)\n            if pb or tfjs:  # pb prerequisite to tfjs\n                f[6], _ = self._export_pb(s_model)\n            if tflite or edgetpu:\n                f[7], _ = self._export_tflite(s_model,\n                                              int8=self.args.int8 or edgetpu,\n                                              data=self.args.data,\n                                              nms=nms,\n                                              agnostic_nms=self.args.agnostic_nms)\n                if edgetpu:\n                    f[8], _ = self._export_edgetpu()\n                self._add_tflite_metadata(f[8] or f[7], num_outputs=len(s_model.outputs))\n            if tfjs:\n                f[9], _ = self._export_tfjs()\n        if paddle:  # PaddlePaddle\n            f[10], _ = self._export_paddle()\n\n        # Finish\n        f = [str(x) for x in f if x]  # filter out '' and None\n        if any(f):\n            task = guess_task_from_head(model.yaml[\"head\"][-1][-2])\n            s = \"-WARNING \u26a0\ufe0f not yet supported for YOLOv8 exported models\"\n            LOGGER.info(f'\\nExport complete ({time.time() - t:.1f}s)'\n                        f\"\\nResults saved to {colorstr('bold', file.parent.resolve())}\"\n                        f\"\\nPredict:         yolo task={task} mode=predict model={f[-1]} {s}\"\n                        f\"\\nValidate:        yolo task={task} mode=val model={f[-1]} {s}\"\n                        f\"\\nVisualize:       https://netron.app\")\n\n        self.run_callbacks(\"on_export_end\")\n        return f  # return list of exported files/dirs\n\n    @try_export\n    def _export_torchscript(self, prefix=colorstr('TorchScript:')):\n        # YOLOv8 TorchScript model export\n        LOGGER.info(f'\\n{prefix} starting export with torch {torch.__version__}...')\n        f = self.file.with_suffix('.torchscript')\n\n        ts = torch.jit.trace(self.model, self.im, strict=False)\n        d = {\"shape\": self.im.shape, \"stride\": int(max(self.model.stride)), \"names\": self.model.names}\n        extra_files = {'config.txt': json.dumps(d)}  # torch._C.ExtraFilesMap()\n        if self.args.optimize:  # https://pytorch.org/tutorials/recipes/mobile_interpreter.html\n            LOGGER.info(f'{prefix} optimizing for mobile...')\n            from torch.utils.mobile_optimizer import optimize_for_mobile\n            optimize_for_mobile(ts)._save_for_lite_interpreter(str(f), _extra_files=extra_files)\n        else:\n            ts.save(str(f), _extra_files=extra_files)\n        return f, None\n\n    @try_export\n    def _export_onnx(self, prefix=colorstr('ONNX:')):\n        # YOLOv8 ONNX export\n        check_requirements('onnx&gt;=1.12.0')\n        import onnx  # noqa\n\n        LOGGER.info(f'\\n{prefix} starting export with onnx {onnx.__version__}...')\n        f = str(self.file.with_suffix('.onnx'))\n\n        output_names = ['output0', 'output1'] if isinstance(self.model, SegmentationModel) else ['output0']\n        dynamic = self.args.dynamic\n        if dynamic:\n            dynamic = {'images': {0: 'batch', 2: 'height', 3: 'width'}}  # shape(1,3,640,640)\n            if isinstance(self.model, SegmentationModel):\n                dynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)\n                dynamic['output1'] = {0: 'batch', 2: 'mask_height', 3: 'mask_width'}  # shape(1,32,160,160)\n            elif isinstance(self.model, DetectionModel):\n                dynamic['output0'] = {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)\n\n        torch.onnx.export(\n            self.model.cpu() if dynamic else self.model,  # --dynamic only compatible with cpu\n            self.im.cpu() if dynamic else self.im,\n            f,\n            verbose=False,\n            opset_version=self.args.opset,\n            do_constant_folding=True,  # WARNING: DNN inference with torch&gt;=1.12 may require do_constant_folding=False\n            input_names=['images'],\n            output_names=output_names,\n            dynamic_axes=dynamic or None)\n\n        # Checks\n        model_onnx = onnx.load(f)  # load onnx model\n        onnx.checker.check_model(model_onnx)  # check onnx model\n\n        # Metadata\n        d = {'stride': int(max(self.model.stride)), 'names': self.model.names}\n        for k, v in d.items():\n            meta = model_onnx.metadata_props.add()\n            meta.key, meta.value = k, str(v)\n        onnx.save(model_onnx, f)\n\n        # Simplify\n        if self.args.simplify:\n            try:\n                check_requirements('onnxsim')\n                import onnxsim\n\n                LOGGER.info(f'{prefix} simplifying with onnx-simplifier {onnxsim.__version__}...')\n                subprocess.run(f'onnxsim {f} {f}', shell=True)\n            except Exception as e:\n                LOGGER.info(f'{prefix} simplifier failure: {e}')\n        return f, model_onnx\n\n    @try_export\n    def _export_openvino(self, prefix=colorstr('OpenVINO:')):\n        # YOLOv8 OpenVINO export\n        check_requirements('openvino-dev')  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n        import openvino.inference_engine as ie  # noqa\n\n        LOGGER.info(f'\\n{prefix} starting export with openvino {ie.__version__}...')\n        f = str(self.file).replace(self.file.suffix, f'_openvino_model{os.sep}')\n        f_onnx = self.file.with_suffix('.onnx')\n\n        cmd = f\"mo --input_model {f_onnx} --output_dir {f} --data_type {'FP16' if self.args.half else 'FP32'}\"\n        subprocess.run(cmd.split(), check=True, env=os.environ)  # export\n        yaml_save(Path(f) / self.file.with_suffix('.yaml').name, self.metadata)  # add metadata.yaml\n        return f, None\n\n    @try_export\n    def _export_paddle(self, prefix=colorstr('PaddlePaddle:')):\n        # YOLOv8 Paddle export\n        check_requirements(('paddlepaddle', 'x2paddle'))\n        import x2paddle  # noqa\n        from x2paddle.convert import pytorch2paddle  # noqa\n\n        LOGGER.info(f'\\n{prefix} starting export with X2Paddle {x2paddle.__version__}...')\n        f = str(self.file).replace(self.file.suffix, f'_paddle_model{os.sep}')\n\n        pytorch2paddle(module=self.model, save_dir=f, jit_type='trace', input_examples=[self.im])  # export\n        yaml_save(Path(f) / self.file.with_suffix('.yaml').name, self.metadata)  # add metadata.yaml\n        return f, None\n\n    @try_export\n    def _export_coreml(self, prefix=colorstr('CoreML:')):\n        # YOLOv8 CoreML export\n        check_requirements('coremltools&gt;=6.0')\n        import coremltools as ct  # noqa\n\n        class iOSModel(torch.nn.Module):\n            # Wrap an Ultralytics YOLO model for iOS export\n            def __init__(self, model, im):\n                super().__init__()\n                b, c, h, w = im.shape  # batch, channel, height, width\n                self.model = model\n                self.nc = len(model.names)  # number of classes\n                if w == h:\n                    self.normalize = 1.0 / w  # scalar\n                else:\n                    self.normalize = torch.tensor([1.0 / w, 1.0 / h, 1.0 / w, 1.0 / h])  # broadcast (slower, smaller)\n\n            def forward(self, x):\n                xywh, cls = self.model(x)[0].transpose(0, 1).split((4, self.nc), 1)\n                return cls, xywh * self.normalize  # confidence (3780, 80), coordinates (3780, 4)\n\n        LOGGER.info(f'\\n{prefix} starting export with coremltools {ct.__version__}...')\n        f = self.file.with_suffix('.mlmodel')\n\n        model = iOSModel(self.model, self.im).eval() if self.args.nms else self.model\n        ts = torch.jit.trace(model, self.im, strict=False)  # TorchScript model\n        ct_model = ct.convert(ts, inputs=[ct.ImageType('image', shape=self.im.shape, scale=1 / 255, bias=[0, 0, 0])])\n        bits, mode = (8, 'kmeans_lut') if self.args.int8 else (16, 'linear') if self.args.half else (32, None)\n        if bits &lt; 32:\n            if MACOS:  # quantization only supported on macOS\n                ct_model = ct.models.neural_network.quantization_utils.quantize_weights(ct_model, bits, mode)\n            else:\n                LOGGER.info(f'{prefix} quantization only supported on macOS, skipping...')\n        if self.args.nms:\n            ct_model = self._pipeline_coreml(ct_model)\n\n        ct_model.save(str(f))\n        return f, ct_model\n\n    @try_export\n    def _export_engine(self, workspace=4, verbose=False, prefix=colorstr('TensorRT:')):\n        # YOLOv8 TensorRT export https://developer.nvidia.com/tensorrt\n        assert self.im.device.type != 'cpu', 'export running on CPU but must be on GPU, i.e. `device==0`'\n        try:\n            import tensorrt as trt  # noqa\n        except ImportError:\n            if platform.system() == 'Linux':\n                check_requirements('nvidia-tensorrt', cmds='-U --index-url https://pypi.ngc.nvidia.com')\n            import tensorrt as trt  # noqa\n\n        check_version(trt.__version__, '7.0.0', hard=True)  # require tensorrt&gt;=8.0.0\n        self._export_onnx()\n        onnx = self.file.with_suffix('.onnx')\n\n        LOGGER.info(f'\\n{prefix} starting export with TensorRT {trt.__version__}...')\n        assert onnx.exists(), f'failed to export ONNX file: {onnx}'\n        f = self.file.with_suffix('.engine')  # TensorRT engine file\n        logger = trt.Logger(trt.Logger.INFO)\n        if verbose:\n            logger.min_severity = trt.Logger.Severity.VERBOSE\n\n        builder = trt.Builder(logger)\n        config = builder.create_builder_config()\n        config.max_workspace_size = workspace * 1 &lt;&lt; 30\n        # config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace &lt;&lt; 30)  # fix TRT 8.4 deprecation notice\n\n        flag = (1 &lt;&lt; int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n        network = builder.create_network(flag)\n        parser = trt.OnnxParser(network, logger)\n        if not parser.parse_from_file(str(onnx)):\n            raise RuntimeError(f'failed to load ONNX file: {onnx}')\n\n        inputs = [network.get_input(i) for i in range(network.num_inputs)]\n        outputs = [network.get_output(i) for i in range(network.num_outputs)]\n        for inp in inputs:\n            LOGGER.info(f'{prefix} input \"{inp.name}\" with shape{inp.shape} {inp.dtype}')\n        for out in outputs:\n            LOGGER.info(f'{prefix} output \"{out.name}\" with shape{out.shape} {out.dtype}')\n\n        if self.args.dynamic:\n            shape = self.im.shape\n            if shape[0] &lt;= 1:\n                LOGGER.warning(f\"{prefix} WARNING \u26a0\ufe0f --dynamic model requires maximum --batch-size argument\")\n            profile = builder.create_optimization_profile()\n            for inp in inputs:\n                profile.set_shape(inp.name, (1, *shape[1:]), (max(1, shape[0] // 2), *shape[1:]), shape)\n            config.add_optimization_profile(profile)\n\n        LOGGER.info(\n            f'{prefix} building FP{16 if builder.platform_has_fast_fp16 and self.args.half else 32} engine as {f}')\n        if builder.platform_has_fast_fp16 and self.args.half:\n            config.set_flag(trt.BuilderFlag.FP16)\n        with builder.build_engine(network, config) as engine, open(f, 'wb') as t:\n            t.write(engine.serialize())\n        return f, None\n\n    @try_export\n    def _export_saved_model(self,\n                            nms=False,\n                            agnostic_nms=False,\n                            topk_per_class=100,\n                            topk_all=100,\n                            iou_thres=0.45,\n                            conf_thres=0.25,\n                            prefix=colorstr('TensorFlow SavedModel:')):\n\n        # YOLOv8 TensorFlow SavedModel export\n        try:\n            import tensorflow as tf  # noqa\n        except ImportError:\n            check_requirements(f\"tensorflow{'' if torch.cuda.is_available() else '-macos' if MACOS else '-cpu'}\")\n            import tensorflow as tf  # noqa\n        check_requirements((\"onnx\", \"onnx2tf\", \"sng4onnx\", \"onnxsim\", \"onnx_graphsurgeon\"),\n                           cmds=\"--extra-index-url https://pypi.ngc.nvidia.com \")\n\n        LOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\n        f = str(self.file).replace(self.file.suffix, '_saved_model')\n\n        # Export to ONNX\n        self._export_onnx()\n        onnx = self.file.with_suffix('.onnx')\n\n        # Export to TF SavedModel\n        subprocess.run(f'onnx2tf -i {onnx} --output_signaturedefs -o {f}', shell=True)\n\n        # Load saved_model\n        keras_model = tf.saved_model.load(f, tags=None, options=None)\n\n        return f, keras_model\n\n    @try_export\n    def _export_saved_model_OLD(self,\n                                nms=False,\n                                agnostic_nms=False,\n                                topk_per_class=100,\n                                topk_all=100,\n                                iou_thres=0.45,\n                                conf_thres=0.25,\n                                prefix=colorstr('TensorFlow SavedModel:')):\n        # YOLOv8 TensorFlow SavedModel export\n        try:\n            import tensorflow as tf  # noqa\n        except ImportError:\n            check_requirements(f\"tensorflow{'' if torch.cuda.is_available() else '-macos' if MACOS else '-cpu'}\")\n            import tensorflow as tf  # noqa\n        # from models.tf import TFModel\n        from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2  # noqa\n\n        LOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\n        f = str(self.file).replace(self.file.suffix, '_saved_model')\n        batch_size, ch, *imgsz = list(self.im.shape)  # BCHW\n\n        tf_models = None  # TODO: no TF modules available\n        tf_model = tf_models.TFModel(cfg=self.model.yaml, model=self.model.cpu(), nc=self.model.nc, imgsz=imgsz)\n        im = tf.zeros((batch_size, *imgsz, ch))  # BHWC order for TensorFlow\n        _ = tf_model.predict(im, nms, agnostic_nms, topk_per_class, topk_all, iou_thres, conf_thres)\n        inputs = tf.keras.Input(shape=(*imgsz, ch), batch_size=None if self.args.dynamic else batch_size)\n        outputs = tf_model.predict(inputs, nms, agnostic_nms, topk_per_class, topk_all, iou_thres, conf_thres)\n        keras_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n        keras_model.trainable = False\n        keras_model.summary()\n        if self.args.keras:\n            keras_model.save(f, save_format='tf')\n        else:\n            spec = tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype)\n            m = tf.function(lambda x: keras_model(x))  # full model\n            m = m.get_concrete_function(spec)\n            frozen_func = convert_variables_to_constants_v2(m)\n            tfm = tf.Module()\n            tfm.__call__ = tf.function(lambda x: frozen_func(x)[:4] if nms else frozen_func(x), [spec])\n            tfm.__call__(im)\n            tf.saved_model.save(tfm,\n                                f,\n                                options=tf.saved_model.SaveOptions(experimental_custom_gradients=False)\n                                if check_version(tf.__version__, '2.6') else tf.saved_model.SaveOptions())\n        return f, keras_model\n\n    @try_export\n    def _export_pb(self, keras_model, file, prefix=colorstr('TensorFlow GraphDef:')):\n        # YOLOv8 TensorFlow GraphDef *.pb export https://github.com/leimao/Frozen_Graph_TensorFlow\n        import tensorflow as tf  # noqa\n        from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2  # noqa\n\n        LOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\n        f = file.with_suffix('.pb')\n\n        m = tf.function(lambda x: keras_model(x))  # full model\n        m = m.get_concrete_function(tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype))\n        frozen_func = convert_variables_to_constants_v2(m)\n        frozen_func.graph.as_graph_def()\n        tf.io.write_graph(graph_or_graph_def=frozen_func.graph, logdir=str(f.parent), name=f.name, as_text=False)\n        return f, None\n\n    @try_export\n    def _export_tflite(self, keras_model, int8, data, nms, agnostic_nms, prefix=colorstr('TensorFlow Lite:')):\n        # YOLOv8 TensorFlow Lite export\n        import tensorflow as tf  # noqa\n\n        LOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\n        batch_size, ch, *imgsz = list(self.im.shape)  # BCHW\n        f = str(self.file).replace(self.file.suffix, '-fp16.tflite')\n\n        converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n        converter.target_spec.supported_types = [tf.float16]\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n        if int8:\n\n            def representative_dataset_gen(dataset, n_images=100):\n                # Dataset generator for use with converter.representative_dataset, returns a generator of np arrays\n                for n, (path, img, im0s, vid_cap, string) in enumerate(dataset):\n                    im = np.transpose(img, [1, 2, 0])\n                    im = np.expand_dims(im, axis=0).astype(np.float32)\n                    im /= 255\n                    yield [im]\n                    if n &gt;= n_images:\n                        break\n\n            dataset = LoadImages(check_dataset(check_yaml(data))['train'], imgsz=imgsz, auto=False)\n            converter.representative_dataset = lambda: representative_dataset_gen(dataset, n_images=100)\n            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n            converter.target_spec.supported_types = []\n            converter.inference_input_type = tf.uint8  # or tf.int8\n            converter.inference_output_type = tf.uint8  # or tf.int8\n            converter.experimental_new_quantizer = True\n            f = str(self.file).replace(self.file.suffix, '-int8.tflite')\n        if nms or agnostic_nms:\n            converter.target_spec.supported_ops.append(tf.lite.OpsSet.SELECT_TF_OPS)\n\n        tflite_model = converter.convert()\n        open(f, \"wb\").write(tflite_model)\n        return f, None\n\n    @try_export\n    def _export_edgetpu(self, prefix=colorstr('Edge TPU:')):\n        # YOLOv8 Edge TPU export https://coral.ai/docs/edgetpu/models-intro/\n        cmd = 'edgetpu_compiler --version'\n        help_url = 'https://coral.ai/docs/edgetpu/compiler/'\n        assert platform.system() == 'Linux', f'export only supported on Linux. See {help_url}'\n        if subprocess.run(f'{cmd} &gt;/dev/null', shell=True).returncode != 0:\n            LOGGER.info(f'\\n{prefix} export requires Edge TPU compiler. Attempting install from {help_url}')\n            sudo = subprocess.run('sudo --version &gt;/dev/null', shell=True).returncode == 0  # sudo installed on system\n            for c in (\n                    'curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -',\n                    'echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | '  # no comma\n                    'sudo tee /etc/apt/sources.list.d/coral-edgetpu.list',\n                    'sudo apt-get update',\n                    'sudo apt-get install edgetpu-compiler'):\n                subprocess.run(c if sudo else c.replace('sudo ', ''), shell=True, check=True)\n        ver = subprocess.run(cmd, shell=True, capture_output=True, check=True).stdout.decode().split()[-1]\n\n        LOGGER.info(f'\\n{prefix} starting export with Edge TPU compiler {ver}...')\n        f = str(self.file).replace(self.file.suffix, '-int8_edgetpu.tflite')  # Edge TPU model\n        f_tfl = str(self.file).replace(self.file.suffix, '-int8.tflite')  # TFLite model\n\n        cmd = f\"edgetpu_compiler -s -d -k 10 --out_dir {self.file.parent} {f_tfl}\"\n        subprocess.run(cmd.split(), check=True)\n        return f, None\n\n    @try_export\n    def _export_tfjs(self, prefix=colorstr('TensorFlow.js:')):\n        # YOLOv8 TensorFlow.js export\n        check_requirements('tensorflowjs')\n        import tensorflowjs as tfjs  # noqa\n\n        LOGGER.info(f'\\n{prefix} starting export with tensorflowjs {tfjs.__version__}...')\n        f = str(self.file).replace(self.file.suffix, '_web_model')  # js dir\n        f_pb = self.file.with_suffix('.pb')  # *.pb path\n        f_json = Path(f) / 'model.json'  # *.json path\n\n        cmd = f'tensorflowjs_converter --input_format=tf_frozen_model ' \\\n              f'--output_node_names=Identity,Identity_1,Identity_2,Identity_3 {f_pb} {f}'\n        subprocess.run(cmd.split())\n\n        with open(f_json, 'w') as j:  # sort JSON Identity_* in ascending order\n            subst = re.sub(\n                r'{\"outputs\": {\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n                r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n                r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n                r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}}}', r'{\"outputs\": {\"Identity\": {\"name\": \"Identity\"}, '\n                r'\"Identity_1\": {\"name\": \"Identity_1\"}, '\n                r'\"Identity_2\": {\"name\": \"Identity_2\"}, '\n                r'\"Identity_3\": {\"name\": \"Identity_3\"}}}', f_json.read_text())\n            j.write(subst)\n        return f, None\n\n    def _add_tflite_metadata(self, file, num_outputs):\n        # Add metadata to *.tflite models per https://www.tensorflow.org/lite/models/convert/metadata\n        with contextlib.suppress(ImportError):\n            # check_requirements('tflite_support')\n            from tflite_support import flatbuffers  # noqa\n            from tflite_support import metadata as _metadata  # noqa\n            from tflite_support import metadata_schema_py_generated as _metadata_fb  # noqa\n\n            tmp_file = Path('/tmp/meta.txt')\n            with open(tmp_file, 'w') as meta_f:\n                meta_f.write(str(self.metadata))\n\n            model_meta = _metadata_fb.ModelMetadataT()\n            label_file = _metadata_fb.AssociatedFileT()\n            label_file.name = tmp_file.name\n            model_meta.associatedFiles = [label_file]\n\n            subgraph = _metadata_fb.SubGraphMetadataT()\n            subgraph.inputTensorMetadata = [_metadata_fb.TensorMetadataT()]\n            subgraph.outputTensorMetadata = [_metadata_fb.TensorMetadataT()] * num_outputs\n            model_meta.subgraphMetadata = [subgraph]\n\n            b = flatbuffers.Builder(0)\n            b.Finish(model_meta.Pack(b), _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\n            metadata_buf = b.Output()\n\n            populator = _metadata.MetadataPopulator.with_model_file(file)\n            populator.load_metadata_buffer(metadata_buf)\n            populator.load_associated_files([str(tmp_file)])\n            populator.populate()\n            tmp_file.unlink()\n\n    def _pipeline_coreml(self, model, prefix=colorstr('CoreML Pipeline:')):\n        # YOLOv8 CoreML pipeline\n        import coremltools as ct  # noqa\n\n        LOGGER.info(f'{prefix} starting pipeline with coremltools {ct.__version__}...')\n        batch_size, ch, h, w = list(self.im.shape)  # BCHW\n\n        # Output shapes\n        spec = model.get_spec()\n        out0, out1 = iter(spec.description.output)\n        if MACOS:\n            from PIL import Image\n            img = Image.new('RGB', (w, h))  # img(192 width, 320 height)\n            # img = torch.zeros((*opt.img_size, 3)).numpy()  # img size(320,192,3) iDetection\n            out = model.predict({'image': img})\n            out0_shape = out[out0.name].shape\n            out1_shape = out[out1.name].shape\n        else:  # linux and windows can not run model.predict(), get sizes from pytorch output y\n            out0_shape = self.output_shape[1], self.output_shape[2] - 5  # (3780, 80)\n            out1_shape = self.output_shape[1], 4  # (3780, 4)\n\n        # Checks\n        names = self.metadata['names']\n        nx, ny = spec.description.input[0].type.imageType.width, spec.description.input[0].type.imageType.height\n        na, nc = out0_shape\n        # na, nc = out0.type.multiArrayType.shape  # number anchors, classes\n        assert len(names) == nc, f'{len(names)} names found for nc={nc}'  # check\n\n        # Define output shapes (missing)\n        out0.type.multiArrayType.shape[:] = out0_shape  # (3780, 80)\n        out1.type.multiArrayType.shape[:] = out1_shape  # (3780, 4)\n        # spec.neuralNetwork.preprocessing[0].featureName = '0'\n\n        # Flexible input shapes\n        # from coremltools.models.neural_network import flexible_shape_utils\n        # s = [] # shapes\n        # s.append(flexible_shape_utils.NeuralNetworkImageSize(320, 192))\n        # s.append(flexible_shape_utils.NeuralNetworkImageSize(640, 384))  # (height, width)\n        # flexible_shape_utils.add_enumerated_image_sizes(spec, feature_name='image', sizes=s)\n        # r = flexible_shape_utils.NeuralNetworkImageSizeRange()  # shape ranges\n        # r.add_height_range((192, 640))\n        # r.add_width_range((192, 640))\n        # flexible_shape_utils.update_image_size_range(spec, feature_name='image', size_range=r)\n\n        # Print\n        print(spec.description)\n\n        # Model from spec\n        model = ct.models.MLModel(spec)\n\n        # 3. Create NMS protobuf\n        nms_spec = ct.proto.Model_pb2.Model()\n        nms_spec.specificationVersion = 5\n        for i in range(2):\n            decoder_output = model._spec.description.output[i].SerializeToString()\n            nms_spec.description.input.add()\n            nms_spec.description.input[i].ParseFromString(decoder_output)\n            nms_spec.description.output.add()\n            nms_spec.description.output[i].ParseFromString(decoder_output)\n\n        nms_spec.description.output[0].name = 'confidence'\n        nms_spec.description.output[1].name = 'coordinates'\n\n        output_sizes = [nc, 4]\n        for i in range(2):\n            ma_type = nms_spec.description.output[i].type.multiArrayType\n            ma_type.shapeRange.sizeRanges.add()\n            ma_type.shapeRange.sizeRanges[0].lowerBound = 0\n            ma_type.shapeRange.sizeRanges[0].upperBound = -1\n            ma_type.shapeRange.sizeRanges.add()\n            ma_type.shapeRange.sizeRanges[1].lowerBound = output_sizes[i]\n            ma_type.shapeRange.sizeRanges[1].upperBound = output_sizes[i]\n            del ma_type.shape[:]\n\n        nms = nms_spec.nonMaximumSuppression\n        nms.confidenceInputFeatureName = out0.name  # 1x507x80\n        nms.coordinatesInputFeatureName = out1.name  # 1x507x4\n        nms.confidenceOutputFeatureName = 'confidence'\n        nms.coordinatesOutputFeatureName = 'coordinates'\n        nms.iouThresholdInputFeatureName = 'iouThreshold'\n        nms.confidenceThresholdInputFeatureName = 'confidenceThreshold'\n        nms.iouThreshold = 0.45\n        nms.confidenceThreshold = 0.25\n        nms.pickTop.perClass = True\n        nms.stringClassLabels.vector.extend(names.values())\n        nms_model = ct.models.MLModel(nms_spec)\n\n        # 4. Pipeline models together\n        pipeline = ct.models.pipeline.Pipeline(input_features=[('image', ct.models.datatypes.Array(3, ny, nx)),\n                                                               ('iouThreshold', ct.models.datatypes.Double()),\n                                                               ('confidenceThreshold', ct.models.datatypes.Double())],\n                                               output_features=['confidence', 'coordinates'])\n        pipeline.add_model(model)\n        pipeline.add_model(nms_model)\n\n        # Correct datatypes\n        pipeline.spec.description.input[0].ParseFromString(model._spec.description.input[0].SerializeToString())\n        pipeline.spec.description.output[0].ParseFromString(nms_model._spec.description.output[0].SerializeToString())\n        pipeline.spec.description.output[1].ParseFromString(nms_model._spec.description.output[1].SerializeToString())\n\n        # Update metadata\n        pipeline.spec.specificationVersion = 5\n        pipeline.spec.description.metadata.versionString = f'Ultralytics YOLOv{ultralytics.__version__}'\n        pipeline.spec.description.metadata.shortDescription = f'Ultralytics {self.pretty_name} CoreML model'\n        pipeline.spec.description.metadata.author = 'Ultralytics (https://ultralytics.com)'\n        pipeline.spec.description.metadata.license = 'GPL-3.0 license (https://ultralytics.com/license)'\n        pipeline.spec.description.metadata.userDefined.update({\n            'IoU threshold': str(nms.iouThreshold),\n            'Confidence threshold': str(nms.confidenceThreshold)})\n\n        # Save the model\n        model = ct.models.MLModel(pipeline.spec)\n        model.input_description['image'] = 'Input image'\n        model.input_description['iouThreshold'] = f'(optional) IOU threshold override (default: {nms.iouThreshold})'\n        model.input_description['confidenceThreshold'] = \\\n            f'(optional) Confidence threshold override (default: {nms.confidenceThreshold})'\n        model.output_description['confidence'] = 'Boxes \u00d7 Class confidence (see user-defined metadata \"classes\")'\n        model.output_description['coordinates'] = 'Boxes \u00d7 [x, y, width, height] (relative to image size)'\n        LOGGER.info(f'{prefix} pipeline success')\n        return model\n\n    def run_callbacks(self, event: str):\n        for callback in self.callbacks.get(event, []):\n            callback(self)\n</code></pre>"},{"location":"reference/exporter/#ultralytics.yolo.engine.exporter.Exporter.__init__","title":"<code>__init__(config=DEFAULT_CONFIG, overrides=None)</code>","text":"<p>Initializes the Exporter class.</p> <p>Parameters:</p>    Name Type Description Default     <code>config</code>  <code>str</code>  <p>Path to a configuration file. Defaults to DEFAULT_CONFIG.</p>  <code>DEFAULT_CONFIG</code>    <code>overrides</code>  <code>dict</code>  <p>Configuration overrides. Defaults to None.</p>  <code>None</code>      Source code in <code>ultralytics/yolo/engine/exporter.py</code> <pre><code>def __init__(self, config=DEFAULT_CONFIG, overrides=None):\n    \"\"\"\n    Initializes the Exporter class.\n\n    Args:\n        config (str, optional): Path to a configuration file. Defaults to DEFAULT_CONFIG.\n        overrides (dict, optional): Configuration overrides. Defaults to None.\n    \"\"\"\n    if overrides is None:\n        overrides = {}\n    self.args = get_config(config, overrides)\n    self.callbacks = defaultdict(list, {k: [v] for k, v in callbacks.default_callbacks.items()})  # add callbacks\n    callbacks.add_integration_callbacks(self)\n</code></pre>"},{"location":"reference/model/","title":"Python Model interface","text":""},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO","title":"<code>YOLO</code>","text":"<p>YOLO</p> <p>A python interface which emulates a model-like behaviour by wrapping trainers.</p>  Source code in <code>ultralytics/yolo/engine/model.py</code> <pre><code>class YOLO:\n    \"\"\"\n    YOLO\n\n    A python interface which emulates a model-like behaviour by wrapping trainers.\n    \"\"\"\n\n    def __init__(self, model='yolov8n.yaml', type=\"v8\") -&gt; None:\n        \"\"\"\n        &gt; Initializes the YOLO object.\n\n        Args:\n            model (str, Path): model to load or create\n            type (str): Type/version of models to use. Defaults to \"v8\".\n        \"\"\"\n        self.type = type\n        self.ModelClass = None  # model class\n        self.TrainerClass = None  # trainer class\n        self.ValidatorClass = None  # validator class\n        self.PredictorClass = None  # predictor class\n        self.model = None  # model object\n        self.trainer = None  # trainer object\n        self.task = None  # task type\n        self.ckpt = None  # if loaded from *.pt\n        self.cfg = None  # if loaded from *.yaml\n        self.ckpt_path = None\n        self.overrides = {}  # overrides for trainer object\n\n        # Load or create new YOLO model\n        {'.pt': self._load, '.yaml': self._new}[Path(model).suffix](model)\n\n    def __call__(self, source, **kwargs):\n        return self.predict(source, **kwargs)\n\n    def _new(self, cfg: str, verbose=True):\n        \"\"\"\n        &gt; Initializes a new model and infers the task type from the model definitions.\n\n        Args:\n            cfg (str): model configuration file\n            verbose (bool): display model info on load\n        \"\"\"\n        cfg = check_yaml(cfg)  # check YAML\n        cfg_dict = yaml_load(cfg, append_filename=True)  # model dict\n        self.task = guess_task_from_head(cfg_dict[\"head\"][-1][-2])\n        self.ModelClass, self.TrainerClass, self.ValidatorClass, self.PredictorClass = \\\n            self._guess_ops_from_task(self.task)\n        self.model = self.ModelClass(cfg_dict, verbose=verbose)  # initialize\n        self.cfg = cfg\n\n    def _load(self, weights: str):\n        \"\"\"\n        &gt; Initializes a new model and infers the task type from the model head.\n\n        Args:\n            weights (str): model checkpoint to be loaded\n        \"\"\"\n        self.model, self.ckpt = attempt_load_one_weight(weights)\n        self.ckpt_path = weights\n        self.task = self.model.args[\"task\"]\n        self.overrides = self.model.args\n        self._reset_ckpt_args(self.overrides)\n        self.ModelClass, self.TrainerClass, self.ValidatorClass, self.PredictorClass = \\\n            self._guess_ops_from_task(self.task)\n\n    def reset(self):\n        \"\"\"\n        &gt; Resets the model modules.\n        \"\"\"\n        for m in self.model.modules():\n            if hasattr(m, 'reset_parameters'):\n                m.reset_parameters()\n        for p in self.model.parameters():\n            p.requires_grad = True\n\n    def info(self, verbose=False):\n        \"\"\"\n        &gt; Logs model info.\n\n        Args:\n            verbose (bool): Controls verbosity.\n        \"\"\"\n        self.model.info(verbose=verbose)\n\n    def fuse(self):\n        self.model.fuse()\n\n    @smart_inference_mode()\n    def predict(self, source, return_outputs=True, **kwargs):\n        \"\"\"\n        Visualize prediction.\n\n        Args:\n            source (str): Accepts all source types accepted by yolo\n            **kwargs : Any other args accepted by the predictors. To see all args check 'configuration' section in docs\n        \"\"\"\n        overrides = self.overrides.copy()\n        overrides[\"conf\"] = 0.25\n        overrides.update(kwargs)\n        overrides[\"mode\"] = \"predict\"\n        overrides[\"save\"] = kwargs.get(\"save\", False)  # not save files by default\n        predictor = self.PredictorClass(overrides=overrides)\n\n        predictor.args.imgsz = check_imgsz(predictor.args.imgsz, min_dim=2)  # check image size\n        predictor.setup(model=self.model, source=source, return_outputs=return_outputs)\n        return predictor() if return_outputs else predictor.predict_cli()\n\n    @smart_inference_mode()\n    def val(self, data=None, **kwargs):\n        \"\"\"\n        &gt; Validate a model on a given dataset .\n\n        Args:\n            data (str): The dataset to validate on. Accepts all formats accepted by yolo\n            **kwargs : Any other args accepted by the validators. To see all args check 'configuration' section in docs\n        \"\"\"\n        overrides = self.overrides.copy()\n        overrides.update(kwargs)\n        overrides[\"mode\"] = \"val\"\n        args = get_config(config=DEFAULT_CONFIG, overrides=overrides)\n        args.data = data or args.data\n        args.task = self.task\n\n        validator = self.ValidatorClass(args=args)\n        validator(model=self.model)\n\n    @smart_inference_mode()\n    def export(self, **kwargs):\n        \"\"\"\n        &gt; Export model.\n\n        Args:\n            **kwargs : Any other args accepted by the predictors. To see all args check 'configuration' section in docs\n        \"\"\"\n\n        overrides = self.overrides.copy()\n        overrides.update(kwargs)\n        args = get_config(config=DEFAULT_CONFIG, overrides=overrides)\n        args.task = self.task\n\n        exporter = Exporter(overrides=args)\n        exporter(model=self.model)\n\n    def train(self, **kwargs):\n        \"\"\"\n        &gt; Trains the model on a given dataset.\n\n        Args:\n            **kwargs (Any): Any number of arguments representing the training configuration. List of all args can be found in 'config' section.\n                            You can pass all arguments as a yaml file in `cfg`. Other args are ignored if `cfg` file is passed\n        \"\"\"\n        overrides = self.overrides.copy()\n        overrides.update(kwargs)\n        if kwargs.get(\"cfg\"):\n            LOGGER.info(f\"cfg file passed. Overriding default params with {kwargs['cfg']}.\")\n            overrides = yaml_load(check_yaml(kwargs[\"cfg\"]), append_filename=True)\n        overrides[\"task\"] = self.task\n        overrides[\"mode\"] = \"train\"\n        if not overrides.get(\"data\"):\n            raise AttributeError(\"dataset not provided! Please define `data` in config.yaml or pass as an argument.\")\n        if overrides.get(\"resume\"):\n            overrides[\"resume\"] = self.ckpt_path\n\n        self.trainer = self.TrainerClass(overrides=overrides)\n        if not overrides.get(\"resume\"):  # manually set model only if not resuming\n            self.trainer.model = self.trainer.get_model(weights=self.model if self.ckpt else None, cfg=self.model.yaml)\n            self.model = self.trainer.model\n        self.trainer.train()\n\n    def to(self, device):\n        \"\"\"\n        &gt; Sends the model to the given device.\n\n        Args:\n            device (str): device\n        \"\"\"\n        self.model.to(device)\n\n    def _guess_ops_from_task(self, task):\n        model_class, train_lit, val_lit, pred_lit = MODEL_MAP[task]\n        # warning: eval is unsafe. Use with caution\n        trainer_class = eval(train_lit.replace(\"TYPE\", f\"{self.type}\"))\n        validator_class = eval(val_lit.replace(\"TYPE\", f\"{self.type}\"))\n        predictor_class = eval(pred_lit.replace(\"TYPE\", f\"{self.type}\"))\n\n        return model_class, trainer_class, validator_class, predictor_class\n\n    @staticmethod\n    def _reset_ckpt_args(args):\n        args.pop(\"project\", None)\n        args.pop(\"name\", None)\n        args.pop(\"batch\", None)\n        args.pop(\"epochs\", None)\n        args.pop(\"cache\", None)\n        args.pop(\"save_json\", None)\n\n        # set device to '' to prevent from auto DDP usage\n        args[\"device\"] = ''\n</code></pre>"},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO.__init__","title":"<code>__init__(model='yolov8n.yaml', type='v8')</code>","text":"<p>Initializes the YOLO object.</p>  <p>Parameters:</p>    Name Type Description Default     <code>model</code>  <code>str, Path</code>  <p>model to load or create</p>  <code>'yolov8n.yaml'</code>    <code>type</code>  <code>str</code>  <p>Type/version of models to use. Defaults to \"v8\".</p>  <code>'v8'</code>      Source code in <code>ultralytics/yolo/engine/model.py</code> <pre><code>def __init__(self, model='yolov8n.yaml', type=\"v8\") -&gt; None:\n    \"\"\"\n    &gt; Initializes the YOLO object.\n\n    Args:\n        model (str, Path): model to load or create\n        type (str): Type/version of models to use. Defaults to \"v8\".\n    \"\"\"\n    self.type = type\n    self.ModelClass = None  # model class\n    self.TrainerClass = None  # trainer class\n    self.ValidatorClass = None  # validator class\n    self.PredictorClass = None  # predictor class\n    self.model = None  # model object\n    self.trainer = None  # trainer object\n    self.task = None  # task type\n    self.ckpt = None  # if loaded from *.pt\n    self.cfg = None  # if loaded from *.yaml\n    self.ckpt_path = None\n    self.overrides = {}  # overrides for trainer object\n\n    # Load or create new YOLO model\n    {'.pt': self._load, '.yaml': self._new}[Path(model).suffix](model)\n</code></pre>"},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO.export","title":"<code>export(**kwargs)</code>","text":"<p>Export model.</p>  <p>Parameters:</p>    Name Type Description Default     <code>**kwargs</code>   <p>Any other args accepted by the predictors. To see all args check 'configuration' section in docs</p>  <code>{}</code>      Source code in <code>ultralytics/yolo/engine/model.py</code> <pre><code>@smart_inference_mode()\ndef export(self, **kwargs):\n    \"\"\"\n    &gt; Export model.\n\n    Args:\n        **kwargs : Any other args accepted by the predictors. To see all args check 'configuration' section in docs\n    \"\"\"\n\n    overrides = self.overrides.copy()\n    overrides.update(kwargs)\n    args = get_config(config=DEFAULT_CONFIG, overrides=overrides)\n    args.task = self.task\n\n    exporter = Exporter(overrides=args)\n    exporter(model=self.model)\n</code></pre>"},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO.info","title":"<code>info(verbose=False)</code>","text":"<p>Logs model info.</p>  <p>Parameters:</p>    Name Type Description Default     <code>verbose</code>  <code>bool</code>  <p>Controls verbosity.</p>  <code>False</code>      Source code in <code>ultralytics/yolo/engine/model.py</code> <pre><code>def info(self, verbose=False):\n    \"\"\"\n    &gt; Logs model info.\n\n    Args:\n        verbose (bool): Controls verbosity.\n    \"\"\"\n    self.model.info(verbose=verbose)\n</code></pre>"},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO.predict","title":"<code>predict(source, return_outputs=True, **kwargs)</code>","text":"<p>Visualize prediction.</p> <p>Parameters:</p>    Name Type Description Default     <code>source</code>  <code>str</code>  <p>Accepts all source types accepted by yolo</p>  required    <code>**kwargs</code>   <p>Any other args accepted by the predictors. To see all args check 'configuration' section in docs</p>  <code>{}</code>      Source code in <code>ultralytics/yolo/engine/model.py</code> <pre><code>@smart_inference_mode()\ndef predict(self, source, return_outputs=True, **kwargs):\n    \"\"\"\n    Visualize prediction.\n\n    Args:\n        source (str): Accepts all source types accepted by yolo\n        **kwargs : Any other args accepted by the predictors. To see all args check 'configuration' section in docs\n    \"\"\"\n    overrides = self.overrides.copy()\n    overrides[\"conf\"] = 0.25\n    overrides.update(kwargs)\n    overrides[\"mode\"] = \"predict\"\n    overrides[\"save\"] = kwargs.get(\"save\", False)  # not save files by default\n    predictor = self.PredictorClass(overrides=overrides)\n\n    predictor.args.imgsz = check_imgsz(predictor.args.imgsz, min_dim=2)  # check image size\n    predictor.setup(model=self.model, source=source, return_outputs=return_outputs)\n    return predictor() if return_outputs else predictor.predict_cli()\n</code></pre>"},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO.reset","title":"<code>reset()</code>","text":"<p>Resets the model modules.</p>   Source code in <code>ultralytics/yolo/engine/model.py</code> <pre><code>def reset(self):\n    \"\"\"\n    &gt; Resets the model modules.\n    \"\"\"\n    for m in self.model.modules():\n        if hasattr(m, 'reset_parameters'):\n            m.reset_parameters()\n    for p in self.model.parameters():\n        p.requires_grad = True\n</code></pre>"},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO.to","title":"<code>to(device)</code>","text":"<p>Sends the model to the given device.</p>  <p>Parameters:</p>    Name Type Description Default     <code>device</code>  <code>str</code>  <p>device</p>  required      Source code in <code>ultralytics/yolo/engine/model.py</code> <pre><code>def to(self, device):\n    \"\"\"\n    &gt; Sends the model to the given device.\n\n    Args:\n        device (str): device\n    \"\"\"\n    self.model.to(device)\n</code></pre>"},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO.train","title":"<code>train(**kwargs)</code>","text":"<p>Trains the model on a given dataset.</p>  <p>Parameters:</p>    Name Type Description Default     <code>**kwargs</code>  <code>Any</code>  <p>Any number of arguments representing the training configuration. List of all args can be found in 'config' section.             You can pass all arguments as a yaml file in <code>cfg</code>. Other args are ignored if <code>cfg</code> file is passed</p>  <code>{}</code>      Source code in <code>ultralytics/yolo/engine/model.py</code> <pre><code>def train(self, **kwargs):\n    \"\"\"\n    &gt; Trains the model on a given dataset.\n\n    Args:\n        **kwargs (Any): Any number of arguments representing the training configuration. List of all args can be found in 'config' section.\n                        You can pass all arguments as a yaml file in `cfg`. Other args are ignored if `cfg` file is passed\n    \"\"\"\n    overrides = self.overrides.copy()\n    overrides.update(kwargs)\n    if kwargs.get(\"cfg\"):\n        LOGGER.info(f\"cfg file passed. Overriding default params with {kwargs['cfg']}.\")\n        overrides = yaml_load(check_yaml(kwargs[\"cfg\"]), append_filename=True)\n    overrides[\"task\"] = self.task\n    overrides[\"mode\"] = \"train\"\n    if not overrides.get(\"data\"):\n        raise AttributeError(\"dataset not provided! Please define `data` in config.yaml or pass as an argument.\")\n    if overrides.get(\"resume\"):\n        overrides[\"resume\"] = self.ckpt_path\n\n    self.trainer = self.TrainerClass(overrides=overrides)\n    if not overrides.get(\"resume\"):  # manually set model only if not resuming\n        self.trainer.model = self.trainer.get_model(weights=self.model if self.ckpt else None, cfg=self.model.yaml)\n        self.model = self.trainer.model\n    self.trainer.train()\n</code></pre>"},{"location":"reference/model/#ultralytics.yolo.engine.model.YOLO.val","title":"<code>val(data=None, **kwargs)</code>","text":"<p>Validate a model on a given dataset .</p>  <p>Parameters:</p>    Name Type Description Default     <code>data</code>  <code>str</code>  <p>The dataset to validate on. Accepts all formats accepted by yolo</p>  <code>None</code>    <code>**kwargs</code>   <p>Any other args accepted by the validators. To see all args check 'configuration' section in docs</p>  <code>{}</code>      Source code in <code>ultralytics/yolo/engine/model.py</code> <pre><code>@smart_inference_mode()\ndef val(self, data=None, **kwargs):\n    \"\"\"\n    &gt; Validate a model on a given dataset .\n\n    Args:\n        data (str): The dataset to validate on. Accepts all formats accepted by yolo\n        **kwargs : Any other args accepted by the validators. To see all args check 'configuration' section in docs\n    \"\"\"\n    overrides = self.overrides.copy()\n    overrides.update(kwargs)\n    overrides[\"mode\"] = \"val\"\n    args = get_config(config=DEFAULT_CONFIG, overrides=overrides)\n    args.data = data or args.data\n    args.task = self.task\n\n    validator = self.ValidatorClass(args=args)\n    validator(model=self.model)\n</code></pre>"},{"location":"reference/nn/","title":"nn Module","text":"<p>Ultralytics nn module contains 3 main components:</p> <ol> <li>AutoBackend: A module that can run inference on all popular model formats</li> <li>BaseModel: <code>BaseModel</code> class defines the operations supported by tasks like Detection and Segmentation</li> <li>modules: Optimized and reusable neural network blocks built on PyTorch.</li> </ol>"},{"location":"reference/nn/#autobackend","title":"AutoBackend","text":"<p>         Bases: <code>nn.Module</code></p>  Source code in <code>ultralytics/nn/autobackend.py</code> <pre><code>class AutoBackend(nn.Module):\n\n    def __init__(self, weights='yolov8n.pt', device=torch.device('cpu'), dnn=False, data=None, fp16=False, fuse=True):\n        \"\"\"\n        MultiBackend class for python inference on various platforms using Ultralytics YOLO.\n\n        Args:\n            weights (str): The path to the weights file. Default: 'yolov8n.pt'\n            device (torch.device): The device to run the model on.\n            dnn (bool): Use OpenCV's DNN module for inference if True, defaults to False.\n            data (dict): Additional data, optional\n            fp16 (bool): If True, use half precision. Default: False\n            fuse (bool): Whether to fuse the model or not. Default: True\n\n        Supported formats and their usage:\n            Platform              | Weights Format\n            -----------------------|------------------\n            PyTorch               | *.pt\n            TorchScript           | *.torchscript\n            ONNX Runtime          | *.onnx\n            ONNX OpenCV DNN       | *.onnx --dnn\n            OpenVINO              | *.xml\n            CoreML                | *.mlmodel\n            TensorRT              | *.engine\n            TensorFlow SavedModel | *_saved_model\n            TensorFlow GraphDef   | *.pb\n            TensorFlow Lite       | *.tflite\n            TensorFlow Edge TPU   | *_edgetpu.tflite\n            PaddlePaddle          | *_paddle_model\n        \"\"\"\n        super().__init__()\n        w = str(weights[0] if isinstance(weights, list) else weights)\n        nn_module = isinstance(weights, torch.nn.Module)\n        pt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle, triton = self._model_type(w)\n        fp16 &amp;= pt or jit or onnx or engine or nn_module  # FP16\n        nhwc = coreml or saved_model or pb or tflite or edgetpu  # BHWC formats (vs torch BCWH)\n        stride = 32  # default stride\n        model = None  # TODO: resolves ONNX inference, verify effect on other backends\n        cuda = torch.cuda.is_available() and device.type != 'cpu'  # use CUDA\n        if not (pt or triton or nn_module):\n            w = attempt_download(w)  # download if not local\n\n        # NOTE: special case: in-memory pytorch model\n        if nn_module:\n            model = weights.to(device)\n            model = model.fuse() if fuse else model\n            names = model.module.names if hasattr(model, 'module') else model.names  # get class names\n            model.half() if fp16 else model.float()\n            self.model = model  # explicitly assign for to(), cpu(), cuda(), half()\n            pt = True\n        elif pt:  # PyTorch\n            from ultralytics.nn.tasks import attempt_load_weights\n            model = attempt_load_weights(weights if isinstance(weights, list) else w,\n                                         device=device,\n                                         inplace=True,\n                                         fuse=fuse)\n            stride = max(int(model.stride.max()), 32)  # model stride\n            names = model.module.names if hasattr(model, 'module') else model.names  # get class names\n            model.half() if fp16 else model.float()\n            self.model = model  # explicitly assign for to(), cpu(), cuda(), half()\n        elif jit:  # TorchScript\n            LOGGER.info(f'Loading {w} for TorchScript inference...')\n            extra_files = {'config.txt': ''}  # model metadata\n            model = torch.jit.load(w, _extra_files=extra_files, map_location=device)\n            model.half() if fp16 else model.float()\n            if extra_files['config.txt']:  # load metadata dict\n                d = json.loads(extra_files['config.txt'],\n                               object_hook=lambda d: {int(k) if k.isdigit() else k: v\n                                                      for k, v in d.items()})\n                stride, names = int(d['stride']), d['names']\n        elif dnn:  # ONNX OpenCV DNN\n            LOGGER.info(f'Loading {w} for ONNX OpenCV DNN inference...')\n            check_requirements('opencv-python&gt;=4.5.4')\n            net = cv2.dnn.readNetFromONNX(w)\n        elif onnx:  # ONNX Runtime\n            LOGGER.info(f'Loading {w} for ONNX Runtime inference...')\n            check_requirements(('onnx', 'onnxruntime-gpu' if cuda else 'onnxruntime'))\n            import onnxruntime\n            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if cuda else ['CPUExecutionProvider']\n            session = onnxruntime.InferenceSession(w, providers=providers)\n            output_names = [x.name for x in session.get_outputs()]\n            meta = session.get_modelmeta().custom_metadata_map  # metadata\n            if 'stride' in meta:\n                stride, names = int(meta['stride']), eval(meta['names'])\n        elif xml:  # OpenVINO\n            LOGGER.info(f'Loading {w} for OpenVINO inference...')\n            check_requirements('openvino')  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n            from openvino.runtime import Core, Layout, get_batch  # noqa\n            ie = Core()\n            if not Path(w).is_file():  # if not *.xml\n                w = next(Path(w).glob('*.xml'))  # get *.xml file from *_openvino_model dir\n            network = ie.read_model(model=w, weights=Path(w).with_suffix('.bin'))\n            if network.get_parameters()[0].get_layout().empty:\n                network.get_parameters()[0].set_layout(Layout(\"NCHW\"))\n            batch_dim = get_batch(network)\n            if batch_dim.is_static:\n                batch_size = batch_dim.get_length()\n            executable_network = ie.compile_model(network, device_name=\"CPU\")  # device_name=\"MYRIAD\" for Intel NCS2\n            stride, names = self._load_metadata(Path(w).with_suffix('.yaml'))  # load metadata\n        elif engine:  # TensorRT\n            LOGGER.info(f'Loading {w} for TensorRT inference...')\n            import tensorrt as trt  # https://developer.nvidia.com/nvidia-tensorrt-download\n            check_version(trt.__version__, '7.0.0', hard=True)  # require tensorrt&gt;=7.0.0\n            if device.type == 'cpu':\n                device = torch.device('cuda:0')\n            Binding = namedtuple('Binding', ('name', 'dtype', 'shape', 'data', 'ptr'))\n            logger = trt.Logger(trt.Logger.INFO)\n            with open(w, 'rb') as f, trt.Runtime(logger) as runtime:\n                model = runtime.deserialize_cuda_engine(f.read())\n            context = model.create_execution_context()\n            bindings = OrderedDict()\n            output_names = []\n            fp16 = False  # default updated below\n            dynamic = False\n            for i in range(model.num_bindings):\n                name = model.get_binding_name(i)\n                dtype = trt.nptype(model.get_binding_dtype(i))\n                if model.binding_is_input(i):\n                    if -1 in tuple(model.get_binding_shape(i)):  # dynamic\n                        dynamic = True\n                        context.set_binding_shape(i, tuple(model.get_profile_shape(0, i)[2]))\n                    if dtype == np.float16:\n                        fp16 = True\n                else:  # output\n                    output_names.append(name)\n                shape = tuple(context.get_binding_shape(i))\n                im = torch.from_numpy(np.empty(shape, dtype=dtype)).to(device)\n                bindings[name] = Binding(name, dtype, shape, im, int(im.data_ptr()))\n            binding_addrs = OrderedDict((n, d.ptr) for n, d in bindings.items())\n            batch_size = bindings['images'].shape[0]  # if dynamic, this is instead max batch size\n        elif coreml:  # CoreML\n            LOGGER.info(f'Loading {w} for CoreML inference...')\n            import coremltools as ct\n            model = ct.models.MLModel(w)\n        elif saved_model:  # TF SavedModel\n            LOGGER.info(f'Loading {w} for TensorFlow SavedModel inference...')\n            import tensorflow as tf\n            keras = False  # assume TF1 saved_model\n            model = tf.keras.models.load_model(w) if keras else tf.saved_model.load(w)\n        elif pb:  # GraphDef https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt\n            LOGGER.info(f'Loading {w} for TensorFlow GraphDef inference...')\n            import tensorflow as tf\n\n            def wrap_frozen_graph(gd, inputs, outputs):\n                x = tf.compat.v1.wrap_function(lambda: tf.compat.v1.import_graph_def(gd, name=\"\"), [])  # wrapped\n                ge = x.graph.as_graph_element\n                return x.prune(tf.nest.map_structure(ge, inputs), tf.nest.map_structure(ge, outputs))\n\n            def gd_outputs(gd):\n                name_list, input_list = [], []\n                for node in gd.node:  # tensorflow.core.framework.node_def_pb2.NodeDef\n                    name_list.append(node.name)\n                    input_list.extend(node.input)\n                return sorted(f'{x}:0' for x in list(set(name_list) - set(input_list)) if not x.startswith('NoOp'))\n\n            gd = tf.Graph().as_graph_def()  # TF GraphDef\n            with open(w, 'rb') as f:\n                gd.ParseFromString(f.read())\n            frozen_func = wrap_frozen_graph(gd, inputs=\"x:0\", outputs=gd_outputs(gd))\n        elif tflite or edgetpu:  # https://www.tensorflow.org/lite/guide/python#install_tensorflow_lite_for_python\n            try:  # https://coral.ai/docs/edgetpu/tflite-python/#update-existing-tf-lite-code-for-the-edge-tpu\n                from tflite_runtime.interpreter import Interpreter, load_delegate\n            except ImportError:\n                import tensorflow as tf\n                Interpreter, load_delegate = tf.lite.Interpreter, tf.lite.experimental.load_delegate,\n            if edgetpu:  # TF Edge TPU https://coral.ai/software/#edgetpu-runtime\n                LOGGER.info(f'Loading {w} for TensorFlow Lite Edge TPU inference...')\n                delegate = {\n                    'Linux': 'libedgetpu.so.1',\n                    'Darwin': 'libedgetpu.1.dylib',\n                    'Windows': 'edgetpu.dll'}[platform.system()]\n                interpreter = Interpreter(model_path=w, experimental_delegates=[load_delegate(delegate)])\n            else:  # TFLite\n                LOGGER.info(f'Loading {w} for TensorFlow Lite inference...')\n                interpreter = Interpreter(model_path=w)  # load TFLite model\n            interpreter.allocate_tensors()  # allocate\n            input_details = interpreter.get_input_details()  # inputs\n            output_details = interpreter.get_output_details()  # outputs\n        elif tfjs:  # TF.js\n            raise NotImplementedError('ERROR: YOLOv5 TF.js inference is not supported')\n        elif paddle:  # PaddlePaddle\n            LOGGER.info(f'Loading {w} for PaddlePaddle inference...')\n            check_requirements('paddlepaddle-gpu' if cuda else 'paddlepaddle')\n            import paddle.inference as pdi\n            if not Path(w).is_file():  # if not *.pdmodel\n                w = next(Path(w).rglob('*.pdmodel'))  # get *.xml file from *_openvino_model dir\n            weights = Path(w).with_suffix('.pdiparams')\n            config = pdi.Config(str(w), str(weights))\n            if cuda:\n                config.enable_use_gpu(memory_pool_init_size_mb=2048, device_id=0)\n            predictor = pdi.create_predictor(config)\n            input_handle = predictor.get_input_handle(predictor.get_input_names()[0])\n            output_names = predictor.get_output_names()\n        elif triton:  # NVIDIA Triton Inference Server\n            LOGGER.info('Triton Inference Server not supported...')\n            '''\n            TODO:\n            check_requirements('tritonclient[all]')\n            from utils.triton import TritonRemoteModel\n            model = TritonRemoteModel(url=w)\n            nhwc = model.runtime.startswith(\"tensorflow\")\n            '''\n        else:\n            raise NotImplementedError(f'ERROR: {w} is not a supported format')\n\n        # class names\n        if 'names' not in locals():\n            names = yaml_load(data)['names'] if data else {i: f'class{i}' for i in range(999)}\n        if names[0] == 'n01440764' and len(names) == 1000:  # ImageNet\n            names = yaml_load(ROOT / 'yolo/data/datasets/ImageNet.yaml')['names']  # human-readable names\n\n        self.__dict__.update(locals())  # assign all variables to self\n\n    def forward(self, im, augment=False, visualize=False):\n        \"\"\"\n        Runs inference on the YOLOv8 MultiBackend model.\n\n        Args:\n            im (torch.tensor): The image tensor to perform inference on.\n            augment (bool): whether to perform data augmentation during inference, defaults to False\n            visualize (bool): whether to visualize the output predictions, defaults to False\n\n        Returns:\n            (tuple): Tuple containing the raw output tensor, and the processed output for visualization (if visualize=True)\n        \"\"\"\n        b, ch, h, w = im.shape  # batch, channel, height, width\n        if self.fp16 and im.dtype != torch.float16:\n            im = im.half()  # to FP16\n        if self.nhwc:\n            im = im.permute(0, 2, 3, 1)  # torch BCHW to numpy BHWC shape(1,320,192,3)\n\n        if self.pt or self.nn_module:  # PyTorch\n            y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n        elif self.jit:  # TorchScript\n            y = self.model(im)\n        elif self.dnn:  # ONNX OpenCV DNN\n            im = im.cpu().numpy()  # torch to numpy\n            self.net.setInput(im)\n            y = self.net.forward()\n        elif self.onnx:  # ONNX Runtime\n            im = im.cpu().numpy()  # torch to numpy\n            y = self.session.run(self.output_names, {self.session.get_inputs()[0].name: im})\n        elif self.xml:  # OpenVINO\n            im = im.cpu().numpy()  # FP32\n            y = list(self.executable_network([im]).values())\n        elif self.engine:  # TensorRT\n            if self.dynamic and im.shape != self.bindings['images'].shape:\n                i = self.model.get_binding_index('images')\n                self.context.set_binding_shape(i, im.shape)  # reshape if dynamic\n                self.bindings['images'] = self.bindings['images']._replace(shape=im.shape)\n                for name in self.output_names:\n                    i = self.model.get_binding_index(name)\n                    self.bindings[name].data.resize_(tuple(self.context.get_binding_shape(i)))\n            s = self.bindings['images'].shape\n            assert im.shape == s, f\"input size {im.shape} {'&gt;' if self.dynamic else 'not equal to'} max model size {s}\"\n            self.binding_addrs['images'] = int(im.data_ptr())\n            self.context.execute_v2(list(self.binding_addrs.values()))\n            y = [self.bindings[x].data for x in sorted(self.output_names)]\n        elif self.coreml:  # CoreML\n            im = im.cpu().numpy()\n            im = Image.fromarray((im[0] * 255).astype('uint8'))\n            # im = im.resize((192, 320), Image.ANTIALIAS)\n            y = self.model.predict({'image': im})  # coordinates are xywh normalized\n            if 'confidence' in y:\n                box = xywh2xyxy(y['coordinates'] * [[w, h, w, h]])  # xyxy pixels\n                conf, cls = y['confidence'].max(1), y['confidence'].argmax(1).astype(np.float)\n                y = np.concatenate((box, conf.reshape(-1, 1), cls.reshape(-1, 1)), 1)\n            else:\n                y = list(reversed(y.values()))  # reversed for segmentation models (pred, proto)\n        elif self.paddle:  # PaddlePaddle\n            im = im.cpu().numpy().astype(np.float32)\n            self.input_handle.copy_from_cpu(im)\n            self.predictor.run()\n            y = [self.predictor.get_output_handle(x).copy_to_cpu() for x in self.output_names]\n        elif self.triton:  # NVIDIA Triton Inference Server\n            y = self.model(im)\n        else:  # TensorFlow (SavedModel, GraphDef, Lite, Edge TPU)\n            im = im.cpu().numpy()\n            if self.saved_model:  # SavedModel\n                y = self.model(im, training=False) if self.keras else self.model(im)\n            elif self.pb:  # GraphDef\n                y = self.frozen_func(x=self.tf.constant(im))\n            else:  # Lite or Edge TPU\n                input = self.input_details[0]\n                int8 = input['dtype'] == np.uint8  # is TFLite quantized uint8 model\n                if int8:\n                    scale, zero_point = input['quantization']\n                    im = (im / scale + zero_point).astype(np.uint8)  # de-scale\n                self.interpreter.set_tensor(input['index'], im)\n                self.interpreter.invoke()\n                y = []\n                for output in self.output_details:\n                    x = self.interpreter.get_tensor(output['index'])\n                    if int8:\n                        scale, zero_point = output['quantization']\n                        x = (x.astype(np.float32) - zero_point) * scale  # re-scale\n                    y.append(x)\n            y = [x if isinstance(x, np.ndarray) else x.numpy() for x in y]\n            y[0][..., :4] *= [w, h, w, h]  # xywh normalized to pixels\n\n        if isinstance(y, (list, tuple)):\n            return self.from_numpy(y[0]) if len(y) == 1 else [self.from_numpy(x) for x in y]\n        else:\n            return self.from_numpy(y)\n\n    def from_numpy(self, x):\n        \"\"\"\n         Convert a numpy array to a tensor.\n\n         Args:\n             x (numpy.ndarray): The array to be converted.\n\n         Returns:\n             (torch.tensor): The converted tensor\n         \"\"\"\n        return torch.from_numpy(x).to(self.device) if isinstance(x, np.ndarray) else x\n\n    def warmup(self, imgsz=(1, 3, 640, 640)):\n        \"\"\"\n        Warm up the model by running one forward pass with a dummy input.\n\n        Args:\n            imgsz (tuple): The shape of the dummy input tensor in the format (batch_size, channels, height, width)\n\n        Returns:\n            (None): This method runs the forward pass and don't return any value\n        \"\"\"\n        warmup_types = self.pt, self.jit, self.onnx, self.engine, self.saved_model, self.pb, self.triton, self.nn_module\n        if any(warmup_types) and (self.device.type != 'cpu' or self.triton):\n            im = torch.empty(*imgsz, dtype=torch.half if self.fp16 else torch.float, device=self.device)  # input\n            for _ in range(2 if self.jit else 1):  #\n                self.forward(im)  # warmup\n\n    @staticmethod\n    def _model_type(p='path/to/model.pt'):\n        \"\"\"\n        This function takes a path to a model file and returns the model type\n\n        Args:\n          p: path to the model file. Defaults to path/to/model.pt\n        \"\"\"\n        # Return model type from model path, i.e. path='path/to/model.onnx' -&gt; type=onnx\n        # types = [pt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle]\n        from ultralytics.yolo.engine.exporter import export_formats\n        sf = list(export_formats().Suffix)  # export suffixes\n        if not is_url(p, check=False) and not isinstance(p, str):\n            check_suffix(p, sf)  # checks\n        url = urlparse(p)  # if url may be Triton inference server\n        types = [s in Path(p).name for s in sf]\n        types[8] &amp;= not types[9]  # tflite &amp;= not edgetpu\n        triton = not any(types) and all([any(s in url.scheme for s in [\"http\", \"grpc\"]), url.netloc])\n        return types + [triton]\n\n    @staticmethod\n    def _load_metadata(f=Path('path/to/meta.yaml')):\n        \"\"\"\n        &gt; Loads the metadata from a yaml file\n\n        Args:\n          f: The path to the metadata file.\n        \"\"\"\n        from ultralytics.yolo.utils.files import yaml_load\n\n        # Load metadata from meta.yaml if it exists\n        if f.exists():\n            d = yaml_load(f)\n            return d['stride'], d['names']  # assign stride, names\n        return None, None\n</code></pre>"},{"location":"reference/nn/#ultralytics.nn.autobackend.AutoBackend.__init__","title":"<code>__init__(weights='yolov8n.pt', device=torch.device('cpu'), dnn=False, data=None, fp16=False, fuse=True)</code>","text":"<p>MultiBackend class for python inference on various platforms using Ultralytics YOLO.</p> <p>Parameters:</p>    Name Type Description Default     <code>weights</code>  <code>str</code>  <p>The path to the weights file. Default: 'yolov8n.pt'</p>  <code>'yolov8n.pt'</code>    <code>device</code>  <code>torch.device</code>  <p>The device to run the model on.</p>  <code>torch.device('cpu')</code>    <code>dnn</code>  <code>bool</code>  <p>Use OpenCV's DNN module for inference if True, defaults to False.</p>  <code>False</code>    <code>data</code>  <code>dict</code>  <p>Additional data, optional</p>  <code>None</code>    <code>fp16</code>  <code>bool</code>  <p>If True, use half precision. Default: False</p>  <code>False</code>    <code>fuse</code>  <code>bool</code>  <p>Whether to fuse the model or not. Default: True</p>  <code>True</code>      Supported formats and their usage    Platform Weights Format     PyTorch *.pt   TorchScript *.torchscript   ONNX Runtime *.onnx   ONNX OpenCV DNN *.onnx --dnn   OpenVINO *.xml   CoreML *.mlmodel   TensorRT *.engine   TensorFlow SavedModel *_saved_model   TensorFlow GraphDef *.pb   TensorFlow Lite *.tflite   TensorFlow Edge TPU *_edgetpu.tflite   PaddlePaddle *_paddle_model      Source code in <code>ultralytics/nn/autobackend.py</code> <pre><code>def __init__(self, weights='yolov8n.pt', device=torch.device('cpu'), dnn=False, data=None, fp16=False, fuse=True):\n    \"\"\"\n    MultiBackend class for python inference on various platforms using Ultralytics YOLO.\n\n    Args:\n        weights (str): The path to the weights file. Default: 'yolov8n.pt'\n        device (torch.device): The device to run the model on.\n        dnn (bool): Use OpenCV's DNN module for inference if True, defaults to False.\n        data (dict): Additional data, optional\n        fp16 (bool): If True, use half precision. Default: False\n        fuse (bool): Whether to fuse the model or not. Default: True\n\n    Supported formats and their usage:\n        Platform              | Weights Format\n        -----------------------|------------------\n        PyTorch               | *.pt\n        TorchScript           | *.torchscript\n        ONNX Runtime          | *.onnx\n        ONNX OpenCV DNN       | *.onnx --dnn\n        OpenVINO              | *.xml\n        CoreML                | *.mlmodel\n        TensorRT              | *.engine\n        TensorFlow SavedModel | *_saved_model\n        TensorFlow GraphDef   | *.pb\n        TensorFlow Lite       | *.tflite\n        TensorFlow Edge TPU   | *_edgetpu.tflite\n        PaddlePaddle          | *_paddle_model\n    \"\"\"\n    super().__init__()\n    w = str(weights[0] if isinstance(weights, list) else weights)\n    nn_module = isinstance(weights, torch.nn.Module)\n    pt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle, triton = self._model_type(w)\n    fp16 &amp;= pt or jit or onnx or engine or nn_module  # FP16\n    nhwc = coreml or saved_model or pb or tflite or edgetpu  # BHWC formats (vs torch BCWH)\n    stride = 32  # default stride\n    model = None  # TODO: resolves ONNX inference, verify effect on other backends\n    cuda = torch.cuda.is_available() and device.type != 'cpu'  # use CUDA\n    if not (pt or triton or nn_module):\n        w = attempt_download(w)  # download if not local\n\n    # NOTE: special case: in-memory pytorch model\n    if nn_module:\n        model = weights.to(device)\n        model = model.fuse() if fuse else model\n        names = model.module.names if hasattr(model, 'module') else model.names  # get class names\n        model.half() if fp16 else model.float()\n        self.model = model  # explicitly assign for to(), cpu(), cuda(), half()\n        pt = True\n    elif pt:  # PyTorch\n        from ultralytics.nn.tasks import attempt_load_weights\n        model = attempt_load_weights(weights if isinstance(weights, list) else w,\n                                     device=device,\n                                     inplace=True,\n                                     fuse=fuse)\n        stride = max(int(model.stride.max()), 32)  # model stride\n        names = model.module.names if hasattr(model, 'module') else model.names  # get class names\n        model.half() if fp16 else model.float()\n        self.model = model  # explicitly assign for to(), cpu(), cuda(), half()\n    elif jit:  # TorchScript\n        LOGGER.info(f'Loading {w} for TorchScript inference...')\n        extra_files = {'config.txt': ''}  # model metadata\n        model = torch.jit.load(w, _extra_files=extra_files, map_location=device)\n        model.half() if fp16 else model.float()\n        if extra_files['config.txt']:  # load metadata dict\n            d = json.loads(extra_files['config.txt'],\n                           object_hook=lambda d: {int(k) if k.isdigit() else k: v\n                                                  for k, v in d.items()})\n            stride, names = int(d['stride']), d['names']\n    elif dnn:  # ONNX OpenCV DNN\n        LOGGER.info(f'Loading {w} for ONNX OpenCV DNN inference...')\n        check_requirements('opencv-python&gt;=4.5.4')\n        net = cv2.dnn.readNetFromONNX(w)\n    elif onnx:  # ONNX Runtime\n        LOGGER.info(f'Loading {w} for ONNX Runtime inference...')\n        check_requirements(('onnx', 'onnxruntime-gpu' if cuda else 'onnxruntime'))\n        import onnxruntime\n        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if cuda else ['CPUExecutionProvider']\n        session = onnxruntime.InferenceSession(w, providers=providers)\n        output_names = [x.name for x in session.get_outputs()]\n        meta = session.get_modelmeta().custom_metadata_map  # metadata\n        if 'stride' in meta:\n            stride, names = int(meta['stride']), eval(meta['names'])\n    elif xml:  # OpenVINO\n        LOGGER.info(f'Loading {w} for OpenVINO inference...')\n        check_requirements('openvino')  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n        from openvino.runtime import Core, Layout, get_batch  # noqa\n        ie = Core()\n        if not Path(w).is_file():  # if not *.xml\n            w = next(Path(w).glob('*.xml'))  # get *.xml file from *_openvino_model dir\n        network = ie.read_model(model=w, weights=Path(w).with_suffix('.bin'))\n        if network.get_parameters()[0].get_layout().empty:\n            network.get_parameters()[0].set_layout(Layout(\"NCHW\"))\n        batch_dim = get_batch(network)\n        if batch_dim.is_static:\n            batch_size = batch_dim.get_length()\n        executable_network = ie.compile_model(network, device_name=\"CPU\")  # device_name=\"MYRIAD\" for Intel NCS2\n        stride, names = self._load_metadata(Path(w).with_suffix('.yaml'))  # load metadata\n    elif engine:  # TensorRT\n        LOGGER.info(f'Loading {w} for TensorRT inference...')\n        import tensorrt as trt  # https://developer.nvidia.com/nvidia-tensorrt-download\n        check_version(trt.__version__, '7.0.0', hard=True)  # require tensorrt&gt;=7.0.0\n        if device.type == 'cpu':\n            device = torch.device('cuda:0')\n        Binding = namedtuple('Binding', ('name', 'dtype', 'shape', 'data', 'ptr'))\n        logger = trt.Logger(trt.Logger.INFO)\n        with open(w, 'rb') as f, trt.Runtime(logger) as runtime:\n            model = runtime.deserialize_cuda_engine(f.read())\n        context = model.create_execution_context()\n        bindings = OrderedDict()\n        output_names = []\n        fp16 = False  # default updated below\n        dynamic = False\n        for i in range(model.num_bindings):\n            name = model.get_binding_name(i)\n            dtype = trt.nptype(model.get_binding_dtype(i))\n            if model.binding_is_input(i):\n                if -1 in tuple(model.get_binding_shape(i)):  # dynamic\n                    dynamic = True\n                    context.set_binding_shape(i, tuple(model.get_profile_shape(0, i)[2]))\n                if dtype == np.float16:\n                    fp16 = True\n            else:  # output\n                output_names.append(name)\n            shape = tuple(context.get_binding_shape(i))\n            im = torch.from_numpy(np.empty(shape, dtype=dtype)).to(device)\n            bindings[name] = Binding(name, dtype, shape, im, int(im.data_ptr()))\n        binding_addrs = OrderedDict((n, d.ptr) for n, d in bindings.items())\n        batch_size = bindings['images'].shape[0]  # if dynamic, this is instead max batch size\n    elif coreml:  # CoreML\n        LOGGER.info(f'Loading {w} for CoreML inference...')\n        import coremltools as ct\n        model = ct.models.MLModel(w)\n    elif saved_model:  # TF SavedModel\n        LOGGER.info(f'Loading {w} for TensorFlow SavedModel inference...')\n        import tensorflow as tf\n        keras = False  # assume TF1 saved_model\n        model = tf.keras.models.load_model(w) if keras else tf.saved_model.load(w)\n    elif pb:  # GraphDef https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt\n        LOGGER.info(f'Loading {w} for TensorFlow GraphDef inference...')\n        import tensorflow as tf\n\n        def wrap_frozen_graph(gd, inputs, outputs):\n            x = tf.compat.v1.wrap_function(lambda: tf.compat.v1.import_graph_def(gd, name=\"\"), [])  # wrapped\n            ge = x.graph.as_graph_element\n            return x.prune(tf.nest.map_structure(ge, inputs), tf.nest.map_structure(ge, outputs))\n\n        def gd_outputs(gd):\n            name_list, input_list = [], []\n            for node in gd.node:  # tensorflow.core.framework.node_def_pb2.NodeDef\n                name_list.append(node.name)\n                input_list.extend(node.input)\n            return sorted(f'{x}:0' for x in list(set(name_list) - set(input_list)) if not x.startswith('NoOp'))\n\n        gd = tf.Graph().as_graph_def()  # TF GraphDef\n        with open(w, 'rb') as f:\n            gd.ParseFromString(f.read())\n        frozen_func = wrap_frozen_graph(gd, inputs=\"x:0\", outputs=gd_outputs(gd))\n    elif tflite or edgetpu:  # https://www.tensorflow.org/lite/guide/python#install_tensorflow_lite_for_python\n        try:  # https://coral.ai/docs/edgetpu/tflite-python/#update-existing-tf-lite-code-for-the-edge-tpu\n            from tflite_runtime.interpreter import Interpreter, load_delegate\n        except ImportError:\n            import tensorflow as tf\n            Interpreter, load_delegate = tf.lite.Interpreter, tf.lite.experimental.load_delegate,\n        if edgetpu:  # TF Edge TPU https://coral.ai/software/#edgetpu-runtime\n            LOGGER.info(f'Loading {w} for TensorFlow Lite Edge TPU inference...')\n            delegate = {\n                'Linux': 'libedgetpu.so.1',\n                'Darwin': 'libedgetpu.1.dylib',\n                'Windows': 'edgetpu.dll'}[platform.system()]\n            interpreter = Interpreter(model_path=w, experimental_delegates=[load_delegate(delegate)])\n        else:  # TFLite\n            LOGGER.info(f'Loading {w} for TensorFlow Lite inference...')\n            interpreter = Interpreter(model_path=w)  # load TFLite model\n        interpreter.allocate_tensors()  # allocate\n        input_details = interpreter.get_input_details()  # inputs\n        output_details = interpreter.get_output_details()  # outputs\n    elif tfjs:  # TF.js\n        raise NotImplementedError('ERROR: YOLOv5 TF.js inference is not supported')\n    elif paddle:  # PaddlePaddle\n        LOGGER.info(f'Loading {w} for PaddlePaddle inference...')\n        check_requirements('paddlepaddle-gpu' if cuda else 'paddlepaddle')\n        import paddle.inference as pdi\n        if not Path(w).is_file():  # if not *.pdmodel\n            w = next(Path(w).rglob('*.pdmodel'))  # get *.xml file from *_openvino_model dir\n        weights = Path(w).with_suffix('.pdiparams')\n        config = pdi.Config(str(w), str(weights))\n        if cuda:\n            config.enable_use_gpu(memory_pool_init_size_mb=2048, device_id=0)\n        predictor = pdi.create_predictor(config)\n        input_handle = predictor.get_input_handle(predictor.get_input_names()[0])\n        output_names = predictor.get_output_names()\n    elif triton:  # NVIDIA Triton Inference Server\n        LOGGER.info('Triton Inference Server not supported...')\n        '''\n        TODO:\n        check_requirements('tritonclient[all]')\n        from utils.triton import TritonRemoteModel\n        model = TritonRemoteModel(url=w)\n        nhwc = model.runtime.startswith(\"tensorflow\")\n        '''\n    else:\n        raise NotImplementedError(f'ERROR: {w} is not a supported format')\n\n    # class names\n    if 'names' not in locals():\n        names = yaml_load(data)['names'] if data else {i: f'class{i}' for i in range(999)}\n    if names[0] == 'n01440764' and len(names) == 1000:  # ImageNet\n        names = yaml_load(ROOT / 'yolo/data/datasets/ImageNet.yaml')['names']  # human-readable names\n\n    self.__dict__.update(locals())  # assign all variables to self\n</code></pre>"},{"location":"reference/nn/#ultralytics.nn.autobackend.AutoBackend.forward","title":"<code>forward(im, augment=False, visualize=False)</code>","text":"<p>Runs inference on the YOLOv8 MultiBackend model.</p> <p>Parameters:</p>    Name Type Description Default     <code>im</code>  <code>torch.tensor</code>  <p>The image tensor to perform inference on.</p>  required    <code>augment</code>  <code>bool</code>  <p>whether to perform data augmentation during inference, defaults to False</p>  <code>False</code>    <code>visualize</code>  <code>bool</code>  <p>whether to visualize the output predictions, defaults to False</p>  <code>False</code>     <p>Returns:</p>    Type Description      <code>tuple</code>  <p>Tuple containing the raw output tensor, and the processed output for visualization (if visualize=True)</p>     Source code in <code>ultralytics/nn/autobackend.py</code> <pre><code>def forward(self, im, augment=False, visualize=False):\n    \"\"\"\n    Runs inference on the YOLOv8 MultiBackend model.\n\n    Args:\n        im (torch.tensor): The image tensor to perform inference on.\n        augment (bool): whether to perform data augmentation during inference, defaults to False\n        visualize (bool): whether to visualize the output predictions, defaults to False\n\n    Returns:\n        (tuple): Tuple containing the raw output tensor, and the processed output for visualization (if visualize=True)\n    \"\"\"\n    b, ch, h, w = im.shape  # batch, channel, height, width\n    if self.fp16 and im.dtype != torch.float16:\n        im = im.half()  # to FP16\n    if self.nhwc:\n        im = im.permute(0, 2, 3, 1)  # torch BCHW to numpy BHWC shape(1,320,192,3)\n\n    if self.pt or self.nn_module:  # PyTorch\n        y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n    elif self.jit:  # TorchScript\n        y = self.model(im)\n    elif self.dnn:  # ONNX OpenCV DNN\n        im = im.cpu().numpy()  # torch to numpy\n        self.net.setInput(im)\n        y = self.net.forward()\n    elif self.onnx:  # ONNX Runtime\n        im = im.cpu().numpy()  # torch to numpy\n        y = self.session.run(self.output_names, {self.session.get_inputs()[0].name: im})\n    elif self.xml:  # OpenVINO\n        im = im.cpu().numpy()  # FP32\n        y = list(self.executable_network([im]).values())\n    elif self.engine:  # TensorRT\n        if self.dynamic and im.shape != self.bindings['images'].shape:\n            i = self.model.get_binding_index('images')\n            self.context.set_binding_shape(i, im.shape)  # reshape if dynamic\n            self.bindings['images'] = self.bindings['images']._replace(shape=im.shape)\n            for name in self.output_names:\n                i = self.model.get_binding_index(name)\n                self.bindings[name].data.resize_(tuple(self.context.get_binding_shape(i)))\n        s = self.bindings['images'].shape\n        assert im.shape == s, f\"input size {im.shape} {'&gt;' if self.dynamic else 'not equal to'} max model size {s}\"\n        self.binding_addrs['images'] = int(im.data_ptr())\n        self.context.execute_v2(list(self.binding_addrs.values()))\n        y = [self.bindings[x].data for x in sorted(self.output_names)]\n    elif self.coreml:  # CoreML\n        im = im.cpu().numpy()\n        im = Image.fromarray((im[0] * 255).astype('uint8'))\n        # im = im.resize((192, 320), Image.ANTIALIAS)\n        y = self.model.predict({'image': im})  # coordinates are xywh normalized\n        if 'confidence' in y:\n            box = xywh2xyxy(y['coordinates'] * [[w, h, w, h]])  # xyxy pixels\n            conf, cls = y['confidence'].max(1), y['confidence'].argmax(1).astype(np.float)\n            y = np.concatenate((box, conf.reshape(-1, 1), cls.reshape(-1, 1)), 1)\n        else:\n            y = list(reversed(y.values()))  # reversed for segmentation models (pred, proto)\n    elif self.paddle:  # PaddlePaddle\n        im = im.cpu().numpy().astype(np.float32)\n        self.input_handle.copy_from_cpu(im)\n        self.predictor.run()\n        y = [self.predictor.get_output_handle(x).copy_to_cpu() for x in self.output_names]\n    elif self.triton:  # NVIDIA Triton Inference Server\n        y = self.model(im)\n    else:  # TensorFlow (SavedModel, GraphDef, Lite, Edge TPU)\n        im = im.cpu().numpy()\n        if self.saved_model:  # SavedModel\n            y = self.model(im, training=False) if self.keras else self.model(im)\n        elif self.pb:  # GraphDef\n            y = self.frozen_func(x=self.tf.constant(im))\n        else:  # Lite or Edge TPU\n            input = self.input_details[0]\n            int8 = input['dtype'] == np.uint8  # is TFLite quantized uint8 model\n            if int8:\n                scale, zero_point = input['quantization']\n                im = (im / scale + zero_point).astype(np.uint8)  # de-scale\n            self.interpreter.set_tensor(input['index'], im)\n            self.interpreter.invoke()\n            y = []\n            for output in self.output_details:\n                x = self.interpreter.get_tensor(output['index'])\n                if int8:\n                    scale, zero_point = output['quantization']\n                    x = (x.astype(np.float32) - zero_point) * scale  # re-scale\n                y.append(x)\n        y = [x if isinstance(x, np.ndarray) else x.numpy() for x in y]\n        y[0][..., :4] *= [w, h, w, h]  # xywh normalized to pixels\n\n    if isinstance(y, (list, tuple)):\n        return self.from_numpy(y[0]) if len(y) == 1 else [self.from_numpy(x) for x in y]\n    else:\n        return self.from_numpy(y)\n</code></pre>"},{"location":"reference/nn/#ultralytics.nn.autobackend.AutoBackend.from_numpy","title":"<code>from_numpy(x)</code>","text":"<p>Convert a numpy array to a tensor.</p> <p>Parameters:</p>    Name Type Description Default     <code>x</code>  <code>numpy.ndarray</code>  <p>The array to be converted.</p>  required     <p>Returns:</p>    Type Description      <code>torch.tensor</code>  <p>The converted tensor</p>     Source code in <code>ultralytics/nn/autobackend.py</code> <pre><code>def from_numpy(self, x):\n    \"\"\"\n     Convert a numpy array to a tensor.\n\n     Args:\n         x (numpy.ndarray): The array to be converted.\n\n     Returns:\n         (torch.tensor): The converted tensor\n     \"\"\"\n    return torch.from_numpy(x).to(self.device) if isinstance(x, np.ndarray) else x\n</code></pre>"},{"location":"reference/nn/#ultralytics.nn.autobackend.AutoBackend.warmup","title":"<code>warmup(imgsz=(1, 3, 640, 640))</code>","text":"<p>Warm up the model by running one forward pass with a dummy input.</p> <p>Parameters:</p>    Name Type Description Default     <code>imgsz</code>  <code>tuple</code>  <p>The shape of the dummy input tensor in the format (batch_size, channels, height, width)</p>  <code>(1, 3, 640, 640)</code>     <p>Returns:</p>    Type Description      <code>None</code>  <p>This method runs the forward pass and don't return any value</p>     Source code in <code>ultralytics/nn/autobackend.py</code> <pre><code>def warmup(self, imgsz=(1, 3, 640, 640)):\n    \"\"\"\n    Warm up the model by running one forward pass with a dummy input.\n\n    Args:\n        imgsz (tuple): The shape of the dummy input tensor in the format (batch_size, channels, height, width)\n\n    Returns:\n        (None): This method runs the forward pass and don't return any value\n    \"\"\"\n    warmup_types = self.pt, self.jit, self.onnx, self.engine, self.saved_model, self.pb, self.triton, self.nn_module\n    if any(warmup_types) and (self.device.type != 'cpu' or self.triton):\n        im = torch.empty(*imgsz, dtype=torch.half if self.fp16 else torch.float, device=self.device)  # input\n        for _ in range(2 if self.jit else 1):  #\n            self.forward(im)  # warmup\n</code></pre>"},{"location":"reference/nn/#basemodel","title":"BaseModel","text":"<p>         Bases: <code>nn.Module</code></p> <p>The BaseModel class serves as a base class for all the models in the Ultralytics YOLO family.</p>  Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>class BaseModel(nn.Module):\n    \"\"\"\n    The BaseModel class serves as a base class for all the models in the Ultralytics YOLO family.\n    \"\"\"\n\n    def forward(self, x, profile=False, visualize=False):\n        \"\"\"\n        Forward pass of the model on a single scale.\n        Wrapper for `_forward_once` method.\n\n        Args:\n            x (torch.tensor): The input image tensor\n            profile (bool): Whether to profile the model, defaults to False\n            visualize (bool): Whether to return the intermediate feature maps, defaults to False\n\n        Returns:\n            (torch.tensor): The output of the network.\n        \"\"\"\n        return self._forward_once(x, profile, visualize)\n\n    def _forward_once(self, x, profile=False, visualize=False):\n        \"\"\"\n        Perform a forward pass through the network.\n\n        Args:\n            x (torch.tensor): The input tensor to the model\n            profile (bool):  Print the computation time of each layer if True, defaults to False.\n            visualize (bool): Save the feature maps of the model if True, defaults to False\n\n        Returns:\n            (torch.tensor): The last output of the model.\n        \"\"\"\n        y, dt = [], []  # outputs\n        for m in self.model:\n            if m.f != -1:  # if not from previous layer\n                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers\n            if profile:\n                self._profile_one_layer(m, x, dt)\n            x = m(x)  # run\n            y.append(x if m.i in self.save else None)  # save output\n            if visualize:\n                pass\n                # TODO: feature_visualization(x, m.type, m.i, save_dir=visualize)\n        return x\n\n    def _profile_one_layer(self, m, x, dt):\n        \"\"\"\n        Profile the computation time and FLOPs of a single layer of the model on a given input. Appends the results to the provided list.\n\n        Args:\n            m (nn.Module): The layer to be profiled.\n            x (torch.Tensor): The input data to the layer.\n            dt (list): A list to store the computation time of the layer.\n\n        Returns:\n            None\n        \"\"\"\n        c = m == self.model[-1]  # is final layer, copy input as inplace fix\n        o = thop.profile(m, inputs=(x.copy() if c else x,), verbose=False)[0] / 1E9 * 2 if thop else 0  # FLOPs\n        t = time_sync()\n        for _ in range(10):\n            m(x.copy() if c else x)\n        dt.append((time_sync() - t) * 100)\n        if m == self.model[0]:\n            LOGGER.info(f\"{'time (ms)':&gt;10s} {'GFLOPs':&gt;10s} {'params':&gt;10s}  module\")\n        LOGGER.info(f'{dt[-1]:10.2f} {o:10.2f} {m.np:10.0f}  {m.type}')\n        if c:\n            LOGGER.info(f\"{sum(dt):10.2f} {'-':&gt;10s} {'-':&gt;10s}  Total\")\n\n    def fuse(self):\n        \"\"\"\n        Fuse the `Conv2d()` and `BatchNorm2d()` layers of the model into a single layer, in order to improve the computation efficiency.\n\n        Returns:\n            (nn.Module): The fused model is returned.\n        \"\"\"\n        LOGGER.info('Fusing layers... ')\n        for m in self.model.modules():\n            if isinstance(m, (Conv, DWConv)) and hasattr(m, 'bn'):\n                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv\n                delattr(m, 'bn')  # remove batchnorm\n                m.forward = m.forward_fuse  # update forward\n        self.info()\n        return self\n\n    def info(self, verbose=False, imgsz=640):\n        \"\"\"\n        Prints model information\n\n        Args:\n          verbose (bool): if True, prints out the model information. Defaults to False\n          imgsz (int): the size of the image that the model will be trained on. Defaults to 640\n        \"\"\"\n        model_info(self, verbose, imgsz)\n\n    def _apply(self, fn):\n        \"\"\"\n        `_apply()` is a function that applies a function to all the tensors in the model that are not\n        parameters or registered buffers\n\n        Args:\n          fn: the function to apply to the model\n\n        Returns:\n          A model that is a Detect() object.\n        \"\"\"\n        self = super()._apply(fn)\n        m = self.model[-1]  # Detect()\n        if isinstance(m, (Detect, Segment)):\n            m.stride = fn(m.stride)\n            m.anchors = fn(m.anchors)\n            m.strides = fn(m.strides)\n        return self\n\n    def load(self, weights):\n        \"\"\"\n        This function loads the weights of the model from a file\n\n        Args:\n          weights (str): The weights to load into the model.\n        \"\"\"\n        # Force all tasks to implement this function\n        raise NotImplementedError(\"This function needs to be implemented by derived classes!\")\n</code></pre>"},{"location":"reference/nn/#ultralytics.nn.tasks.BaseModel.forward","title":"<code>forward(x, profile=False, visualize=False)</code>","text":"<p>Forward pass of the model on a single scale. Wrapper for <code>_forward_once</code> method.</p> <p>Parameters:</p>    Name Type Description Default     <code>x</code>  <code>torch.tensor</code>  <p>The input image tensor</p>  required    <code>profile</code>  <code>bool</code>  <p>Whether to profile the model, defaults to False</p>  <code>False</code>    <code>visualize</code>  <code>bool</code>  <p>Whether to return the intermediate feature maps, defaults to False</p>  <code>False</code>     <p>Returns:</p>    Type Description      <code>torch.tensor</code>  <p>The output of the network.</p>     Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def forward(self, x, profile=False, visualize=False):\n    \"\"\"\n    Forward pass of the model on a single scale.\n    Wrapper for `_forward_once` method.\n\n    Args:\n        x (torch.tensor): The input image tensor\n        profile (bool): Whether to profile the model, defaults to False\n        visualize (bool): Whether to return the intermediate feature maps, defaults to False\n\n    Returns:\n        (torch.tensor): The output of the network.\n    \"\"\"\n    return self._forward_once(x, profile, visualize)\n</code></pre>"},{"location":"reference/nn/#ultralytics.nn.tasks.BaseModel.fuse","title":"<code>fuse()</code>","text":"<p>Fuse the <code>Conv2d()</code> and <code>BatchNorm2d()</code> layers of the model into a single layer, in order to improve the computation efficiency.</p> <p>Returns:</p>    Type Description      <code>nn.Module</code>  <p>The fused model is returned.</p>     Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def fuse(self):\n    \"\"\"\n    Fuse the `Conv2d()` and `BatchNorm2d()` layers of the model into a single layer, in order to improve the computation efficiency.\n\n    Returns:\n        (nn.Module): The fused model is returned.\n    \"\"\"\n    LOGGER.info('Fusing layers... ')\n    for m in self.model.modules():\n        if isinstance(m, (Conv, DWConv)) and hasattr(m, 'bn'):\n            m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv\n            delattr(m, 'bn')  # remove batchnorm\n            m.forward = m.forward_fuse  # update forward\n    self.info()\n    return self\n</code></pre>"},{"location":"reference/nn/#ultralytics.nn.tasks.BaseModel.info","title":"<code>info(verbose=False, imgsz=640)</code>","text":"<p>Prints model information</p> <p>Parameters:</p>    Name Type Description Default     <code>verbose</code>  <code>bool</code>  <p>if True, prints out the model information. Defaults to False</p>  <code>False</code>    <code>imgsz</code>  <code>int</code>  <p>the size of the image that the model will be trained on. Defaults to 640</p>  <code>640</code>      Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def info(self, verbose=False, imgsz=640):\n    \"\"\"\n    Prints model information\n\n    Args:\n      verbose (bool): if True, prints out the model information. Defaults to False\n      imgsz (int): the size of the image that the model will be trained on. Defaults to 640\n    \"\"\"\n    model_info(self, verbose, imgsz)\n</code></pre>"},{"location":"reference/nn/#ultralytics.nn.tasks.BaseModel.load","title":"<code>load(weights)</code>","text":"<p>This function loads the weights of the model from a file</p> <p>Parameters:</p>    Name Type Description Default     <code>weights</code>  <code>str</code>  <p>The weights to load into the model.</p>  required      Source code in <code>ultralytics/nn/tasks.py</code> <pre><code>def load(self, weights):\n    \"\"\"\n    This function loads the weights of the model from a file\n\n    Args:\n      weights (str): The weights to load into the model.\n    \"\"\"\n    # Force all tasks to implement this function\n    raise NotImplementedError(\"This function needs to be implemented by derived classes!\")\n</code></pre>"},{"location":"reference/nn/#modules","title":"Modules","text":"<p>TODO</p>"},{"location":"reference/ops/","title":"operations","text":"<p>This module contains optimized deep learning related operations used in the Ultralytics YOLO framework</p>"},{"location":"reference/ops/#non-max-suppression","title":"Non-max suppression","text":"<p>Perform non-maximum suppression (NMS) on a set of boxes, with support for masks and multiple labels per box.</p> <p>Parameters:</p>    Name Type Description Default     <code>prediction</code>  <code>torch.Tensor</code>  <p>A tensor of shape (batch_size, num_boxes, num_classes + 4 + num_masks) containing the predicted boxes, classes, and masks. The tensor should be in the format output by a model, such as YOLO.</p>  required    <code>conf_thres</code>  <code>float</code>  <p>The confidence threshold below which boxes will be filtered out. Valid values are between 0.0 and 1.0.</p>  <code>0.25</code>    <code>iou_thres</code>  <code>float</code>  <p>The IoU threshold below which boxes will be filtered out during NMS. Valid values are between 0.0 and 1.0.</p>  <code>0.45</code>    <code>classes</code>  <code>List[int]</code>  <p>A list of class indices to consider. If None, all classes will be considered.</p>  <code>None</code>    <code>agnostic</code>  <code>bool</code>  <p>If True, the model is agnostic to the number of classes, and all classes will be considered as one.</p>  <code>False</code>    <code>multi_label</code>  <code>bool</code>  <p>If True, each box may have multiple labels.</p>  <code>False</code>    <code>labels</code>  <code>List[List[Union[int, float, torch.Tensor]]]</code>  <p>A list of lists, where each inner list contains the apriori labels for a given image. The list should be in the format output by a dataloader, with each label being a tuple of (class_index, x1, y1, x2, y2).</p>  <code>()</code>    <code>max_det</code>  <code>int</code>  <p>The maximum number of boxes to keep after NMS.</p>  <code>300</code>    <code>nm</code>  <code>int</code>  <p>The number of masks output by the model.</p>  <code>0</code>     <p>Returns:</p>    Type Description      <code>List[torch.Tensor]</code>  <p>A list of length batch_size, where each element is a tensor of shape (num_boxes, 6 + num_masks) containing the kept boxes, with columns (x1, y1, x2, y2, confidence, class, mask1, mask2, ...).</p>"},{"location":"reference/ops/#scale-boxes","title":"Scale boxes","text":"<p>Rescales bounding boxes (in the format of xyxy) from the shape of the image they were originally specified in (img1_shape) to the shape of a different image (img0_shape).</p> <p>Parameters:</p>    Name Type Description Default     <code>img1_shape</code>  <code>tuple</code>  <p>The shape of the image that the bounding boxes are for, in the format of (height, width).</p>  required    <code>boxes</code>  <code>torch.tensor</code>  <p>the bounding boxes of the objects in the image, in the format of (x1, y1, x2, y2)</p>  required    <code>img0_shape</code>  <code>tuple</code>  <p>the shape of the target image, in the format of (height, width).</p>  required    <code>ratio_pad</code>  <code>tuple</code>  <p>a tuple of (ratio, pad) for scaling the boxes. If not provided, the ratio and pad will be calculated based on the size difference between the two images.</p>  <code>None</code>     <p>Returns:</p>    Name Type Description     <code>boxes</code>  <code>torch.tensor</code>  <p>The scaled bounding boxes, in the format of (x1, y1, x2, y2)</p>"},{"location":"reference/ops/#scale-image","title":"Scale image","text":"<p>Takes a mask, and resizes it to the original image size</p> <p>Parameters:</p>    Name Type Description Default     <code>im1_shape</code>  <code>tuple</code>  <p>model input shape, [h, w]</p>  required    <code>masks</code>  <code>torch.tensor</code>  <p>[h, w, num]</p>  required    <code>im0_shape</code>  <code>tuple</code>  <p>the original image shape</p>  required    <code>ratio_pad</code>  <code>tuple</code>  <p>the ratio of the padding to the original image.</p>  <code>None</code>     <p>Returns:</p>    Name Type Description     <code>masks</code>  <code>torch.tensor</code>  <p>The masks that are being returned.</p>"},{"location":"reference/ops/#clip-boxes","title":"clip boxes","text":"<p>It takes a list of bounding boxes and a shape (height, width) and clips the bounding boxes to the shape</p> <p>Parameters:</p>    Name Type Description Default     <code>boxes</code>  <code>torch.tensor</code>  <p>the bounding boxes to clip</p>  required    <code>shape</code>  <code>tuple</code>  <p>the shape of the image</p>  required"},{"location":"reference/ops/#box-format-conversion","title":"Box Format Conversion","text":""},{"location":"reference/ops/#xyxy2xywh","title":"xyxy2xywh","text":"<p>Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height) format.</p> <p>Parameters:</p>    Name Type Description Default     <code>x</code>  <code>np.ndarray) or (torch.Tensor</code>  <p>The input tensor containing the bounding box coordinates in (x1, y1, x2, y2) format.</p>  required     <p>Returns:</p>    Name Type Description     <code>y</code>  <code>numpy.ndarray) or (torch.Tensor</code>  <p>The bounding box coordinates in (x, y, width, height) format.</p>"},{"location":"reference/ops/#xywh2xyxy","title":"xywh2xyxy","text":"<p>Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner.</p> <p>Parameters:</p>    Name Type Description Default     <code>x</code>  <code>np.ndarray) or (torch.Tensor</code>  <p>The input tensor containing the bounding box coordinates in (x, y, width, height) format.</p>  required     <p>Returns:</p>    Name Type Description     <code>y</code>  <code>numpy.ndarray) or (torch.Tensor</code>  <p>The bounding box coordinates in (x1, y1, x2, y2) format.</p>"},{"location":"reference/ops/#xywhn2xyxy","title":"xywhn2xyxy","text":"<p>Convert normalized bounding box coordinates to pixel coordinates.</p> <p>Parameters:</p>    Name Type Description Default     <code>x</code>  <code>np.ndarray) or (torch.Tensor</code>  <p>The bounding box coordinates.</p>  required    <code>w</code>  <code>int</code>  <p>Width of the image. Defaults to 640</p>  <code>640</code>    <code>h</code>  <code>int</code>  <p>Height of the image. Defaults to 640</p>  <code>640</code>    <code>padw</code>  <code>int</code>  <p>Padding width. Defaults to 0</p>  <code>0</code>    <code>padh</code>  <code>int</code>  <p>Padding height. Defaults to 0</p>  <code>0</code>     <p>Returns:</p>    Name Type Description     <code>y</code>  <code>numpy.ndarray) or (torch.Tensor</code>  <p>The coordinates of the bounding box in the format [x1, y1, x2, y2] where x1,y1 is the top-left corner, x2,y2 is the bottom-right corner of the bounding box.</p>"},{"location":"reference/ops/#xyxy2xywhn","title":"xyxy2xywhn","text":"<p>Convert bounding box coordinates from (x1, y1, x2, y2) format to (x, y, width, height, normalized) format. x, y, width and height are normalized to image dimensions</p> <p>Parameters:</p>    Name Type Description Default     <code>x</code>  <code>np.ndarray) or (torch.Tensor</code>  <p>The input tensor containing the bounding box coordinates in (x1, y1, x2, y2) format.</p>  required    <code>w</code>  <code>int</code>  <p>The width of the image. Defaults to 640</p>  <code>640</code>    <code>h</code>  <code>int</code>  <p>The height of the image. Defaults to 640</p>  <code>640</code>    <code>clip</code>  <code>bool</code>  <p>If True, the boxes will be clipped to the image boundaries. Defaults to False</p>  <code>False</code>    <code>eps</code>  <code>float</code>  <p>The minimum value of the box's width and height. Defaults to 0.0</p>  <code>0.0</code>     <p>Returns:</p>    Name Type Description     <code>y</code>  <code>numpy.ndarray) or (torch.Tensor</code>  <p>The bounding box coordinates in (x, y, width, height, normalized) format</p>"},{"location":"reference/ops/#xyn2xy","title":"xyn2xy","text":"<p>Convert normalized coordinates to pixel coordinates of shape (n,2)</p> <p>Parameters:</p>    Name Type Description Default     <code>x</code>  <code>numpy.ndarray) or (torch.Tensor</code>  <p>The input tensor of normalized bounding box coordinates</p>  required    <code>w</code>  <code>int</code>  <p>The width of the image. Defaults to 640</p>  <code>640</code>    <code>h</code>  <code>int</code>  <p>The height of the image. Defaults to 640</p>  <code>640</code>    <code>padw</code>  <code>int</code>  <p>The width of the padding. Defaults to 0</p>  <code>0</code>    <code>padh</code>  <code>int</code>  <p>The height of the padding. Defaults to 0</p>  <code>0</code>     <p>Returns:</p>    Name Type Description     <code>y</code>  <code>numpy.ndarray) or (torch.Tensor</code>  <p>The x and y coordinates of the top left corner of the bounding box</p>"},{"location":"reference/ops/#xywh2ltwh","title":"xywh2ltwh","text":"<p>Convert the bounding box format from [x, y, w, h] to [x1, y1, w, h], where x1, y1 are the top-left coordinates.</p> <p>Parameters:</p>    Name Type Description Default     <code>x</code>  <code>numpy.ndarray) or (torch.Tensor</code>  <p>The input tensor with the bounding box coordinates in the xywh format</p>  required     <p>Returns:</p>    Name Type Description     <code>y</code>  <code>numpy.ndarray) or (torch.Tensor</code>  <p>The bounding box coordinates in the xyltwh format</p>"},{"location":"reference/ops/#xyxy2ltwh","title":"xyxy2ltwh","text":"<p>Convert nx4 bounding boxes from [x1, y1, x2, y2] to [x1, y1, w, h], where xy1=top-left, xy2=bottom-right</p> <p>Parameters:</p>    Name Type Description Default     <code>x</code>  <code>numpy.ndarray) or (torch.Tensor</code>  <p>The input tensor with the bounding boxes coordinates in the xyxy format</p>  required     <p>Returns:</p>    Name Type Description     <code>y</code>  <code>numpy.ndarray) or (torch.Tensor</code>  <p>The bounding box coordinates in the xyltwh format.</p>"},{"location":"reference/ops/#ltwh2xywh","title":"ltwh2xywh","text":"<p>Convert nx4 boxes from [x1, y1, w, h] to [x, y, w, h] where xy1=top-left, xy=center</p> <p>Parameters:</p>    Name Type Description Default     <code>x</code>  <code>torch.tensor</code>  <p>the input tensor</p>  required"},{"location":"reference/ops/#ltwh2xyxy","title":"ltwh2xyxy","text":"<p>It converts the bounding box from [x1, y1, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right</p> <p>Parameters:</p>    Name Type Description Default     <code>x</code>  <code>numpy.ndarray) or (torch.Tensor</code>  <p>the input image</p>  required     <p>Returns:</p>    Name Type Description     <code>y</code>  <code>numpy.ndarray) or (torch.Tensor</code>  <p>the xyxy coordinates of the bounding boxes.</p>"},{"location":"reference/ops/#segment2box","title":"segment2box","text":"<p>Convert 1 segment label to 1 box label, applying inside-image constraint, i.e. (xy1, xy2, ...) to (xyxy)</p> <p>Parameters:</p>    Name Type Description Default     <code>segment</code>  <code>torch.tensor</code>  <p>the segment label</p>  required    <code>width</code>  <code>int</code>  <p>the width of the image. Defaults to 640</p>  <code>640</code>    <code>height</code>  <code>int</code>  <p>The height of the image. Defaults to 640</p>  <code>640</code>     <p>Returns:</p>    Type Description      <code>np.array</code>  <p>the minimum and maximum x and y values of the segment.</p>"},{"location":"reference/ops/#mask-operations","title":"Mask Operations","text":""},{"location":"reference/ops/#resample_segments","title":"resample_segments","text":"<p>It takes a list of segments (n,2) and returns a list of segments (n,2) where each segment has been up-sampled to n points</p> <p>Parameters:</p>    Name Type Description Default     <code>segments</code>  <code>list</code>  <p>a list of (n,2) arrays, where n is the number of points in the segment.</p>  required    <code>n</code>  <code>int</code>  <p>number of points to resample the segment to. Defaults to 1000</p>  <code>1000</code>     <p>Returns:</p>    Name Type Description     <code>segments</code>  <code>list</code>  <p>the resampled segments.</p>"},{"location":"reference/ops/#crop_mask","title":"crop_mask","text":"<p>It takes a mask and a bounding box, and returns a mask that is cropped to the bounding box</p> <p>Parameters:</p>    Name Type Description Default     <code>masks</code>  <code>torch.tensor</code>  <p>[h, w, n] tensor of masks</p>  required    <code>boxes</code>  <code>torch.tensor</code>  <p>[n, 4] tensor of bbox coordinates in relative point form</p>  required     <p>Returns:</p>    Type Description      <code>torch.tensor</code>  <p>The masks are being cropped to the bounding box.</p>"},{"location":"reference/ops/#process_mask_upsample","title":"process_mask_upsample","text":"<p>It takes the output of the mask head, and applies the mask to the bounding boxes. This produces masks of higher quality but is slower.</p> <p>Parameters:</p>    Name Type Description Default     <code>protos</code>  <code>torch.tensor</code>  <p>[mask_dim, mask_h, mask_w]</p>  required    <code>masks_in</code>  <code>torch.tensor</code>  <p>[n, mask_dim], n is number of masks after nms</p>  required    <code>bboxes</code>  <code>torch.tensor</code>  <p>[n, 4], n is number of masks after nms</p>  required    <code>shape</code>  <code>tuple</code>  <p>the size of the input image (h,w)</p>  required     <p>Returns:</p>    Type Description      <code>torch.tensor</code>  <p>The upsampled masks.</p>"},{"location":"reference/ops/#process_mask","title":"process_mask","text":"<p>It takes the output of the mask head, and applies the mask to the bounding boxes. This is faster but produces downsampled quality of mask</p> <p>Parameters:</p>    Name Type Description Default     <code>protos</code>  <code>torch.tensor</code>  <p>[mask_dim, mask_h, mask_w]</p>  required    <code>masks_in</code>  <code>torch.tensor</code>  <p>[n, mask_dim], n is number of masks after nms</p>  required    <code>bboxes</code>  <code>torch.tensor</code>  <p>[n, 4], n is number of masks after nms</p>  required    <code>shape</code>  <code>tuple</code>  <p>the size of the input image (h,w)</p>  required     <p>Returns:</p>    Type Description      <code>torch.tensor</code>  <p>The processed masks.</p>"},{"location":"reference/ops/#process_mask_native","title":"process_mask_native","text":"<p>It takes the output of the mask head, and crops it after upsampling to the bounding boxes.</p> <p>Parameters:</p>    Name Type Description Default     <code>protos</code>  <code>torch.tensor</code>  <p>[mask_dim, mask_h, mask_w]</p>  required    <code>masks_in</code>  <code>torch.tensor</code>  <p>[n, mask_dim], n is number of masks after nms</p>  required    <code>bboxes</code>  <code>torch.tensor</code>  <p>[n, 4], n is number of masks after nms</p>  required    <code>shape</code>  <code>tuple</code>  <p>the size of the input image (h,w)</p>  required     <p>Returns:</p>    Name Type Description     <code>masks</code>  <code>torch.tensor</code>  <p>The returned masks with dimensions [h, w, n]</p>"},{"location":"reference/ops/#scale_segments","title":"scale_segments","text":"<p>Rescale segment coordinates (xyxy) from img1_shape to img0_shape</p> <p>Parameters:</p>    Name Type Description Default     <code>img1_shape</code>  <code>tuple</code>  <p>The shape of the image that the segments are from.</p>  required    <code>segments</code>  <code>torch.tensor</code>  <p>the segments to be scaled</p>  required    <code>img0_shape</code>  <code>tuple</code>  <p>the shape of the image that the segmentation is being applied to</p>  required    <code>ratio_pad</code>  <code>tuple</code>  <p>the ratio of the image size to the padded image size.</p>  <code>None</code>    <code>normalize</code>  <code>bool</code>  <p>If True, the coordinates will be normalized to the range [0, 1]. Defaults to False</p>  <code>False</code>     <p>Returns:</p>    Name Type Description     <code>segments</code>  <code>torch.tensor</code>  <p>the segmented image.</p>"},{"location":"reference/ops/#masks2segments","title":"masks2segments","text":"<p>It takes a list of masks(n,h,w) and returns a list of segments(n,xy)</p> <p>Parameters:</p>    Name Type Description Default     <code>masks</code>  <code>torch.tensor</code>  <p>the output of the model, which is a tensor of shape (batch_size, 160, 160)</p>  required    <code>strategy</code>  <code>str</code>  <p>'concat' or 'largest'. Defaults to largest</p>  <code>'largest'</code>     <p>Returns:</p>    Name Type Description     <code>segments</code>  <code>List</code>  <p>list of segment masks</p>"},{"location":"reference/ops/#clip_segments","title":"clip_segments","text":"<p>It takes a list of line segments (x1,y1,x2,y2) and clips them to the image shape (height, width)</p> <p>Parameters:</p>    Name Type Description Default     <code>segments</code>  <code>list</code>  <p>a list of segments, each segment is a list of points, each point is a list of x,y</p>  required     <p>coordinates   shape (tuple): the shape of the image</p>"}]}